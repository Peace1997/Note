
## 难点总结

### 一、训练难点

***1. 奖励函数设计与多目标的优化：***
对于 AMP 算法，奖励函数通常包括两大类：风格奖励项和任务奖励项。风格奖励项用于衡量机器人的动作与参考数据集的相似程度，任务奖励项（姿态与平衡控制、运动轨迹、步态风格以及能量效率）则引导机器人以更低的消耗完成设定的任务目标。
各种奖励函数的权重需要合理分配，即多目标优化问题，权重设计不合理偏向某个过于局限的策略，导致“奖励欺骗”问题。

***奖励的优化：***
- **风格奖励与任务奖励的权衡**：如果仅是提升跟踪任务的权重，很有可能导致机器人步态出现问题，因此在训练过程中，我通常先增加风格奖励所占的比例，随后逐渐降低风格奖励奖励的比重，同时在训练初期正则化惩罚我会适当减少一些，增加机器人探索的能力。
- **其他补偿**：除需要使用风格奖励对动作进行约束外，通常需要根据训练的效果进行机器人进行约束。
	- **对称损失**：在损失函数中加入对称损失项，目的是为了左右腿呈现对称的步态
	- **足底距离**、方向和膝盖位置距离进行约束；其中足底位置避免机器人两脚间距过大或过小，膝盖位置和方向尽可能避免内、外八的问题。

**Symmetry Loss：**
- 对于每个采样状态 $s_i$，策略网络会生成一个动作$\pi_\theta(s_i)$ 
- 对于镜像状态$\Psi_o(s_i)$ ，策略网络也会在该镜像状态下生成一个动作：$\pi_\theta(\Psi_o(s_i))$。
- 最后，计算两个动作的镜像对称损失函数 $L_{sym}(\theta)$ ：
$$L_{sym}(\theta) = \sum_{i=0}^{B} ||\pi_{\theta}(s_i) - \Psi_a(\pi_{\theta}(\Psi_o(s_i)))||^2$$

$\Psi_o(\cdot)$ 将状态映射到其镜像版本。
$\Psi_a(\cdot)$ 将动作映射到其镜像版本。


### 二、部署难点

1. 系统问题：程序优先级问题、内核冲突、延时问题。
2. 硬件问题：
	- 结构件稳定性、电机响应延迟、热效应引起的性能衰减
	- IMU 漂移

## 三、泛化和安全

1. 地形差异（硬度、摩擦、倾斜）
2. 负载差异
3. 故障安全模式：在异常情况下快速切换到安全姿态