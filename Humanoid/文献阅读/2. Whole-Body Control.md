
# PLANC: CLF 在作为奖励维持机器人稳定

> California Institute of Technology,

本文将 model-based 的方法与强化学习方法相结合，通过 LIP 吗模型计算将控制李亚普诺夫函数（CLF）作为奖励，控制机器人的稳定性，解决机器人在狭窄/离散落脚点上稳定行走的稳定，让机器人采得准、走的稳。



# BFM-Zero：单一策略实现机器人行为基座模型
[BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning](https://arxiv.Org/abs/2511.04131)
> CMU & Meta



# OmniRetarget：交互式重定向方法与数据增强
[OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction](https://arxiv.org/abs/2509.26633)
> Amazon FAR (Frontier AI & Robotics) & UCB

为了解决重定向数据经常出现的足底滑移、穿透等不符合物理规律的现象，本文通过**可交互的网格、数据增强**的方法，构建人-环境-物体之间的空间关系，机掘滑步和碰撞等问题，并通过数据增强的方式，将人类示教数据转换为多样化、高质量的运动学参考数据。

***Question***
- 人与机器人不匹配：由于人与机器人之间骨骼点存在 GAP，重定向后的数据经常出现足底滑移与穿模的现象
- 缺少空间关联：当前重定向的忽略了机器人与物体和环境的交互

***Contribution***:
- **可交互式的重定向模块**：在保持物理约束的同时，显式地的构建和保留机器人-目标物体-地形之间关键的空间和接触关系。
- **基于约束的重定向方法**：通过最小化人类与机器人网格之间的 Laplacian 形变来强制保持运动学约束，保持动作的几何一致性。- **系统性数据增强**：加入 imitation-guided RL 方法，将单一的人类演示扩充为大规模训练数据集。


***Interaction Mesh Construction***

构建一个 interaction mesh，目标是人体和物体之间形成的这个网络，和机器人与物体之间形成的网络，之间差异不要过大，是通过**硬约束**来限制，即“非穿透约束”，也就是所有身体部位之间，以及身体与物体之间，都不能发生穿透。

> 穿模：例如将人按机器人的身高同比缩放时，人的手刚好碰到箱子的边缘，那么机器人此时会直接穿到箱子里。也就是没有办法保持手、手腕和物体之间的相对位置关系。



>[! Interaction mesh]
>采用了计算机图形学（CG）的思路，Interaction-mesh 可以看作是一个网的概念，不仅关注某个点的信息，而是关注这些点之间的连接关系（例如点与点的相对位置关系）。也就是在人和机器人的身体表面**采样一些关键点**（手臂、手腕、脚跟、头部和肩膀），同时在物体上也选择一些关键点，通过保留**双方的相对空间信息**来实现重定向。
>如果我们更关注某些点或某些部位的映射，就可以增加这些部位的采样点数量。这样在重定向的时候。



***Data Augmentation***

通过数据增强后，即使经过约束后的重定向数据，在大多数情况下，这些轨迹并不能满足动力学可行性因此可以通过模仿学习的方式，通过 RL 的方法对这些数据进行跟踪模仿，进而将机器人执行的轨迹记录下来，这样就能得到动力学可行的轨迹。


> 数据增强方式：改变物体的尺寸、方位；更换机器人模型等。之所以 OmnRetarget 方法可以做数据增强，是因为他可以保留局部的位置关系，进而调整姿势去适应。


# AMS：混合生成数据单一策略实现动态与平衡动作
[Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data](https://arxiv.org/abs/2511.17373)
> 香港大学 & NVIDIA

本文利用异构数据训练单一策略，通过结合人类动捕数据以及使用合成的带约束的平衡动作一起训练，通过单一策略同时完成动态敏捷动作与极限平衡动作的人形机器人控制方法。与此同时，作者还加入混合奖励避免训练冲突，并引入自适应学习提升训练效率。

***Contribution***
- **异构数据**：AMS 生成物理约束的合成极限平衡动作，用于补充 MoCap 数据
- **混合奖励**：设计通用追踪奖励与平衡特定奖励，让策略面对不同的动作类型时不发生目标冲突
- **自适应学习策略**：针对动作的表现动态调整数据采样频率与奖励权重，让策略在异构数据上高效学习。

***Synthetic Balance Motion Generation***
为了消除人类动捕数据带来的重定向的噪声与人体形态的差异带来的影响，直接从机器人/人形体操控空间中进行采样并构造物理可行的平衡全身轨迹，扩展数据至机器人特有的平衡可行解空间，从而提升在平衡关键动作上的稳定性与泛化能力。
**主要流程**：
- 初始化：采样摆动腿目标位姿、骨盆高度、并对上半身关节作随机化
- 阶段一：运动学可行性与光滑性最小化，即先求解保证运动学可行与光滑性目标吗
- 阶段二：再加入平衡约束细化轨迹
- 验证与筛选：检查脚接触一致性、质心在支撑多边形内、关节软限位/硬限位、轨迹平滑度等物理合理性指标，然后纳入训练集。




# SONIC：大规模可扩展全身遥操作

文章通过扩展模型规模、数据量和计算资源逐步提升模型的通用跟踪能力，能够创建自然、鲁棒全身动作的通用型控制器，并且能够零样本泛化到未见过的运动。


运动规划器：连接运动控制与下游任务执行，
统一的通用接口：支持不同的运动输入接口，像是 VR 遥操、人类视频、VLA 模型

利用人类动捕数据提供密集、逐帧监督，无需手动调整奖励即可获得人类运动先验。



规模化维度：
- 网络规模：从 1.2 M 扩展到 42 M 参数
- 数据集：超过 100 M 帧（700 小时）的高质量运动数据
- 计算资源：消耗 9K GPU 小时（128GPU）


***Contribution***
- 可扩展运控模块：通过增加模型规模、数据量和算法验证了机器人运动控制是一个可扩展的任务模块。
- 通用运动学规划器：
- 统一的 token 空间：



***Summary***
- 通过 Sonic 遥操作采集的数据集可以被用作 VLA 基础模型的训练，
- 


***Domain Randomization***


人形机器人运动控制的 scaling law 时刻已来，迈向 whole-body loco-manipulation 的一项

# TWIST 2: 可扩展便携式的全身实时遥操作

该论文在 TWIST 的基础上，使用 PICO 4U 替换动捕设备，完成人形机器人全身数据采集与遥操作系统。该论文提出了层级式视觉视觉运动控制框架，低层控制器通过强化学习完成运动跟踪控制；高层控制器基于 Diffusion Policy ，从视觉输入预测全身关节动作，实现视觉驱动的全身控制。

![[Pasted image 20251107094606.png]]

***Contributions***
- PICO 全身重定向：使用 GMR 
- 层级式运动控制框架：

***Body retargeting***
改造了原有的 GMR 的两阶段优化流程，将 PICO 输出格式适配到机器人关节目标。
**将上下肢分离处理**，使用不同的约束策略去处理上/下体不同的估计误差与接触需求。
（1）上肢：只优化旋转约束，避免通过不可靠的的全局位姿调整导致上半身漂移或者脚部发生滑动，避免因 PICO 全局估计不稳导致上半身“漂移”感。
（2）下肢：同时优化位置与旋转约束，以减少足底滑动并保证接触稳定

***Low-level Control***
给定参考控制命令和机器人本体感知进行运动控制跟踪 （task-agnostic）

- **数据源**：20 K 动捕数据，包括 TWIST 的（13 K）、GMR 重定向的数据（7 K），这些数据来自 AMASS、OMOMO、虚拟动点动捕数据；同时为了减少 domain gap，在 PICO 上采集了 73 条数据，涵盖行走、下蹲以及操作等运动。
- **参考控制命令**：x 和 y 方向的线速度、机身高度、机身姿态（roll/pitch）、角速度（yaw）、全身关节位置
- **网络架构**：将历史本体感知信息和历史参考数据信息通过 convolutional history encoder 压缩为潜在向量，并连接 MLP Backbone 输出动作。

***High-level Control***
根据视觉观测与高层决策生成任务级/语义级控制命令，使低层控制模块进行跟踪控制。（task-specific）

**输入**：第一人称视角的视觉观测（egocentric、机器人头部相机）以及本体感知信息
**输出**：参考控制命令，同低层控制模块命令结构一致
**两种高层策略**：
- Teleoperation Policy：人类直接操作 PICO/VR 设备并通过实时重定向模块生成参考控制命令。
- Visuomotor Policy：基于视觉和历史本体感知信息通过 Diffusion 模块直接预测参考控制命令




***

# UniFP：无需力传感器的力-位混合控制策略

 [Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation](https://arxiv.org/pdf/2505.20829)
> 北京通研院 & 北邮  CoRL 2025 Best Paper Award  -- 2025.5 

![[Pasted image 20251014095402.png|500]]

论文提出了第一个无需力传感器的统一力-位控制策略，用于解决腿足机器人在运动与操作任务中位置控制与力控制耦合的问题。该框架包括力估计模块和力感知模型学习模块，并通过强化学习训练单一策略，融合力与位置控制，能在不同模式（位控、力控、混合柔顺控制）和不同形态的机器人上均实现稳定表现。


末端用位姿补偿、底座用速度补偿，二者共享同一 "合力"

***Contribution***
- **提出了统一的受力-位姿补偿模型**：用简化的阻抗关系把 “外部环境扰动力+力命令-环境反作用力”的净效应映射为对末端位置和基体速度补偿，从而用同一策略自然覆盖位置控制、力控制、阻抗/顺应以及混合力位*四种模式*。其中末端做位置补偿、基座做速度补偿，实现全身顺应与抗扰。
- **受力-状态估计模型**：通过历史本体感知信息和动作来显式估计净外力、末端位置和基体速度，该状态估计器通 RL 主任务进行联合训练，避免了对力传感器的依赖。
- **课程学习策略**：第一阶段仅学习全身跟踪，不增加随机力命令/外扰；第二阶段再引入随机的力命令与外部扰动，采用“线性爬坡-恒值-回落-间歇”的策略，让策略更好的学会力-位耦合。
- **力感知模仿学习**：先用低层策略自带的力估计器去采集含“接触力信息”的示教数据，在用这些数据训练一个基于扩散模型的高层模型策略，最终由高层在视觉-本体感知的基础上同时输出末端位置命令与力命令，交给力-位策略去执行。



 **1. 受力-位姿补偿模型（A Unified Formulation for Force and Position Control）**
本文在训练时把末端和基座分开建模与补偿：末端做位置层的补偿，基座做速度层的补偿。
对于末端执行器面向的是具体物体的相对

**2. 力-状态估计模型（Unified Force-Position Control Policy）**


**3. 力感知模仿学习（Force-aware Imitation Learning）**

# Any2Track：两阶段抗扰策略

[Track Any Motions under Any Disturbances](https://arxiv.org/pdf/2509.13833) 
> 清华 & 北大 & 银河通用  -- 2025.9

论文提出了一个用于人形机器人跟踪的两阶段强化学习框架，用于解决机器人在跟踪高动态、多样化和接触丰富动作时遇到各种动态干扰问题。Any2Track 包含 **AnyTracker**（跟踪各种运动的通用策略）和 **AnyAdapter** （将动态适应性视为在基础动作执行之上的额外能力），并将其学习过程解耦处理，Any2Track 能够在保持动作表达的同时，展示出对环境干扰的卓越鲁棒性。

![[Pasted image 20251013200538.png]]

***Contribution***
- **AnyTracker**：构建一个通用的动作追踪器模块
	- 通过聚类方法为每个动作簇训练一个专家策略，
	- 并通过 DAgger 方法将所有专家策略蒸馏成一个通用策略，以解决不同的动作类别具有显著不同动作模式和动作分布的挑战。
	- 在训练时不引入任何动态随机化，以避免追踪性能下降。
- **AnyAdapter**：基于历史感知信息的动态适应模块
	- 适应性目标：应对的干扰包括：地形、外部作用力（绳索拉力、认脚推力）和物理属性变化（负载、摩擦力）
	- 通过世界模型预测和世界模型提升 AnyTracker 强大的在线动态适应能力。

***AnyAdapter***
- **世界模型预测**（动态感知）：从机器人与环境交互的历史信息中提取动态特征潜入
	- 历史编码器：从历史信息中提取动态特征嵌入
	- 世界模型：利用动态特征进行状态预测，自回归地预测下一帧的机器人状态
- **策略适应**（Policy Adaptation）：利用世界模型提取的动态嵌入，通过适配器架构来调整基础策略。
	- 冻结 AnyTracker ： 为破坏原有的 AnyTracker 已经获得的追踪技能
	- 引入 Adapter 架构：通过层级特征融合机制，将适配器网络插入到 AnyTracker 网络的每一层的特征处理流程中，而不是直接影响最后的 action。


>[! NOTE]
>在传统的策略适应中，将基本动作执行能力和动态适应性耦合在一个网络中，这种耦合会导致网络学习难度增加，并在面对大的动态变化时，使策略过于保守，从而损害人形机器人对动作表达能力和拟人化质量的需求。





# KungfuBot 2: 单一策略实现全身运控
[KUNGFUBOT2: Learning Versatile Motion Skills for Humanoid Whole-Body Control](https://arxiv.org/pdf/2509.16638) 
> 中国电信人工智能研究院 && 上海交大 2025.09 

论文提出了一个统一的全身控制器 **VMS (Versatile Motion Skills)**，旨在让类人机器人通过单一策略学习多样化且动态的行为。论文通过 “**专家解耦+混合跟踪+分段奖励**”的三重创新，最终在仿真和真实世界的 Unitree G1机器人上，实现了对包括功夫、舞蹈在内的、长达数分钟的复杂动态动作序列的高保真、高稳定性模仿。

![[Pasted image 20251013094446.png]]

***Contribution***
- **正交专家混合网络 (OMoE)** ：为了解决多样性与专业性的矛盾，作者引入了一种创新的 OMoE 架构。它将不同的运动技能“解耦”到不同的“专家”子网络中，每个专家负责一个特定的技能领域，并通过一个“路由器”动态组合，从而在保证专业性的同时，极大地提升了模型的表达能力和泛化性。
>**正交化约束让每个专家学会了专注于不同的技能子空间**。

- **混合跟踪目标 (Hybrid Tracking Objective)**：为了平衡局部与全局，VMS 同时优化*全局的根关节位姿*和*局部的身体关键点姿态*，实现了既能保持动作风格，又能确保长期稳定的双重目标。

- **分段式跟踪奖励 (Segment-level Tracking Reward)**：为了提升长时程任务的鲁棒性，VMS 不再要求机器人严格匹配当前帧的参考动作，而是允许它在一个*未来小的时间窗口*内，匹配最“舒服”、最可行的一个未来帧。这种“向前看”的机制，极大地增强了策略对瞬时误差的容忍度。

***网络结构***
采用 Teacher-Student 架构，Teacher 网络的采用 OMoE 的架构，同时对历史信息进行卷积编码，编码的信息结合特权信息进行训练；Student 网络采用 DAgger 进行策略蒸馏，其中固定 Teacher 策略训练好的编码器结合目标轨迹进行训练。

***Experiment***
1. OMoE 的正交性
在标准MoE中，不同专家的输出特征大量重叠。而在OMoE中，不同专家的特征形成了清晰、分离的簇，证明**正交化约束成功地让每个专家学会了专注于不同的技能子空间**。
![[Pasted image 20251013114413.png|300]]
OMoE 的“专业分工”。**行走**动作模式单一，主要只激活了少数几个专家。而**跳舞**动作模式复杂多变，激活了几乎所有专家。这证明 OMoE 能够根据动作的复杂性，自适应地调整专家的使用策略。

![[Pasted image 20251013114304.png|300]]

2. 分段式奖励
纯全局跟踪和严格的逐帧跟踪都容易失败，而 VMS则利用分段式奖励，通过牺牲一点局部精度（如踢腿高度）换取了整体的稳定性，成功完成了动作。
![[Pasted image 20251013134605.png|500]]


# TWIST：全身实时遥操作
`[CoRL 2025] TWIST: Teleoperated Whole-Body Imitation System ` --2025.05
> Stanford University && Simon Fraser University

本文通过 Teacher-Studet 框架，通过动捕设备将人体动作实时映射到机器人上，通过单一网络完成对全身的控制，从而实现实时全身遥操作。

![[Pasted image 20251010200330.png]]
***contribute***:
- **实时全身遥操作**：一个模型控制全身动作实现在线遥操作。
- **未来帧特权观测**：TWIST 在训练阶段的 Teacher 网络引入**未来动作帧**作为特权观测信息，帮助模型更好的理解动作的动态趋势和意图。


训练时采用离线的动捕数据集 AMASS 进行训练，部署时通过动捕设备实时进行运动控制。


# BumbleBee：任务聚类训练与策略蒸馏
`From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots` -- 2025.06
> 北京大学 && BeingBeyond


本文提出一个结合任务聚类、动作补偿与策略蒸馏的专用-通用的训练框架，通过  AutoEncoder 将任务进行聚类划分，然后分别进行训练和训练补偿，最后通过策略蒸馏将单独的任务策略进行整合。

![[Pasted image 20250906140349.png]]

***Contribution***
- **任务聚类**：通过 AutoEncoder 利用运动学信息以及运动描述信息将数据集分为六类。
	- 数据集： HumanML 3D，包含AMASS运动的描述信息
- **专用策略训练**：对于每一类单数据分别进行策略训练，并分别施加动作补偿网络进行优化。
- **策略蒸馏聚合**：最后将六个策略模型进行聚合提纯，形成一个单一策略网络，完成对所有任务的控制。

***AE Clustering***

1. **数据处理与特征构建**：SMPL 只有关节角度和基体位姿，先通过 FK 获得末端位姿，并引入脚步相对速度来区分动作类型
2. **多模态对齐**：
	1. 动作序列经过 Transformer 编码器得到潜在表示向量 $z_m$
	2. 利用 BERT 将 HumanML 3D 数据集提供的语义描述转述为表示$z_t$
	3. 两个潜在空间通过对齐损失（Huber Loss + 重构损失）进行联合训练，使得语义信息和动作信息能够在潜在空间对应起来
3. **自编码器训练与重建**
	1. 编码器：基于 Transformer ，对时间序列动作特征编码
	2. 解码器：同样是 Transformer ，但只重建关键关节（头、骨盆、手和脚）特征，避免冗余噪声，
4. **聚类阶段**
	1. 使用训练的 AE 得到所有动作的潜在表示
	2. 对这些潜在向量进行 K-means 聚类
	3. 聚类数 K 通过 Elbow Method 选择（文中为 6）
5. **聚类结果**
	1. Cluster 1: Z 轴位移最大 --- 跳跃类动作
	2. Cluster 2/3/4： 位移大 -- 行走类动作（速度不同）
	3. Cluster 4/5/6: 原地动作 -- 上肢突出/下肢突出/混合动作

$$
L_{cluster} = L_{InfoNCE}(z^l,z^m) + L_2(z^l,z^m) + L_{huber}(\hat{M}^l,M) + L_{huber}(\hat{M}^m,M)
$$
- 跨模态对齐损失（前两项）：让$z_l$和$z_m$对齐
- 模态内部损失（后两项）：分别重建运动特征$\hat{M}^m$与语义特征$\hat{M}^l$


> 例如走路分为直线和绕圈，如果仅靠运动学可能会把这两种行为归到不同的类中，但引入语义信息后可以得到改善。


***Experts ---> Generalist***

$$
L_{distill} = E_{s～ D} [\text{KL} (p_{general}(a|s) || p_{expert,k(s)}(a|s))]
$$
对于专家策略训练使用的数据集 D，根据当前智能体在交互过程中的状态，判断当前状态所属的动作数据集的分类$k(s)$，从而去选择合适的专家策略（经过动作补偿网络优化后的策略）与通用策略进行 KL 散度计算，目标是最小化通用策略与各个专家策略的 KL 散度。

在策略蒸馏过程中，采用 Transformer 架构替换三层全连接层网络，使其承载更高的容量，在跨任务数据集上兼具敏捷性与稳定性。





# BeyondMimic：基于引导扩散的多任务运动跟踪
`BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion` --2025.8
> UCB & Stanford

本文提出了一个将**运动跟踪**与**动作轨迹生成**相结合的运动控制策略。首先，设计一个可扩展且高质量的动作跟踪框架，能够将运动学参考转化为稳健且高动态的机器人动作；其次，通过离线蒸馏，学习一个**状态-动作扩散模型**，并结合引导机制完成指定任务，使其在测试阶段能够灵活组合多种技能，实现目标驱动控制而无需重新训练。

![[Pasted image 20250923155617.png|500]]

***A. Scalable Motion Tracking***
可扩展的底层运动框架
**Detail**
- 跟踪目标：选用机器人本体坐标系替换世界坐标系
- KPKD 经验设置：通过**Raibert** 等人的经验公式设定 [[机器测试#4. KP、KD 调节|KP、KD设定]]
- 奖励函数： **高斯风格指数函数**归一化
- 数据集：LAFAN

**Adaptive Sampling**

*问题描述*：对长动作序列使用均匀采样可能导致训练偏向简单片段，困难片段训练不足，从而造成奖励波动大、训练效率低等问题。

*解决思路*：本文采用了一种**自适应策略**对动作序列进行采样：在一个很长的动作序列里面，每隔 1 秒划分一个 bin，采样的起始位置就是在这些 bin 里选出来，训练时，就根据失败统计来决定更常从哪些 bin 里采样起始帧。同时采用了指数滑动平均、非因果卷积、自适应概率与均匀采样结合的措施，来优化自适应采样策略。
- 对每个动作区间的失败率采用**指数移动平均（Exponential Moving Average, EMA）** 来平滑，使自适应采样不会因为短期随机失败而产生突变，从而更稳定地偏向困难片段。
-  失败通常是因为在终止前不久执行了次优动作，让“最近的失败”对采样权重影响更大，为了考虑这种“失败来自临近过去”的特点，作者使用了一个 **非因果卷积**，让“最近的失败”对采样权重影响更大。
- 为了保证简单区间也能被采样并防止灾难性遗忘，将上述自适应概率与均匀分布结合。
> 例如，如果摔倒常发生在 10 秒时刻附近，那么训练会更倾向于从 9–11 秒的 bin 里采样，让模型多练这个难点。


***B.Trajectory Synthesis Via Guided Diffusion***
在已经有了高质量的运动跟踪策略之后，进一步通过扩散模型来生成未来的机器人状态-动作轨迹，并且在推理时可以通过*任务相关的代价函数进行引导*，实现灵活的下游控制。

**a. Training**
- 采用 Diffuse-CLoC 方法，构建一个状态-动作联合扩散模型
- 输入：历史观测$O_t=[s_{t-N},a_{t-N},...,s_t]$
- 输出：预测未来轨迹 $\tau_t=[a_t,s_{t+1},a_{t+1},...,s_{t+H},a_{t+H}]$，包括未来 H 步的动作和状态


- 前向过程：逐步对真实轨迹$\tau_t$加高斯噪声
- 网络$x_{0,\theta}$学习去噪 （学习的噪声分布），进而预测原始干净轨迹：$\hat r_t^0=x_{0,\theta}(r_t^k,O_t,k)$
- 损失函数：最小化 MSE，学习如何恢复加噪轨迹
- 反向采样：按照  [[Diffusion Model#DDPM|DDPM]] 更新规则迭代采样，$\alpha_t、\gamma_t、\sigma_t$是 DDPM 的系数
$$
\tau_t^{k-1}=\alpha_t(\tau_t^k-\gamma_k\epsilon_\theta(r_t^k,O_t,k)) + \mathcal{N}(0,\sigma_k^2I)
$$

**b. Guidance**
推理时，为了让生成轨迹符合任务目标，引入了类似于 classifier guidance 类似的方法

条件分布拆分（**贝叶斯分解**）为模型的*无条件分布*和一个*额外的引导项*
$$
\nabla_\tau \text{log}p（\tau|\tau^*） = \nabla_{\tau}\text{log}p(\tau)+\nabla_\tau\text{log}p(\tau^*|\tau)
$$
- $\nabla_{\tau}\text{log}p(\tau)$：模型生成轨迹的自然倾向（原始无条件扩散采样）
- $\nabla_\tau\text{log}p(\tau^*|\tau)$：额外的引导项，用于指引轨迹满足任务目标

引导项通过可微分代价函数$G_c(\tau)$进行替换：
$$
p(\tau^*|\tau) \propto \text{exp}(-G_c(\tau))
$$
> 对应的梯度是：$\nabla_\tau G_c(\tau)$


把代价梯度加入到扩散采样步骤：
$$\tau^{k-1}_t = \alpha_k \Big( \tau^k_t - \gamma_k \big[ \epsilon_\theta(\tau^k_t, O_t, k) - w \cdot \nabla_\tau G_c(\tau^k_t) \big] \Big) + \mathcal{N}(0, \sigma_k^2 I)$$
- $w$ 是引导强度（权重），调节模型学习到的分布和任务引导的平衡。
- 这样，每一步去噪不仅消除了噪声，还被“推向”更符合任务的轨迹区域

在采样过程中，每一步都会被代价函数（可以根据不同的任务需要设定相应的代价函数）的梯度推向低代价的轨迹空间，从而实现目标导向的生成。


**c.Downstream Tasks**
- *Joystick 控制*：
 $$G_c(\tau) = \|V_{xy,t} - g_v\|^2$$
→ 代价梯度推动轨迹速度匹配手柄输入。
- *Waypoint 导航*：
$$G_c(\tau) = \|P_x(s_{t'}) - g_p\| + \lambda \|v_{t'}\|$$
→ 鼓励接近目标点，接近时速度变小。
- *避障*：
$$G_c(\tau) = B(d(\tau), \delta)$$
→ 距离小于阈值时，代价急剧上升，梯度推动轨迹远离障碍 

# EMP：上身可执行运动先验约束
`EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation` -- 2025.7
> 浙江大学

本文提出了一个下半身稳定站立上半身运动全身运动控制架构，通过可执行运动先验模块（Executable Motion Prior, EMP）对上身重定向数据进行约束，机器人并非人类的完美复刻，而是在模仿过程中需要根据自身物理约束进行智能调整，从被动模仿到主动适应性模仿。
![[Pasted image 20250806143828.png]]
***contribution***
**运动重定向网络**：通过图卷机网络（ [[DL/VAE#VQ-VAE|VQ-VAE]] ）将人类上身运动转换为人形机器人可执行的动作，每层通过 
**可执行运动先验模块**：根据机器人当前状态调整输入的目标上身运动，防止超出执行能力。
**RL 控制策略**：使人形机器人跟踪上半身运动目标，同时控制下半身稳定


***EMP***
- 编码：编码器由三个子网络组成：state encoder、target encoder、fusion network。将机器人的当前状态和目标动作同时编码到一个潜在空间中，使模型能够同时考虑机器人当前的物理条件和期望的动作意图
	- State Encoder：将机器人当前时刻的状态信息进行编码，目的是考虑当前机器人的物理状态。MLP
	- Target Encoder：将外部输入的目标动作（重定向后未经优化的动作指令）进行编码，目的是提取人类动作。MLP
	- Fusion Network：将来自状态编码器和目标编码器的两个潜在向量进行融合，同样采用 MLP 的结构，不过采用更大的隐藏层信息。目的是将机器人当前状态与目标动作相结合，以期生成符合机器人当前物理状态的目标动作。
- 解码：动作生成。

EMP 的目的就是生成一个技能跟踪上身目标运动，又能在物理上保持站立的动作指令。类似于人类站立并挥动手臂时，会本能地通过核心肌肉和腿部的微小调整来保持平衡。



# KungfuBot：高动态任务全身运动控制
`KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills` --2025.6
> 中国电信人工智能研究院 （TeleAI）；上海交通大学
> 代码开源：https://github. Com/TeleHuman/PBHC/tree/main

***Abstract***
该论文提出一种高动态技能学习的全身运动控制架构：PBHC（Physics-Based Humanoid Control），目标是让机器人能够模仿功夫和舞蹈等复杂任务。相较于以往跟踪低俗平滑动作的方法，PBHC 通过基于物理量估算的动作筛选以及自适应跟踪机制实现对高动态任务的学习与控制。

![[Pasted image 20250801111740.png]]

---
***Contributions***
- **动作处理阶段**（Motion Processing Pipeline）：首先从视频中进行人类动作的提取，并通过相应的物理量（*质量中心和压力中心*）对超出物理极限的动作进行筛选，并提取*足底接触信息*和动作优化，最后通过 IK 的方式进行动作重定向。
- **自适应跟踪机制**（Adaptive Motion Tracking）：建立一个误差估计与跟踪因子$\sigma$（$\text{ex}(-x/\sigma)$）调整之间的反馈循环，在训练过程中进行动态优化。
---

***Motion Processing Pipeline***

**Physics-based Motion Filtering**
通过分析物理稳定性指标来确定一个运动序列是否可以由人形角色可行地执行，核心原则是评估运动过程中质心和压力中心之间的关系。
- 质心（CoM）：一个系统中所有质量的加权平均点
- 压力中心（CoP）：总压力作用在身体上的点，即身体与接触的点

$$
\Delta d_t =||p_t^{-CoM} - p_t^{-CoP}||_2 < \epsilon_{stab}
$$

运动的稳定性在很大程度上是由这两点之间的关系决定，当投影到地面平面时，CoM 和 CoP 之间的距离表明智能体是否平衡或摔倒。

*质心计算*
- 计算各身体部位的体积
- 将 SMPL 网格从 6890 个顶点扩展到 20000 个高密度顶点
- 获取高密度网络中每个顶点对应的体积权重
- 使用体积加权计算质心
*压力中心计算* 
- 计算顶点相对于地面的高度
- 识别与地面接触的顶点（基于地面接触压力分布模型）
	- 地面以下顶点：越深入地面，压力权重越大
	- 地面以上顶点，离地面越远，压力权重指数衰减
- 计算压力权重
- 压力加权计算压力中心

>[! NOTE] 
> SMPL 网络的体积权重只有在高质量 mesh 数据时才有优势，物理仿真通常直接用刚体质量

---
**Motion Correction based on Contact Mask**

*足底接触掩码计算*（zero- velocity assumption）
- 脚步位置变化小于阈值
- 脚步高度低于阈值




---
***Adaptive Motion Tracking***

**双层优化问题（bi-level optimization problem）**
双层优化问题是指其中一个优化问题嵌套在另一个优化问题之内，*上层问题的目标和变量依赖于下层问题的最优解*。在这里构建了一个 BLO 来推导最优跟踪因子。

**问题描述**：
最优跟踪因子$\sigma^*$和最优跟踪误差$x^*$之间存在循环依赖，这种耦合关系使得$\sigma^*$无法直接计算出来。

**问题建模**：
自适应机制正是为了解决上述循环依赖问题设计的，通过一个在线的、迭代的近似过程来逼近双层优化问题的解：
- 上层问题：寻找最优$\sigma^*$。
- 下层问题：策略在给定当前$\sigma$的情况下，尝试最小化跟踪误差并学习如何精确的模仿参考运动。

$$
\text{max}_{\sigma \in R_+} \qquad J^{ex}(\mathbf{x}^*), \qquad s.t.  \qquad \mathbf{x}^*\in\text{arg max}_{\mathbf{x}\in R_+^N} J^{in}(x,\sigma) + R(\mathbf{x})
$$
$$
J^{in}(\mathbf{x},\sigma)=\sum_{i=1}^N\text{exp}(-\mathbf{x}_i/ \sigma)
$$
- $\mathbf{x}\in R_+^N$：决策变量，$x_i$表示在总步长为 N 的序列中第 i 步的跟踪误差，
- $J^{in}(x,\sigma)$: 跟踪奖励
- $R (x)$： 除$J^{in}$之外的奖励


**问题解决**：
通过维护一个瞬时跟踪误差的指数移动平均（EMA）$\hat{x}$来连接上下层:
$$
\hat{x} = \sum_{i=1}^N x_i^* / N
$$

- $\hat{x}$充当下层问题（策略在当前$\sigma$下的性能）的在线估计，它近似了理论上的最优跟踪误差$x^*$的均值
- 在每个训练步骤：$\sigma \leftarrow \text{min}(\sigma,\hat{x}$) ,这意味着$\sigma$会不断向当前策略所能达到的实际跟踪误差的水平靠齐
- 这种闭环调整，使得系统在训练过程中动态的、渐进的将$\sigma$驱动到一个最优的值，这个最优的值正是理论上双层优化问题所期望的解。

---

# FLAM: 基于模型的强化学习算法实现稳定姿态
`FLAM: Foundation Model-Based Body Stabilization for Humanoid  Locomotion and Manipulation` -2025
> Harbin Institute of Technology && Peng Cheng Laboratory

***Abstract***
该论文提出一种提升机器人身体稳定性的运控操作方法：FLAM。该方法将**稳定化奖励函数**和**基于模型的强化学习**（TD-MPC 2）算法进行结合，鼓励机器人学习稳定的姿态。

![[Pasted image 20250409173511.png|475]]


***Contribution***
- **稳定化奖励函数**：通过对比运动重构前后姿态差异，判断机器人姿态的稳定性。
- **基于模型的强化学习算法**：采用基于模型的强化学习算法 TD-MPC 2，该算法采用数据驱动的模型预测控制方法（MPC），通过时序差分（TD）训练潜在动力学模型和价值函数。
--- 

***A. Stabilizing Reward Function***
稳定性奖励函数可以帮助机器人学习稳定的姿态，主要分为两个部分：
- **Robot-human pose mapping**：将机器人的运动轨迹映射为含噪声的人类运动轨迹
- **Human motion reconstruction**: 通过一种基于人类运动重构基础模型（RoHM, a diffusion-based foundation model），将含有噪声的人类运动轨迹进行重构，生成稳定的人类运动轨迹。
稳定性奖励函数通过对比含噪声的人类运动轨迹和重构的稳定的人类运动轨迹进行奖励函数的设计：如果两者之间差异小于特定的阈值，则获得正奖励：
$$ \Gamma(s^i_h, \hat{s}^i_h) = \begin{cases} r_j, & \text{if } \| s^i_h - \hat{s}^i_h \| < t_j \\ 0, & \text{otherwise} \end{cases} $$
$$ R_S = \sum_{i=1}^{N} \Gamma(s^i_h, \hat{s}^i_h) \times r_j $$

![[Pasted image 20250409182619.png|375]]

![[Pasted image 20250409182847.png|375]]

---

***B. Basic Policy***

基准策略采用 [[MBMF#^fc94a5|TD-MPC2]] 算法与环境进行交互训练，该框架采用时序差分学习来训练潜在动力学模型和价值函数，TD-MPC2 包括五个组件：
-  **Encoder**: 对状态进行编码，用于生成潜在表示 $z = E(s)$ ，其中 $s$ 代表状态。
- **Latent Dynamics**: 利用潜在表示 $z$ 和动作 $a$ 预测下一个潜在状态 $z_{t+1} = D(z_t, a)$  
- **Reward Function**: 在潜在空间中使用奖赏函数 $\hat r= R(z, a)$ 来评估模仿行为的好坏。
- **Terminal Value**: 使用终止值函数 $qˆ = Q(z, a)$ 评估策略在长期的收益情况。
- **Policy Prior**: 基于潜在表示生成策略先验 $\hat a = P(z)$ . 
 
该策略框架在 FLAM 中主要用于根据稳定奖励（stabilizing reward）与任务奖励（task reward）来训练策略。FLAM 将稳定奖励与任务奖励组合起来引导策略的学习过程，以实现更高效更稳定性的任务完成。总体奖励函数计算公式为：

$$
R=R_T + \lambda \frac{q}{l_e} \cdot R_S
$$
- $R_T$：任务型奖励函数，主要指导智能体高效的完成设置的任务。
- $R_S$: 稳定性奖励，引导智能体在完成任务的同时保持身体的稳定性，使其更符合人类的运动模式，即偏风格性奖励。
- $\lambda$ : 缩放因子，平衡稳定性奖励所占比例，以满足不同任务的需求。
- $\frac{q}{l_e}$: $q$为特定任务下的期望的回报值，$l_e$为回合步长，$\frac{q}{l_e}$用于估计每一步的期望贡献。



# ULC：单一策略的全身运动控制
` ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation` -2025 
> 哈尔滨工业大学 && 西湖大学

本文提出了一个名为 ULC （Unified Loco-Manipulation Controller）的全身运动框架，通过单一策略同时协调下肢运动和上肢操作。ULC 提出了包括序列技能学习、残差动作建模、指令多项式插值的方式来实现更高的动作协调性、精度与稳定性。

通过一个策略同时实现上下半身操作，下半身运控制直接通过 MLP 网络输出动作进行控制；上半身精确操作则通过一个残差动作网络（Residual Action Model）进行补偿。

![[Pasted image 20250717171301.png]]

***Contribution：***
- **渐进课程学习**（Sequence Skill Acquisition）：任务课程学习和命令课程学习（无参考数据），从简单控制任务开始学习，只有当前课程任务学好后才到下一个任务。
- **残差动作建模**（residual action modeling）：提高命令跟踪能力，尤其针对手臂关节，对重力进行补偿。
- **指令多项式插值**（Command Polynomial Interpolation）：实现控制指令平滑过渡



# UniTracker:  单一策略的全身运动框架
`# UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots` -2025
> 上海交通大学  && 上海人工智能实验室

本文基于 Teacher-Student 框架提出一个通过单一策略实现全身运动控制的架构，UniTracker 的核心是将条件变分编码器（Conditional Variational Autoencoder ，CVAE)）整合至学生策略中，CVAE 将本体感知数据和参考数据映射成一个潜在变量的分布，再通过解码器几何当前观测和潜在变量分布生成动作。

![[Pasted image 20250718152318.png]]

***Contribution***
- 条件变分编码器建模：通过 CVAE 的潜在变量捕获从状态观测&参考运动到动作运动的模糊性，提高动作的合理性。

***CVAE 结构***
- 编码器 $\epsilon (z_t|p_t^{p-deploy}, s_t^{g-deploy})$：接收与教师策略相同的特权信息，描述期望的理论的运动分布
- 先验分布 $p(z_t|p_t^{p-deploy},s_t^{g-deploy})$ ,条件先验，同时结合自身的状态和参考运动，与直接只结合参考运动的方式相比，生成的语义抽象更符合自身情况。
- 解码器  $D(a_t|s_t^{p-deploy},z_t)$：动作生成，根据潜在变量和当前观测生成动作

> 编码器、先验分布和解码器是同时学习的

**编码器是被建模为先验分布的残差**：
- CVAE 的优化目标（ELBO）中会有一个 KL 散度项，它衡量了编码器学到的分布和先验分布的差异，**鼓励编码器学习一个接近先验分布的潜在空间**（编码器被建模为先验分布的残差），这样做的目的是有助于防止编码器学习到一个过于复杂或退化的潜在空间，从而提高泛化性。所以这个项是**在潜在空间的正则化和重构质量**进行进行平衡。
- 先验分布是可部署的参考信息，编码器输入是更全面的特权信息，KL 散度的作用是确保在特权信息下学到的潜在特征与可部署信息下定义的潜在空间保持一致性和合理性，同时允许编码器通过额外的特权信息进行更精确的调整，这种残差建模进一步强化了这一点，**即编码器学习的是对先验提供的基准的修正**。

***质心跟踪奖励***
鼓励机器人的重心的 XY 的平面投影在足部定义的支撑多边形内（简化为脚踝中心点）来实现机器人的稳定运动。
![[Pasted image 20250721145206.png|300]]


# FALCON: 双智能体分别控制上下肢运动
`FALCON: Learning Force-Adaptive Humanoid Loco-Manipulation` --2025.5
> CMU

本文提出了一个基于双智能体的强化学习运动控制框架 FALCON，其中一个智能体负责在外部干扰下保持稳定运动；另一个智能体精确跟踪末端执行器位置并进行**隐式自适应力补偿**。

![[Pasted image 20250723165348.png]]
***Contribution***
- **双智能体结构**（Dual-Agent）：上下半身分别采用独立的网络进行运动控制，两个智能体同时学习，并共享全身本体感知信息和控制命令。
- **自适应力课程学习**（3D Force curriculum）：在训练过程中，在手臂关节末端渐近增加力，并通过逆动力学控制扭矩的约束。

***Torque-Limit-Aware 3D Force Curriculum***

**目的**：相较于随机施加外力，通过计算随机外力上限（Torque-Aware Force），可以确保所施加的外力在关节扭矩范围内，避免损坏机器。具体计算过程

1. 估计末端执行器能承载的最大力矩
2. 依据末端执行器的雅可比矩阵、笛卡尔空间的末端力以及重力补偿扭矩判断关节力矩是否在关节扭矩范围之内：
$$
-\tau^{lim} \leq \tau^g+ J_{EE}^Tf^{ee} \leq r^{lim} 
$$
3. 因为添加随机外力时是在$i \in x、y、z$ 轴分别设置的，所以还需要对外力进行分解，通过分析单位力在每个方向上产生的最坏情况的关机扭矩（即施加外力对某个关节$j$会产生最大的力矩影响）来估计每个笛卡尔轴的力边界：
$$
f_i^{max} = \text{min}_j(\frac{\tau_j^{lim}-\tau_j^g}{|J_{EE}^T|+ \epsilon }) ;\qquad
f_i^{min} = \text{max}_j(\frac{-\tau_j^{lim}-\tau_j^g}{|J_{EE}^T|+ \epsilon })
$$
4. 最终的可行力会在估计范围内通过 Dirichlet 分布进行均匀采样：
$$
f_t^{ee} = \sum_i F_i \cdot e_i, \qquad F_i \sim \mu[\gamma_i \cdot f_i^{min} ,\gamma_i \cdot f_i^{max}] \qquad \sum\gamma_i = 1
$$

# HOMIE：同构外骨骼驾驶舱的全身运动控制
`HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit` -- 2025
> 上海人工智能实验室 && 香港中文大学（Multimedia Laboratory）

本文介绍了一种半自主的遥操系统，由强化学习控制策略和基于外骨骼的硬件系统组成。其中，强化学习方法进行下半身运动控制（同构脚踏板下发控制指令）、同构外骨骼手臂用于手臂控制、运动传感手套进行手部控制。

![[Pasted image 20250726103013.png]]

***Contribution***：
- 分离运动控制架构：
	- 上身运动控制：通过硬件系统进行遥操作控制
		- **同构外骨骼手臂**（Isomorphic Exoskeleton arm）：直接根据外骨骼读数设置机器人上半身的关节位置，消除传统遥操中 IK 和姿态估计不准确带来的不确定性。
		- **运动传感手套**（Motion-sensing Gloves）：使用霍尔传感器而非伺服电机，可以自由适应任何型号灵巧手。
		- **脚踏板**（Pedal）：通过踩踏踏板来体统下身运动控制的速度指令和高度指令。
	-  下身运动控制：采用 RL 策略，使其能在上半身姿态持续变化的情况下完成下蹲和行走任务。
		- **上半身姿态课程学习**：每当跟踪任务/奖励函数达到一个设定的阈值时，就增加上半身运动的难度，确保机器人上半身姿态持续变化的情况下仍能完成运动任务。
		- **对称性利用**：用于动作正则化和数据增强。
		- **无运动先验数据依赖**

***Data Augmentation***
在仿真环境中获取每个 Transition（$s_t,a_t,r_t,s_{t+1}$） 数据，HOMIE 会对其进行一个翻转操作，这个翻转操作是相对于 x-z 平面进行的，它会对称的翻转以下元素：
- 关节位置和速度
- 机器人的动作
- 期望的转向速度

通过翻转操作会得到一个镜像的过渡数据，**最后将镜像过渡数据和原始过渡数据都添加到经验回放区中。**

这种数据增强有助于提高数据效率，并确保采样数据的对称性，降低训练出的策略在左右性能上不对称的可能性。


同时如果想要增强这种对称性且原始数据本身是对称的，则可增加对称性损失。



# HuB: 执行复杂平衡任务
`HuB: Learning Extreme Humanoid Balance`  - 2025.5
***Abstract***
为解决人形机器人在执行高难度平衡任务的难点，本文提出了一个名为 HuB（Humanoid Balance） 的运动框架。该框架主要包括运动重定向优化、运动平衡策略学习、Sim 2Real 三个模块，更偏向于工程化论文。

***Contribution***：
- **运动重定向动作优化**：
	- 初始对齐，将机器人模型与 SMPL 模型的初始姿态进行对齐
	- 质心过滤：将质心偏移支撑脚中心超过 0.2 m 的轨迹放弃
	- 足底纠正：将单脚支撑状态的时足底静止不动，避免打滑
	- 稳定化过渡：扩充复杂平衡任务前后的帧数，即复制参考运动的第一帧和最后一帧，使他们持续时间等于平衡任务的时间。
- **运动平衡策略学习**：
	- 松弛化的跟踪目标：并非要求策略严格跟踪参考轨迹，允许策略在参考轨迹附近优化调整，以探索到更稳定的运动模式。
	- 接触点约束惩罚：避免脚底发生意外滑动

- **Sim 2 Real**：
	-  局部参考跟踪：因为状态空间中包含质心投影，论文中弃用全局坐标系，建立以机器人躯干为中心的局部控制框架，因此参考数据和机器人在局部坐标系下对齐。
	- IMU 随机噪声：IMU 增加高斯白噪声
	- 高频扰动：对质心和关节增加随机推力

![[Pasted image 20250527144713.png]]


