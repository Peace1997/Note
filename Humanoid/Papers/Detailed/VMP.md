
## Overview

### Framework

本文提出一个两阶段运动控制架构： VMP（Versatile Motion Priors），该架构能够从从大规模运动数据中学习并稳健地执行各种复杂的运动。在第一阶段，使用**变分自编码器（VAE）** 从大规模的运动数据集中学习**通用的潜在运动编码**；在第二阶段，结合第一阶段的潜在运动特征表示和实时参考运动进行**强化学习的控制策略训练**。该方法避免了对抗学习的缺点，并通过结合多模态输入和显式模仿奖励，实现了更高的运动跟踪精度和泛化能力。

VMP 是一种基于学习的运动跟踪框架，智能体在按照运动参考轨迹进行运动的同时遵守物理定律。VMP 算法将运动编码和策略训练分离成两个独立的阶段。

![[Pasted image 20241205135745.png]]


### Contribution

- **两阶段的运动控制架构**：模块化的方法使得训练过程更加高效，并且允许单独优化潜在空间和控制策略。

- **通用的潜在运动特征表示**：利用变分自编码器学习一个通用的潜在运动空间，这一阶段的关键创新在于，**它对运动数据集中的每一个运动参考帧（及其周围的窗口）都生成一个对应的潜在代码**，而不是像以往工作那样为整个运动片段生成单个潜在代码。

- **多模态输入的运动控制策略**：使用强化学习训练一个单一、通用的控制策略，该策略以第一阶段生成的潜在代码和即时的运动参考作为输入，从而生成物理角色的执行器命令。其中运动参考提供了即时反馈，帮助策略精确跟踪目标运动，而潜在代码则包含了运动的过去和未来信息，帮助策略更好地预测和执行复杂的运动。



## Stage I：Kinematic Latent Space Extraction

在第一阶段中，VMP 的目标是从海量的运动片段中学习一个紧凑且能表达短时间动作结构的潜在空间。具体步骤如下：


### Dataset Preprocessing

**数据集**：
 - Reallusion (214 clips, 0.5 h)：Reallusion 旗下的 ActorCore 平台提供免费高质量的 3D 动作下载，包含身体、手部、面部等多种动作，目前大约有数百条动作可供下载：[https://actorcore.reallusion.com/3d-motion/free]([https://www.mixamo.com/](https://www.mixamo.com/))
 - CMU mocap dataset (1870 clips, 8.5 h）：免费开放的动作捕捉数据库，包含 140+ 受试者的走、跑、跳、舞蹈等共计上千段动作。可按主题或受试者编号检索并批量下载 ASF/AMC 或 C3D 格式数据。[http://mocap.cs.cmu.edu/motcat.php]([https://www.mixamo.com/](https://www.mixamo.com/))
 - Mixamo (2150 clips, 2.0 h) ：Adobe 提供的在线角色与动画库，可免费登录导出 FBX/GLTF 格式的动作数据，包含数千条人物动作。[https://www.mixamo.com/](https://www.mixamo.com/) 、[https://huggingface.co/datasets/jasongzy/Mixamo](https://huggingface.co/datasets/jasongzy/Mixamo)

### Motion Retargeting



 **1. 运动数据准备**  
   - 输入数据由一系列连续的运动帧组成，每一帧包含关键的运动信息，如基体高度 $h_t$（相对于地面）、基体的朝向（$\theta_t$  6 dim）、基体速度（6 dim）、关节角度 $q_t$、角速度 $\dot{q}_t$、以及手足相对于根的位置 $p_t$。  
   - 为了捕捉动作的时空演变，论文从原始运动数据中随机抽取短时间窗口，即对中心帧 $t$ 前后各 $W$ 帧，共计 $2W+1$ 帧，记为：
$$ M_t = \{ m_{t-W}, \dots, m_t, \dots, m_{t+W} \}. $$




### VAE
**2. 使用变分自编码器（VAE）建模**  
   - 设计一个基于 1D 卷积层和残差块（resnet blocks）的编码器-解码器结构。编码器将输入的运动窗口映射到一个低维的潜在空间，通常设置潜在向量维度为 $d_z = 64$。  
   - 在编码器的瓶颈层（bottleneck），借鉴 $\beta$-VAE 的思想，通过双倍映射（doubling the encoder output dimension）后采样得到一个多元高斯分布的潜在编码。  
   - 解码器则反向映射该潜在码，重构出原始运动窗口。重构损失结合 KL 散度正则项，从而促使学习到的潜在空间能够捕捉到动作的基本构成和局部运动结构。
[[DL/VAE]]

**3. 目标函数与训练细节**  
   - 使用如下形式的目标函数，其中重构误差和 KL 散度被平衡权衡（通过一个较小的 KL 权重，如 0.002，结合循环调度策略来缓解 KL 消失问题）：  
$$ \mathcal{L} = \mathcal{L}_{\text{recon}} + \beta \, \text{KL}(q(z|M_t) \| p(z)). $$
   - 训练过程中，随机从大规模未经过滤的运动片段数据库中采样运动窗口，确保潜在空间泛化到各种运动技能。


## Stage II：Policy Learning

在第二阶段中，利用第一阶段中训练好的运动先验（即编码器获得的潜在空间），进行物理控制策略的学习。主要内容包括：

**1. 状态表示与输入构造**  
   - 控制策略（Policy）的输入主要由当前帧的**物理状态**和对应时间窗口内运动先验的**潜在编码**构成。  
   - 状态信息包括角色的局部坐标系内的位置、关节状态等，而潜在码则提供了上下文（过往及未来短时间内动作演化）的信息，使策略可以提前“预知”角色在未来可能的运动趋势。

**2. 目标与奖励设计**  
   - 为了确保在物理仿真中运动能够精确跟踪用户指定的运动轨迹和保持动作平滑，奖励函数主要包括两部分：  
     - **运动重构奖励**：该奖励衡量模拟角色在当前帧的状态与由运动先验（从 VAE 解码得到的状态）产生的目标状态之间的接近程度。  
     - **平滑性奖励**：确保动作输出在连续帧间保持平滑，防止产生不连贯或过于剧烈的运动突变。
     
   - 数学上，可将某一帧的状态误差定义为类似如下公式（这里以姿态误差为例）：
$$ e_{\text{pose}} = \frac{1}{N_{joint}} \sum_{j \in \text{joints}} \Big\| (x^j_t - x^{root}_t) - (\hat{x}^j_t - \hat{x}^{root}_t) \Big\|^2, $$
	 其中 $x^j_t$ 与 $\hat{x}^j_t$ 分别代表模拟角色和目标参考运动中关节 $j$ 的 3 D 位置。

**3. 策略训练过程**  
   - 利用强化学习（RL）方法（例如 PPO 算法）训练控制策略，目标是最大化累积奖励，使得物理仿真中的角色能够精确地跟踪运动先验产生的目标。
   - 在训练过程中，控制策略输出的动作作为 PD 控制器的目标，使得角色关节能够产生合适的扭矩，从而实现精确的物理控制。
   - 训练得到的控制策略与编码器形成一个联合系统，在推理时，将用户提供的目标运动序列先通过编码器提取潜在信息，进而由控制策略输出相应动作，实现精细的全身运动控制。



## Summary


***总结***

VMP 的算法框架通过两阶段处理实现了从大规模、无标注、复杂运动数据中提取通用运动先验，并将之应用于基于物理的实时控制中。关键特点包括：

- **运动先验提取**：利用 VAE 通过短时间运动窗口学习到紧凑的潜在运动空间，既能捕捉局部的运动结构，又具有良好的泛化能力。
- **控制策略训练**：基于 RL 方法训练的策略，输入包含当前物理状态和潜在编码，奖励设计旨在精确跟踪运动先验并保障运动平滑。
- **鲁棒性与通用性**：这种分离先验学习与策略训练的方式，使得系统在面对不同的物理对象（如虚拟角色或机器人）和复杂场景时，都能保持较高的动作精度与稳定性，同时便于扩展到更大规模的运动数据与多样化任务。


VMP（Versatile Motion Priors）的算法框架主要由两个阶段构成，其核心思想是将从大量运动数据中提取的运动先验与基于物理的控制策略分离开来，从而实现对复杂动作的鲁棒跟踪和精细控制。这种框架兼顾了数据驱动的运动表达和物理控制的严格性，是实现鲁棒全身运动控制的重要思路。


重定向：OmniH2O 子模块 vs 四书 evaluation，下周三准备一个效果比较

VAE：速度问题，下周三准备一些问题分析

PPO：框架已经改好了。