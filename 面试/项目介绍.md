

## 1. 基于深度强化学习的四足机器人运动控制
1. 根据URDF文件和相关的STL文件完成模型的导入，创建矢量化环境；
2. 初始时获取每个四足机器人基体、关节状态等信息作为相应智能体的观测值（observation）；
3. 将所有智能体观测值输入到一个深度强化学习网络（PPO）中输出每个智能体各个关节要执行的动作（action）
4. 将动作信息输入到关节PD控制器中完成相应的控制任务，智能体执行完该动作后环境会反馈一个奖励（reward），得到新的智能体的观测值；
5. 不断重复 3-4 步骤，与此同时将 observation、action、reward 等信息存储到数据集中（Buffer），用于 PPO 网络的更新。

**输出**的是目标关节位置，经过PD控制器转换为力矩
**动力学因素**：最大力矩、摩擦力、随机推力、阻尼、关节下限、关节上限

**控制频率**：策略控制频率是0.02（50Hz），仿真步长为0.005 (200hz）、由于GPU限制，同时并行训练的限制，控制器、执行期、仿真的运算、网络更新等仿真都需要消耗时间,设置的50hz，基本就是综合整个计算资源设置的。（我有统计每个时间段的最大、最小、平均消耗时间）

**线程数**：Maximum number of threads per block * SM（流多处理器） = 1024 * 68 = 69632 ；SM 和 Bolock 都对寄存器数和共享内存有限制

> 我完成的部分：
> - 仿真调研；别人是怎样通过RL与四足机器人做交互的、仿真平台选择、选择合适的算法
> - 环境适配：就是搭建整个运行环境，在环境中导入我们的四足机器人，并加入相关动力学，创建平坦、复杂地形的复杂环境，
> - 强化学习算法与四足机器人交互；获取机器人状态等信息、设计动作、奖励；最后针对实验效果不断调参优化；
> - 测试加迁移方案：在增加推力、随机摩擦是否能正常行走；并书写了硬件迁移方案。

**状态：**
| 状态         | 维度 | 缩放       |
| ------------ | ---- | ---------- |
| 基体的线速度 | 3    | 2          |
| 基体的角速度 | 3    | 0.25       |
| 基体重力向量 | 3    | 1          |
| 基体指令速度 | 3    | 2、2、0.25 |
| 关节位置差值 | 12   | 1          |
| 关节速度     | 12   | 0.05       |
| 上一时刻动作 | 12   | 1          |

**动作：**
$$
\tau = K_p*(p_d- p) - K_d* (v)
$$

**奖励：**

- 跟踪速度的奖励、沿其他方向运动的惩罚
- 为了使运动更加平滑自然，对关节力度、动作幅度、碰撞给予一定惩罚

| 超参数                    | 数值          | 解释                           |
|:------------------------- | ------------- | ------------------------------ |
| Linear velocity tracking  | 1             | xy 轴线速度跟踪奖励            |
| Angular velocity tracking | 1             | z 轴角速度跟踪奖励             |
| Linear velocity penalty   | -2            | z 轴线速度惩罚                 |
| Angular velocity penalty  | -0.05         | xy 轴角速度惩罚                |
| Joint torques             | -0.000025     | 关节力矩惩罚                   |
| Feet air time             | 2             | 脚腾空时间奖惩                 |
| Collisions                | -1            | 碰撞小腿、大腿、臀部惩罚       |
| Action rate               | -0.02         | 动作幅度惩罚                   |
| Orientation               | -5            | 方向偏移惩罚                   |

 *1. 运动学（ Dynamics）*
研究的是运动本身，主要是表述物体的速度、加速度和空间位置这几个量之间的大小和方向关系。单纯的运动学研究不涉及物体的质量，也就不涉及到力；经常将物体抽象为质点或某个几何形状，研究特征点之间的速度、加速度、相对位置关系。

**正运动学**：指的是已知关节空间变量关节角或角速度，求取操作空间的位置或速度；
**逆运动学**：是指已知操作空间的位姿或速度，求取关节空间的关节角或关节速度。

 *2. 动力学（Kinematics）*
考虑了物体的质量，引入了力和能量，也就是研究物体运动及运动的原因。那么什么时候用运动学，什么时候用动力学。

个人总结，当我们设计某个机器初期，研究其关键零部件的运动轨迹、速度使其满足相应要求时，可以用运动学就可以；当研究如何使机器按照相应速度、加速度平稳的运行起来，涉及到控制时，就需要动力学分析。

**正动力学**：已知机器人的关节驱动力矩和上一时刻的运动状态（角度和角速度），计算得到机器人下一时刻的运动加速度，再积分得到速度和角度；
**逆动力学**：已知某一时刻机器人各关节的位置 、关节速度以及关节加速度，求此时施加在机器人各杆件上的驱动力（力矩）

> 逆动力学可以利用牛顿欧拉(Newton-Euler)方程来求解，也可以利用拉格朗日(Lagrange)方程来求解


## 2. 未知环境下基于强化学习的自主体智能决策与协同控制技术

这个项目环境的话是基于 ROS 平台，每个轮式机器人（Turtlebot3）装配了一个 2D 激光雷达，整个任务的分成两个大阶段来完成，第一阶段主要针对于简单未知环境，第二阶段是对第一阶段的改进，它可以更好对复杂环境进行探索。

**目标：** 在未知环境下，每个机器人在其激光雷达检测范围内，不断选择接下来要前往目标点，以最小的代价完成对未知环境的探索。

两个阶段

- **第一阶段**（决策-规划）：对于决策模块，利用激光雷达反馈的信息，结合 Voronoi 分区和选点公式，分配每个智能体要前往的目标点，然后由每个智能体的控制模块（强化学习）引导智能体无碰撞的到达相应目标点，这个控制模块是通过 优先经验回放池+DDPG+人工经验数据实现的。


- **第二阶段**（决策-规划模块）：第一阶段中，对一些简单的环境是有效的，当整个环境变得更大，障碍物更多时，仅用Voronoi分区和选点公式的方法不足以很好的避免重复探索，对整个地图的建图率大概在80%；因此针对这一问题，选用多智能体深度强化学习的方法，结合循环神经网络、内在奖励以及Voronoi分区辅助手段来完成目标点的决策，然后选用A* 算法作为全局路径规划，规划智能体行进路线，选用DWA算法作为局部路径规划，将路线转换为速度，引导智能体到达所选目标点，该方法在尽可能避免重复探索的同时，将整个地图的建图率提升到90%。

小车 ：EAI 小车
架构：ARM 架构（精简指令集）
芯片：
Arduino — 树莓派
STM32 — Jetson

### 第一阶段

#### 决策

**激光雷达反馈信息**：选择激光雷达中未与障碍物发生碰撞的光束，结合机器人当前的位置，计算这个光束末端所在位置，将该位置加入可选目标点集合中，等待接下来的筛选。

**筛选的条件**： Voronoi 分区 和选点公式；即从激光雷达的检测范围内选择一个目标点。

- **Voronoi 分区**：对于可选目标点集中任意一点，如果其到其他智能体的距离小于到自身的距离，那么就将其从可选目标点集合中删除。
- **选点公式**：基于可选目标点集合
	- 机器人上次所选目标点与下一个可选目标点的距离
	- 机器人初始位置与可选目标点的距离
	- 在交流范围内，可选目标点与其他智能体之间的距离

#### 规划

通过强化学习的方法，引导智能体无碰撞的到达目标位置。

**算法**：

DDPG 算法+ 人工经验数据+ PER（优先经验回放池）

PER：优先采集 TD-Error 较大的数据进行更新
人工经验数据：控制机器人进行移动，在移动过程中，记录状态、动作、奖励过程，保存在一个文件中， 每次训练开始前，将数据输入到优先经验回放池中。

A* 算法在对已地图的导航确实是好，但是开始时，环境未知，对地图感知信息较少时，强化学习效果要好，但是强化学习算法存在失败的情况，比较激进，有一定的失败率，对突然出现的障碍物表现不是特别好，

**奖励函数**：
-   任务完成奖励：与目标点距离小于某个阈值时，则认为完成任务，获得一个较大正奖励
-   安全距离奖励：与障碍距离小于一定距离时，会获得一个负奖励，距离越近，负奖励越大
-   速度控制奖励：控制角速度不宜过大、控制线速度不要过小。
    
**状态空间**：
-   激光雷达距离信息
-   上一时刻速度
-   与目标位置的相对距离（距离+角度）

**动作空间**：
-   线速度 & 角速度
    
**难点**：
1.  如何和 ROS 环境进行交互
2. 如何更有效
3.  如何尽可能避免重复探索：Voronoi、选点公式超参数选定

### 第二阶段

#### 决策

通过多智能体深度强化学习方法（MATD3）结合循环神经网络、内在奖励、Voronoi 分区等辅助任务，共同决策智能体接下来的选点；

**Voronoi**：先通过 Voronoi 分区，对可选目标点集合进行筛选
**V-R-MATD3**：根据状态信息，输出动作（0-1）
**映射**：建立动作与可选目标点集合的映射，比如输出0则选择第0个点，输出q选择最后一个点。


**状态空间**：
-   激光雷达距离信息
-   与上一时刻所选点的相对距离
-   与其他智能体的相对距离
    
**动作空间**：
-   \[0,1]的连续值，用于与可选目标点进行映射，比如0的话就对应第一个可选目标点，1的话就对应最后一个可选目标点。

**奖励空间**：
-   与上一时刻建图差值
-   任务完成奖励
-   内在奖励：当前所选的点与所有智能体的历史选点大于某个阈值则获得内在奖励，分两段，回合数大于3后奖励加倍。
-   惩罚：当前所选的点与其他智能体选择目标点接近、与自身历史目标点接近

**难点**：
- 如何避免重复选点

> 在选择接下来目标点时，之所以不固定每个激光的起始位置，是因为小车在移动的过程中，激光会根据小车的转向，会随时发生变化，如果固定每个激光位置，会发生都需要额外的转换，况且当前是360度的激光，如果小于360度时，情况处理更麻烦。因此我们通过actor神经网络输出动作后与可选目标点映射时，也是与小车的激光相对应的。
> 
> 之所以采用连续动作，是当激光雷达的数量增加时，采用离散动作时，动作空间就会比较大。

#### 规划
[[5.  路径规划]]


## 2. 基于深度强化学习的智能小车对战

基于Box 2d环境，根据实体小车（麦克纳姆轮小车），对仿真小车进行设计，每辆小车搭载一个固定激光发射装置和激光接收器。在1v1对战时，智能小车采用深度强化学习方法（TD3）进行训练，可以在无障碍物环境、有障碍环境、人机对战下完成测试；在2v1中，将1v1中训练较好的版本的actor网络直接迁移，作为MADDPG算法的起始actor网络，经测试，两个智能体可以旋转攻击，在避免队友伤害的同时攻击到敌方小车


**状态空间**：
- health —— 生命值
- pos —— 小车所处位置
- angle —— 小车当前转动的角度
- scan —— 通过射线投射检测发生碰撞物体的类型以及碰撞距离百分比 fraction
- detect —— 检测敌方小车
>与枪口位置(水平)成大约的范围检测到小车，并且两小车的距离小于4，则设为True
>检测范围一般大于射线投射的范围.

**动作设置**
- v_t —— 前后移动
- v_n —— 左右移动
- angular —— 左右旋转角度
- shoot —— 发射子弹
>只要detect到就可以发射子弹

**1v1 奖励函数**：
- 越远距离攻击到敌方小车获得的奖励越多：
$$  
    (Distance - 0.8)*0.1  
$$

> 小车的坐标是由小车的中心点确定的，小车的长度为 0.6m x 0.42m 因此两辆小车接触在一起时他们之间的距离为 0.42m 或 0.51m 或 0.6m
> 
-   攻击敌方小车使其生命值降为0获得一个较大正奖励 （+3）
-   检测到敌方小车时会获得一个较小正奖励（+0.001）
-   两小车相撞获得一个负奖励 （ -0.01）
-   小车撞倒墙体获得一个负奖励（ -0.01）
-   敌方小车攻击智能体使其生命值降为0获得一个较大负奖励 （-3）

  
**2v1 奖励函数**
- 任何一个红色智能体攻击到敌方小车获得一个正奖励：
$$  
    (Distance - 0.4)*0.2  
$$
-   任何一个智能体攻击敌方小车使其生命值降为0获得一个较大正奖励 （+5）
-   任何一个智能体受到攻击都将获得一个较小正奖励（-0.1）

> 任何一个智能体受到的攻击可能来自于敌方小车也有可能来自于友方小车

-   任何一个智能体检测到敌方小车时会获得一个较小正奖励（+0.001）
-   任何一个智能体和其他任何两个小车相撞相撞获得一个负奖励 （ -0.01）
-   任何一个智能体撞到墙体都将获得一个负奖励（ -0.01）
-   任何一个智能体其生命值降为0都将获得一个较大负奖励 （-3）

## 3. 七自由度机械臂设计与关节选型

空间人工智能体控制中心的一个项目。航天机械臂的一个地面试验，主要的任务就是通过单个或两个机械臂协同合作完成一个精细维修任务。这个项目大体可以分为三个部分：1. 调研机械臂公司，通过 Matlab 机器人工具箱完成机械臂的空间仿真。2. 设计一个上位机系统，可以控制机械臂，并完成可视化的显示。3. 通过手眼相机，利用图像处理控制机械臂完成避障、协同控制、维修等任务。

-  该项目任务是在仿真环境中设计七自由度机械臂构型（用于完成在轨维修任务）并完成各个关节模组选型。
-  负责调研市场上七自由度机械臂构型和各厂家关节模组的具体信息（扭矩、质量、工作温度等信息）；
-  负责在MATLAB Robotics Toolbox 仿真环境中，设计一个3R-1R-3R构型的机械臂；
- 负责通过轨迹规划和正逆运动学求解，验证各个厂家关节是否符合任务书的要求。

[[轨迹规划]]
选型的指标：位置、速度、加速度、力矩、功率（关节速度* 力矩）

### 粒子群算法优化 BP 神经网络在边坡稳定性中的应用

神经网络的学习过程，就是从训练数据中自动获取最优权重参数的过程，学习的目的通常以损失函数为基准，找出使它的值达到最小的权重参数。为了尽可能找出可能小的损失函数的值，通常传统 BP 神经网络采用反向传播梯度下降更新权重参数。在这里的话通过粒子群算法优化这些权重参数。将权重和偏置（位置）看作是一个粒子，通过多个粒子组成一个粒子群，通过速度、位置公式以及适应度函数（损失函数），不断更新粒子信息，每个粒子都向最优的粒子进行靠齐，经过不断迭代后，选择最优粒子的权重和偏置作为我们最终的神经网络。

> 将边坡土数据进行预处理（归一化、训练数据、测试数据）。

### 基于 MDL 分割和粒子群优化的轨迹压缩算法

-   MDL 分割算法：类似于数据压缩、近似、拟合，通过一定的判断条件，不断选择满足条件的分割点（垂直距离、角度距离）加入一个**特征点集**，通过这这些特征点集去完成对原始轨迹的近似。

-   改进的MDL分割算法：在MDL分割算法的基础上，增加了一个限制条件，引入了一个平行夹边的概念，作为进一步筛选分割点的条件。    

-   基于粒子群优化的轨迹压缩算法：对原始轨迹中，重要的点进行提取，并实现轨迹的分割，轨迹压缩技术的主要目标就是不影响轨迹精度的条件下，减少轨迹数据的大小。

> 首先人为选定一个角度阈值和距离阈值范围，即在这个范围内找到能代价更小(TAD)的压缩轨迹去近似原轨迹；通过粒子群算法，通过不断调整角度阈值和距离阈值（这两个参数为一个粒子）的大小，得出最后使TAD值（适应度函数；代价函数）最小的那个角度阈值和距离阈值以及K值。


### 谷歌街景门票图像(SVHN)识别

数据集：天池竞赛中谷歌街景图像中门牌号的数据集，大概有8w张左右的照片，大部分的字符都是2-4个字符左右。

难点：因为是变长字符，需要先将变长字符转变成定长字符来处理，字符长度不足的图像可以用“10”填充。

流程：

-  通过DataSet对数据进行封装，提供索引的方式对数据样本进行读取。DataLoder对Dataset进行封装，提供批量读取的迭代读取。
-   学习时：通过CNN/Restnet18 外接五个全连接层进行分类
  
   > 加入 Dropout 层进行防止过拟合的处理。
   
测试时，通过测试数据对测试。

### 基于 Hadoop 的 K-means 算法的并行实现

基于 Hadoop 平台的 MapReduce 模型，在单机环境、伪分布式环境和完全分布式环境下对百万数据实现 K-means 算法，并结合旅游机票数据，分析不同城市游客对旅游城市的偏好，通过Python的Flash框架和百度地图API实现对相关数据的显示。

> K-means 算法的具体步骤包括:
> 
> -   根据特定的方法选择 K 个质心，作为初始的质心文件;
> -   计算每个样本点到质心的距离，并将其归到距离其最近的质心中; 
> -   当每个样本点都分配完成以后，重新计算 K 个质心的位置;
> -   重复步骤 2 和 3，直到所有的样本点不再被分配或是达到最大的迭代次数。
>     
> 在 Map 任务可以计算每个样本点到质心的距离，并进行归类;在 Reduce 任 务中对归类后的结果重新计算新的质心。