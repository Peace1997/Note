研究生的方向是多智能体深度强化学习方向，目前是在中国空间技术研究院杭州中心实习，做的方向是深度强化学习在仿生机器人中的应用。
研究生期间，接触较多的是强化学习相关的项目。
- 在研一时做过基于深度强化学习的智能小车对战，目标就是智能体通过激光的攻击方式尽可能短的时间击败敌方小车，完成了 1v1 和 2v1 的训练和测试，
- 研二上，完成了未知环境下基于强化学习的多自主体智能决策和协同控制的项目，目标就是合理避障的同时，优化智能体的行走路径，以最小的代价完成未知环境的探索和建图。基于 Linux 下 ROS 框架，通过分层控制结构，Voronoi 分区进行为每个智能体分配目标点，深度强化学习方法引导智能体前往分配的目标点；
- 项目结题后，发现可以进行改进，通过多智能体深度强化学习并结合 Voronoi 分区、基于好奇心驱动的内在奖励进行决策，通过 A* 算法完成规划导航，从而有效率的完成未知环境下多智能体的协同探索和建图。

研究生期间也做过和其他的专业交叉融合的项目，将粒子群和神经网络应用在交通、土木方向上，接触过图像处理和机械臂的相关任务。



# 项目介绍

## 1. 未知环境下基于强化学习的自主体智能决策与协同控制技术

这个项目环境的话是基于 ROS 平台，每个轮式机器人（Turtlebot3）装配了一个 2D 激光雷达，整个任务的分成两个大阶段来完成，第一阶段主要针对于简单未知环境，第二阶段是对第一阶段的改进，它可以更好对复杂环境进行探索。

- **第一阶段**（决策-规划）：对于决策模块，利用激光雷达反馈的信息，结合 Voronoi 分区和选点公式，分配每个智能体要前往的目标点，然后由每个智能体的控制模块引导智能体无碰撞的到达相应目标点，这个控制模块是通过 优先经验回放池+DDPG+人工经验数据实现的。


- **第二阶段**（决策-规划模块）：第一阶段中，对一些简单的环境是有效的，当整个环境变得更大，障碍物更多时，仅用Voronoi分区的方法不足以很好的避免重复探索，对整个地图的建图率大概在80%；因此针对这一问题，选用多智能体深度强化学习的方法，结合循环神经网络、内在奖励以及Voronoi分区辅助手段来完成目标点的决策，然后选用A* 算法作为全局路径规划，规划智能体行进路线，选用DWA算法作为局部路径规划，将路线转换为速度，引导智能体到达所选目标点，该方法在尽可能避免重复探索的同时，将整个地图的建图率提升到90%。



### 第一阶段

#### 决策

**激光雷达反馈信息**：选择激光雷达中未与障碍物发生碰撞的光束，结合机器人当前的位置，计算这个光束末端所在位置，将该位置加入可选目标点集合中，等待接下来的筛选。

**筛选的条件**： Voronoi 分区 和选点公式

- **Voronoi 分区**：对于可选目标点集中任意一点，如果其到其他智能体的距离小于到自身的距离，那么就将其从可选目标点集合中删除。
- **选点公式**：基于可选目标点集合
	- 机器人上次所选目标点与下一个可选目标点的距离
	- 机器人初始位置与可选目标点的距离
	- 在交流范围内，可选目标点与其他智能体之间的距离

#### 规划

通过强化学习的方法，引导智能体无碰撞的到达目标位置。

**算法**：

DDPG 算法+ 人工经验数据+ PER（优先经验回放池）

PER：优先采集 TD-Error 较大的数据进行更新
人工经验数据：控制机器人进行移动，在移动过程中，记录状态、动作、奖励过程，保存在一个文件中， 每次训练开始前，将数据输入到优先经验回放池中。


**奖励函数**：
-   任务完成奖励：与目标点距离小于某个阈值时，则认为完成任务，获得一个较大正奖励
-   安全距离奖励：与障碍距离小于一定距离时，会获得一个负奖励，距离越近，负奖励越大
-   速度控制奖励：控制角速度不宜过大、控制线速度不要过小。
    
**状态空间**：
-   激光雷达距离信息
-   上一时刻速度
-   与目标位置的相对距离（距离+角度）

**动作空间**：
-   线速度 & 角速度
    
**难点**：
1.  如何和 ROS 环境进行交互
2. 如何更有效
3.  如何尽可能避免重复探索：Voronoi、选点公式超参数选定

### 第二阶段

#### 决策

通过多智能体深度强化学习方法（MATD3）结合循环神经网络、内在奖励、Voronoi 分区等辅助任务，共同决策智能体接下来的选点；

**Voronoi**：先通过 Voronoi 分区，对可选目标点集合进行筛选
**V-R-MATD3**：根据状态信息，输出动作（0-1）
**映射**：建立动作与可选目标点集合的映射，比如输出0则选择第0个点，输出q选择最后一个点。


**状态空间**：
-   激光雷达距离信息
-   与上一时刻所选点的相对距离
-   与其他智能体的相对距离
    
**动作空间**：
-   \[0,1]的连续值，用于与可选目标点进行映射，比如0的话就对应第一个可选目标点，1的话就对应最后一个可选目标点。

**奖励空间**：
-   与上一时刻建图差值
-   任务完成奖励
-   内在奖励：当前所选的点与所有智能体的历史选点大于某个阈值则获得内在奖励，分两段，回合数大于3后奖励加倍。
-   惩罚：当前所选的点与其他智能体选择目标点接近、与自身历史目标点接近

**难点**：
- 如何避免重复选点


> 在选择接下来目标点时，之所以不固定每个激光的起始位置，是因为小车在移动的过程中，激光会根据小车的转向，会随时发生变化，如果固定每个激光位置，会发生都需要额外的转换，况且当前是360度的激光，如果小于360度时，情况处理更麻烦。因此我们通过actor神经网络输出动作后与可选目标点映射时，也是与小车的激光相对应的。
> 
> 之所以采用连续动作，是当激光雷达的数量增加时，采用离散动作时，动作空间就会比较大。


## 2. 基于深度强化学习的智能小车对战

基于Box 2d环境，根据实体小车（麦克纳姆轮小车），对仿真小车进行设计，每辆小车搭载一个固定激光发射装置和激光接收器。在1v1对战时，智能小车采用深度强化学习方法（TD3）进行训练，可以在无障碍物环境、有障碍环境、人机对战下完成测试；在2v1中，将1v1中训练较好的版本的actor网络直接迁移，作为MADDPG算法的起始actor网络，经测试，两个智能体可以旋转攻击，在避免队友伤害的同时攻击到敌方小车


**状态空间**：
- health —— 生命值
- pos —— 小车所处位置
- angle —— 小车当前转动的角度
- scan —— 通过射线投射检测发生碰撞物体的类型以及碰撞距离百分比 fraction
- detect —— 检测敌方小车
>与枪口位置(水平)成大约的范围检测到小车，并且两小车的距离小于4，则设为True
>检测范围一般大于射线投射的范围.

**动作设置**
- v_t —— 前后移动
- v_n —— 左右移动
- angular —— 左右旋转角度
- shoot —— 发射子弹
>只要detect到就可以发射子弹

**1v1 奖励函数**：
- 越远距离攻击到敌方小车获得的奖励越多：
$$  
    (Distance - 0.8)*0.1  
$$
> 小车的坐标是由小车的中心点确定的，小车的长度为0.6m x 0.42m 因此两辆小车接触在一起时他们之间的距离为0.42m或0.51m或0.6m
-   攻击敌方小车使其生命值降为0获得一个较大正奖励 （+3）
-   检测到敌方小车时会获得一个较小正奖励（+0.001）
-   两小车相撞获得一个负奖励 （ -0.01）
-   小车撞倒墙体获得一个负奖励（ -0.01）
-   敌方小车攻击智能体使其生命值降为0获得一个较大负奖励 （-3）

  
**2v1 奖励函数**
- 任何一个红色智能体攻击到敌方小车获得一个正奖励：
$$  
    (Distance - 0.4)*0.2  
$$
-   任何一个智能体攻击敌方小车使其生命值降为0获得一个较大正奖励 （+5）
-   任何一个智能体受到攻击都将获得一个较小正奖励（-0.1）
> 任何一个智能体受到的攻击可能来自于敌方小车也有可能来自于友方小车
-   任何一个智能体检测到敌方小车时会获得一个较小正奖励（+0.001）
-   任何一个智能体和其他任何两个小车相撞相撞获得一个负奖励 （ -0.01）
-   任何一个智能体撞到墙体都将获得一个负奖励（ -0.01）
-   任何一个智能体其生命值降为0都将获得一个较大负奖励 （-3）

## 七自由度机械臂设计与关节选型

空间人工智能体控制中心的一个项目。航天机械臂的一个地面试验，主要的任务就是通过单个或两个机械臂协同合作完成一个精细维修任务。这个项目大体可以分为三个部分：1. 调研机械臂公司，通过 Matlab 机器人工具箱完成机械臂的空间仿真。2. 设计一个上位机系统，可以控制机械臂，并完成可视化的显示。3. 通过手眼相机，利用图像处理控制机械臂完成避障、协同控制、维修等任务。

-  该项目任务是在仿真环境中设计七自由度机械臂构型（用于完成在轨维修任务）并完成各个关节模组选型。
-  负责调研市场上七自由度机械臂构型和各厂家关节模组的具体信息（扭矩、质量、工作温度等信息）；
-  负责在MATLAB Robotics Toolbox 仿真环境中，设计一个3R-1R-3R构型的机械臂；
- 负责通过轨迹规划和正逆运动学求解，验证各个厂家关节是否符合任务书的要求。

选型的指标：位置、速度、加速度、力矩、功率（关节速度* 力矩）

### 粒子群算法优化 BP 神经网络在边坡稳定性中的应用

神经网络的学习过程，就是从训练数据中自动获取最优权重参数的过程，学习的目的通常以损失函数为基准，找出使它的值达到最小的权重参数。为了尽可能找出可能小的损失函数的值，通常传统 BP 神经网络采用梯度下降的反向传播更新权重参数。在这里的话通过粒子群算法优化这些权重参数。将权重和偏置（位置）看作是一个粒子，通过多个粒子组成一个粒子群，通过速度、位置公式以及适应度函数（损失函数），不断更新粒子信息，每个粒子都向最优的粒子进行靠齐，经过不断迭代后，选择最优粒子的权重和偏置作为我们最终的神经网络。

> 将边坡土数据进行预处理（归一化、训练数据、测试数据）。

### 基于 MDL 分割和粒子群优化的轨迹压缩算法

-   MDL分割算法：类似于数据压缩、近似、拟合，通过一定的判断条件，不断选择满足条件的分割点（垂直距离、角度距离）加入一个**特征点集**，通过这这些特征点集去完成对原始轨迹的近似。
    
-   改进的MDL分割算法：在MDL分割算法的基础上，增加了一个限制条件，引入了一个平行夹边的概念，作为进一步筛选分割点的条件。
    
-   基于粒子群优化的轨迹压缩算法：对原始轨迹中，重要的点进行提取，并实现轨迹的分割，轨迹压缩技术的主要目标就是不影响轨迹精度的条件下，减少轨迹数据的大小。
    
> 首先人为选定一个角度阈值和距离阈值范围，即在这个范围内找到能代价更小(TAD)的压缩轨迹去近似原轨迹；通过粒子群算法，通过不断调整角度阈值和距离阈值（这两个参数为一个粒子）的大小，得出最后使TAD值（适应度函数；代价函数）最小的那个角度阈值和距离阈值以及K值。
    

### 谷歌街景门票图像(SVHN)识别

数据集：天池竞赛中谷歌街景图像中门牌号的数据集，大概有8w张左右的照片，大部分的字符都是2-4个字符左右。

难点：因为是变长字符，需要先将变长字符转变成定长字符来处理，字符长度不足的图像可以用“10”填充。

流程：

-  通过DataSet对数据进行封装，提供索引的方式对数据样本进行读取。DataLoder对Dataset进行封装，提供批量读取的迭代读取。
-   学习时：通过CNN/Restnet18 外接五个全连接层进行分类
    
    > 加入Dropout层进行防止过拟合的处理。
    
测试时，通过测试数据对测试。

### 基于 Hadoop 的 K-means 算法的并行实现

基于 Hadoop 平台的 MapReduce 模型，在单机环境、伪分布式环境和完全分布式环境下对百万数据实现 K-means 算法，并结合旅游机票数据，分析不同城市游客对旅游城市的偏好，通过Python的Flash框架和百度地图API实现对相关数据的显示。

> K-means 算法的具体步骤包括:
> 
> -   根据特定的方法选择 K 个质心，作为初始的质心文件;
> -   计算每个样本点到质心的距离，并将其归到距离其最近的质心中; 
> -   当每个样本点都分配完成以后，重新计算 K 个质心的位置;
> -   重复步骤 2 和 3，直到所有的样本点不再被分配或是达到最大的迭代次数。
>     
> 在 Map 任务可以计算每个样本点到质心的距离，并进行归类;在 Reduce 任 务中对归类后的结果重新计算新的质心。