面试官您好，我叫马佩鑫 ，很高兴能参加彼得大卫机器人有限公司的面试，我期望面试的岗位是深度强化学习算法工程师。我目前的话是在航天三院常驻，（浙江长三角飞航智能技术中心，由三院三部牵头联合浙大成立的一个研究中心）做飞行器运动控制算法的研究，并配合三院完成军方的项目需求。研究生期间主要是做深度强化学习与机器人决策规划相融合的项目，在此期间也接触过机械臂和计算机视觉的内容，也和土木和交通方向做过交叉融合项目，在中国空间技术研究院杭州中心实习期间，从事四足机器人运动控制算法的研究，从0到1搭建了四足机器人强化学习训练仿真系统我认为我的优势是具备较好的学习适应能力，能够较快融入一个新的环境，并按时完成项目要求。以上是我的自我介绍情况。


**为什么想选择新的公司？**
当初签这份工作的话就是选择常驻北京三院的岗位，最近公司出台了新的制度，日后我们将被安排至杭州统一办公，和自己期望的工作地点不太一致。

负责飞行器编队控制和协同制导的算法开发，并配合三院的技术负责人完成军方的项目需求。

协同制导：
**状态空间**：
- 归一化的导弹位置坐标
- 归一化的导弹线速度
- 与目标点的相对距离
- 导弹当前时刻角度
- 导弹当前时刻角速度
- 导弹头部是是否与地面发生接触
- 导弹本体、腿部、推进器是否与地面发生接触
- 上一时刻的动作
**动作空间：**
- 主推力
- 侧向推力

**奖励函数**
- 任务完成奖励：当导弹头部与目标区域的地面发生碰撞时会获得一个较大的正奖励
- 失败惩罚：
	- 导弹运行过程中，如果导弹腿部或主体与地面发生碰撞会获得一个较大的负奖励
	- 导弹运行过程中，如果导弹头部与地面碰撞，与目标点距离越远惩罚越大（设置此奖励的目的是想让智能体先学会以头部完成地面碰撞）
- 距离差值的目标奖励；t时刻导弹距离目标点的距离与t-1时刻导弹距离目标点的距离差值
- 能量消耗惩罚；期望消耗更少的能量去到达目标点。


## 基于深度强化学习的多飞行器运动控制算法
为解决多飞行器编队形成和编队保持问题，设计了基于PPO算法的多飞行器编队控制算法。在多飞行器编队控制任务中，采用领航者-跟随者的编队控制结构，基于深度强化学习算法的控制参数调整方法，生成飞行器的速度和滚转角控制指令。领航者按照规定的航迹飞行，跟随者根据队形要求和领航者的飞行状态信息进行跟踪控制。


## 1. 基于深度强化学习的四足机器人运动控制

**【项目介绍】**
- 该项目任务是设计四足机器人的控制策略，提高四足机器人的环境适应性和运动稳定性。
 - 负责调研当前强化学习在四足机器人运动控制上的科研成果，搭建 Nvidia Isaac Gym 仿真环境，对接机器 人设计团队，导入四足机器人模型，搭建复杂环境（楼梯、斜坡、粗糙路面）；
 - 负责利用深度强化学习算法（PPO）完成四足机器人各个关节的运动控制，对 2048 个四足机器人同时进行 训练，13 分钟可以学会在平坦环境下行走，2 小时可以学会在不同的复杂环境下行走； 
 - 负责模型的测试验证，在平坦环境下，最大运动速度可以达到 2.8m/s，在复杂环境中，可以在 37°的粗糙 斜坡上行走，在随机扰动环境中（随机推力、摩擦力）也可以保持运动稳定。

**【我完成的部分】**
 - 任务/仿真环境调研；别人是怎样通过RL与四足机器人做交互的、仿真平台选择、选择合适的算法。
 - 环境适配：就是搭建整个运行环境，在环境中导入我们的四足机器人，并加入相关动力学，创建平坦、复杂地形的复杂环境，
 - 强化学习算法与四足机器人交互；获取机器人状态等信息、设计动作、奖励；最后针对实验效果不断调参优化，在复杂环境下通过自动课程学习的方法进行；
 - 测试加迁移方案：在增加推力、随机摩擦是否能正常行走；并书写了硬件迁移方案。

**【基本流程】**
1. 根据URDF文件和相关的STL文件完成模型的导入，创建矢量化环境；
2. 初始时获取每个四足机器人基体、关节状态等信息作为相应智能体的观测值（observation）；
3. 将所有智能体观测值输入到一个深度强化学习网络（PPO）中输出每个智能体各个关节要执行的动作（action）
4. 将动作信息输入到关节PD控制器中完成相应的控制任务，智能体执行完该动作后环境会反馈一个奖励（reward），得到新的智能体的观测值；
5. 不断重复 3-4 步骤，与此同时将 observation、action、reward 等信息存储到数据集中（Buffer），用于 PPO 网络的更新。

**【状态、动作、奖励】**
**状态：**
平台环境

| 状态         | 维度 | 缩放       |
| ------------ | ---- | ---------- |
| 基体的线速度 | 3    | 2          |
| 基体的角速度 | 3    | 0.25       |
| 基体重力向量 | 3    | 1          |
| 基体指令速度 | 3    | 2、2、0.25 |
| 关节位置差值 | 12   | 1          |
| 关节速度     | 12   | 0.05       |
| 上一时刻动作 | 12   | 1          |
| 高度信息     | 150  | 5           |

复杂环境还需要增加**激光雷达信息：**，共有两个三维激光雷达，

| 激光雷达         | 相对本体坐标    | 水平测量范围  | 垂直测量范围    |
| ---------------- | --------------- | ------------- | --------------- |
| 上方水平激光雷达 | \[0.45,0,0.165] | \[-2.35,2.35] | \[-0.785,0.785] |
| 下方垂直激光雷达 | \[0.42,0,-0.05] | \[-1.57,1.57] | \[-0.785,0.785] |
检测点设置：
x： \[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
y：\[-0.7,-0.6,-0.5,-0.4,-0.3,-0.2,-0.1,0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7]
高度检测点共有：10 x 15 =150 


**动作：**
- 输出各个关节的相对目标位置（需要加上关节初始位置，作为目标位置（角度）$p_d$： target angle = actionscale * action + defaultAngle）
$$
\tau = K_p*(p_d- p) - K_d* (v)
$$

**奖励：**
- 跟踪速度的奖励、沿其他方向运动的惩罚
- 为了使运动更加平滑自然，对关节力度、动作幅度、碰撞给予一定惩罚

| 超参数                    | 数值          | 解释                           |
|:------------------------- | ------------- | ------------------------------ |
| Linear velocity tracking  | 1             | xy 轴线速度跟踪奖励            |
| Angular velocity tracking | 1             | z 轴角速度跟踪奖励             |
| Linear velocity penalty   | -2            | z 轴线速度惩罚                 |
| Angular velocity penalty  | -0.05         | xy 轴角速度惩罚                |
| Joint torques             | -0.000025     | 关节力矩惩罚                   |
| Feet air time             | 2             | 脚腾空时间奖惩                 |
| Collisions                | -1            | 碰撞小腿、大腿、臀部惩罚       |
| Action rate               | -0.02         | 动作幅度惩罚                   |
| Orientation               | -5            | 方向偏移惩罚                   |

**【实习的难点】**
- 【项目整体把握】做项目的整体过程，对一个项目，通常调研不是很充分，通常就是边做边规划，而在工作中通常需要有一个完整的调研，然后规划出几个月的任务安排，任务安排到天，这需要对对整个项目有一个清晰的认知。 
- 【调研】学校时通常只需要调研当前学术论文；这里调研时，得需要对当前国内外整个行业进展调研，他们完成了哪些内容，达成了哪些指标，我们要做的内容是什么，达成怎样的治标，为什么要这样做，不这样做行不行，多久能做出来。
- 【项目难点】项目开始时的难点是，对一个问题进行数学建模，转换成强化学习的模式。搭建整个系统框架。
- 【任务汇总】任务汇报的时候，不同于学校的一些技术、组会、答辩汇报，通常是面向技术的汇报；对领导汇报时，内容要充实，汇报要抓好重点，需要更加简洁、清晰、明了的解释你在做的东西或你将要去的东西，需要更好的总结，不仅面向技术，还要有整体任务的把握。

**【项目不足】**
- 当前复杂环境中存在一个失败率较高的场景，目前智能体可以较好的在单个地形上完成后退任务，但是在各个地形之间切换时，由于无法获得后方地形高度，可以凭借自身状态信息，没有办法提前修改自身步态，所以经常出现与地形碰撞后才会调整自身步态，面对较难地形时，不能很好的切换就会摔倒，因此需要额外增加对环境的感知信息。
- 上下楼梯的稳定性不够；解决方法的话，因为是针对 200 个子地形同时进行训练的，在训练过程中，增加楼梯地形算占的比例。

**【物理机迁移方案】**
- 【网络训练文件】首先我们获取我们已经在仿真环境中训练好的模型文件 (actor.pt)，接下来就需要考虑网络输入参数的获取和输出的转换。
- 【输入数据的获取】：通过ros_control中间件，利用IMU或moveit应用获得内置ROS系统的物理机器人的状态信息
- 【网络预测】将状态信息输入到训练好的模型文件，得到各个关节的目标位置，运行时间大概0.几毫秒
- 【输出数据的转换】：通过moveit应用将目标关节位置转换为力矩信息，通过ros_control反馈到真实物理机器人中
- 不断重复2-4步


**输出**的是目标关节位置，经过 PD 控制器转换为力矩

**动力学因素**：力矩、摩擦力、随机推力、阻尼、关节下限、关节上限

**控制频率**：策略控制频率是0.02（50Hz），仿真步长为0.005 (200hz）、由于GPU限制，同时并行训练的限制，控制器、执行器、仿真的运算、网络更新等仿真都需要消耗时间,设置的50hz，基本就是综合整个计算资源设置的。（我有统计每个时间段的最大、最小、平均消耗时间）

**线程数**：Maximum number of threads per block * SM（流多处理器） = 1024 * 68 = 69632 ；SM 和 Bolock 都对寄存器数和共享内存有限制

**仿真平台调研**：
Gazebo具有丰富的传感器模块和机器人模型，几乎可以适配各种仿真任务，因为与ROS紧密联系，当与环境中的智能进行交互时，得需要经过ROS这个“中介”来完成，需要通过话题来完成数据的发布和订阅，因此效率相对较低，Raisim 未进行开源，用于学术研究的可以免费申请，目前参考资料较少、支持的传感器较少。Pybullet和Mujoco两个仿真器比较接近，对强化学习的API易用性较好，因为Mujoco在2021年10月18日由DeepMind收购并开源了MuJoCo软件，使用量相对较少，通过查阅Pybullet和Mujoco最新的用户开发文档，Mujoco相较于Pybullet提供了更加丰富的传感器信息，因为其物理引擎由C语言编写，运行效率更高。不过上述几个方针环境都是通过的物理引擎去完成计算，当进行并行训练时，效果不是很好，而Isaac Gym 把这些计算搬到了GPU上，更加适用于并行训练，可以在一个环境中同时训练几百上千个智能体，同时ISaacGym 也提供了较为丰富的传感器，提供了对ROS1和ROS2的支持，


 *1. 运动学（ Dynamics）*
研究的是运动本身，主要是表述物体的速度、加速度和空间位置这几个量之间的大小和方向关系。单纯的运动学研究不涉及物体的质量，也就不涉及到力；经常将物体抽象为质点或某个几何形状，研究特征点之间的速度、加速度、相对位置关系。

**正运动学**：指的是已知关节空间变量关节角或角速度，求取操作空间的位置或速度；
**逆运动学**：是指已知操作空间的位姿或速度，求取关节空间的关节角或关节速度。

 *2. 动力学（Kinematics）*
考虑了物体的质量，引入了力和能量，也就是研究物体运动及运动的原因。那么什么时候用运动学，什么时候用动力学。

个人总结，当我们设计某个机器初期，研究其关键零部件的运动轨迹、速度使其满足相应要求时，可以用运动学就可以；当研究如何使机器按照相应速度、加速度平稳的运行起来，涉及到控制时，就需要动力学分析。

**正动力学**：已知机器人的关节驱动力矩和上一时刻的运动状态（角度和角速度），计算得到机器人下一时刻的运动加速度，再积分得到速度和角度；
**逆动力学**：已知某一时刻机器人各关节的位置 、关节速度以及关节加速度，求此时施加在机器人各杆件上的驱动力（力矩）

> 逆动力学可以利用牛顿欧拉(Newton-Euler)方程来求解，也可以利用拉格朗日(Lagrange)方程来求解


## 2. 未知环境下基于强化学习的自主体智能决策与协同控制技术

这个项目环境的话是基于 ROS 平台，每个轮式机器人（Turtlebot3）装配了一个 2D 激光雷达，整个任务的分成两个大阶段来完成，第一阶段主要针对于简单未知环境，第二阶段是对第一阶段的改进，它可以更好对复杂环境进行探索。

**目标：** 在未知环境下，每个机器人在其激光雷达检测范围内，不断选择接下来要前往目标点，以最小的代价完成对未知环境的探索。
> 在未知复杂环境下利用多智能体深度强化学习、内在动机、Voronoi分区性质解决协作探索问题。

两个阶段

- **第一阶段**（决策-规划）：对于决策模块，利用激光雷达反馈的信息，结合 Voronoi 分区和选点公式，分配每个智能体要前往的目标点，然后由每个智能体的控制模块（强化学习）引导智能体无碰撞的到达相应目标点，这个控制模块是通过 优先经验回放池+DDPG+人工经验数据实现的。


- **第二阶段**（决策-规划模块）：第一阶段中，对一些简单的环境是有效的，当整个环境变得更大，障碍物更多时，仅用Voronoi分区和选点公式的方法不足以很好的避免重复探索，对整个地图的建图率大概在80%；因此针对这一问题，选用多智能体深度强化学习的方法，结合循环神经网络、内在奖励以及Voronoi分区辅助手段来完成目标点的决策，然后选用A* 算法作为全局路径规划，规划智能体行进路线，选用DWA算法作为局部路径规划，将路线转换为速度，引导智能体到达所选目标点，该方法在尽可能避免重复探索的同时，将整个地图的建图率提升到90%。

小车 ：EAI 小车
架构：ARM 架构（精简指令集）
芯片：
Arduino — 树莓派
STM32 — Jetson

### 第一阶段

***决策***

**激光雷达反馈信息**：选择激光雷达中未与障碍物发生碰撞的光束，结合机器人当前的位置，计算这个光束末端所在位置，将该位置加入可选目标点集合中，等待接下来的筛选。

**筛选的条件**： Voronoi 分区 和选点公式；即从激光雷达的检测范围内选择一个目标点。

- **Voronoi 分区**：对于可选目标点集中任意一点，如果其到其他智能体的距离小于到自身的距离，那么就将其从可选目标点集合中删除。
- **选点公式**：基于可选目标点集合
	- 机器人上次所选目标点与下一个可选目标点的距离
	- 机器人初始位置与可选目标点的距离
	- 在交流范围内，可选目标点与其他智能体之间的距离

***规划***

通过强化学习的方法，引导智能体无碰撞的到达目标位置。

**算法**：

DDPG 算法+ 人工经验数据+ PER（优先经验回放池）

PER：优先采集 TD-Error 较大的数据进行更新
人工经验数据：控制机器人进行移动，在移动过程中，记录状态、动作、奖励过程，保存在一个文件中， 每次训练开始前，将数据输入到优先经验回放池中。

A* 算法在对已地图的导航确实是好，但是开始时，环境未知，对地图感知信息较少时，强化学习效果要好，但是强化学习算法存在失败的情况，比较激进，有一定的失败率，对突然出现的障碍物表现不是特别好，

**奖励函数**：
-   任务完成奖励：与目标点距离小于某个阈值时，则认为完成任务，获得一个较大正奖励
-   安全距离奖励：与障碍距离小于一定距离时，会获得一个负奖励，距离越近，负奖励越大
-   速度控制奖励：控制角速度不宜过大、控制线速度不要过小。
    
**状态空间**：
-   激光雷达距离信息
-   上一时刻速度
-   与目标位置的相对距离（距离+角度）

**动作空间**：
-   线速度 & 角速度
    
**难点**：
1.  如何和 ROS 环境进行交互
2. 如何更有效
3.  如何尽可能避免重复探索：Voronoi、选点公式超参数选定

### 第二阶段

***1. 决策***

通过多智能体深度强化学习方法（MATD3）结合循环神经网络、内在奖励、Voronoi 分区等辅助任务，共同决策智能体接下来的选点；

**Voronoi**：先通过 Voronoi 分区，对可选目标点集合进行筛选
**V-R-MATD3**：根据状态信息，输出动作（0-1）
**映射**：建立动作与可选目标点集合的映射，比如输出0则选择第0个点，输出1选择最后一个点。


**状态空间**：
-   激光雷达距离信息
-   与上一时刻所选点的相对距离
-   与其他智能体的相对距离
    
**动作空间**：
-   \[0,1]的连续值，用于与可选目标点进行映射，比如0的话就对应第一个可选目标点，1的话就对应最后一个可选目标点。

**奖励空间**：
-   与上一时刻建图差值
-   任务完成奖励
-   内在奖励：当前所选的点与所有智能体的历史选点大于某个阈值则获得内在奖励，分两段，回合数大于3后奖励加倍。
-   惩罚：当前所选的点与其他智能体选择目标点接近、与自身历史目标点接近

**难点**：
- 如何避免重复选点

> 在选择接下来目标点时，之所以不固定每个激光的起始位置，是因为小车在移动的过程中，激光会根据小车的转向，会随时发生变化，如果固定每个激光位置，会发生都需要额外的转换，况且当前是360度的激光，如果小于360度时，情况处理更麻烦。因此我们通过actor神经网络输出动作后与可选目标点映射时，也是与小车的激光相对应的。
> 
> 之所以采用连续动作，是当激光雷达的数量增加时，采用离散动作时，动作空间就会比较大。


***2. 规划***

[[5.  路径规划]]


## 2. 基于深度强化学习的智能小车对战

基于Box 2d环境，根据实体小车（麦克纳姆轮小车），对仿真小车进行设计，每辆小车搭载一个固定激光发射装置和激光接收器。在1v1对战时，智能小车采用深度强化学习方法（TD3）进行训练，可以在无障碍物环境、有障碍环境、人机对战下完成测试；在2v1中，将1v1中训练较好的版本的actor网络直接迁移，作为MADDPG算法的起始actor网络，经测试，两个智能体可以旋转攻击，在避免队友伤害的同时攻击到敌方小车


**状态空间**：
- health —— 生命值
- pos —— 小车所处位置
- angle —— 小车当前转动的角度
- scan —— 通过射线投射检测发生碰撞物体的类型以及碰撞距离百分比 fraction
- detect —— 检测敌方小车
>与枪口位置(水平)成大约的范围检测到小车，并且两小车的距离小于4，则设为True
>检测范围一般大于射线投射的范围.

**动作设置**
- v_t —— 前后移动
- v_n —— 左右移动
- angular —— 左右旋转角度
- shoot —— 发射子弹
>只要detect到就可以发射子弹

**1v1 奖励函数**：
- 越远距离攻击到敌方小车获得的奖励越多：
$$  
    (Distance - 0.8)*0.1  
$$

> 小车的坐标是由小车的中心点确定的，小车的长度为 0.6m x 0.42m 因此两辆小车接触在一起时他们之间的距离为 0.42m 或 0.51m 或 0.6m
> 
-   攻击敌方小车使其生命值降为0获得一个较大正奖励 （+3）
-   检测到敌方小车时会获得一个较小正奖励（+0.001）
-   两小车相撞获得一个负奖励 （ -0.01）
-   小车撞倒墙体获得一个负奖励（ -0.01）
-   敌方小车攻击智能体使其生命值降为0获得一个较大负奖励 （-3）

  
**2v1 奖励函数**
- 任何一个红色智能体攻击到敌方小车获得一个正奖励：
$$  
    (Distance - 0.4)*0.2  
$$
-   任何一个智能体攻击敌方小车使其生命值降为0获得一个较大正奖励 （+5）
-   任何一个智能体受到攻击都将获得一个较小正奖励（-0.1）

> 任何一个智能体受到的攻击可能来自于敌方小车也有可能来自于友方小车

-   任何一个智能体检测到敌方小车时会获得一个较小正奖励（+0.001）
-   任何一个智能体和其他任何两个小车相撞相撞获得一个负奖励 （ -0.01）
-   任何一个智能体撞到墙体都将获得一个负奖励（ -0.01）
-   任何一个智能体其生命值降为0都将获得一个较大负奖励 （-3）

## 3. 七自由度机械臂设计与关节选型

空间人工智能体控制中心的一个项目。航天机械臂的一个地面试验，主要的任务就是通过单个或两个机械臂协同合作完成一个精细维修任务。这个项目大体可以分为三个部分：1. 调研机械臂公司，通过 Matlab 机器人工具箱完成机械臂的空间仿真。2. 设计一个上位机系统，可以控制机械臂，并完成可视化的显示。3. 通过手眼相机，利用图像处理控制机械臂完成避障、协同控制、维修等任务。

-  该项目任务是在仿真环境中设计七自由度机械臂构型（用于完成在轨维修任务）并完成各个关节模组选型。
-  负责调研市场上七自由度机械臂构型和各厂家关节模组的具体信息（扭矩、质量、工作温度等信息）；
-  负责在MATLAB Robotics Toolbox 仿真环境中，设计一个3R-1R-3R构型的机械臂；
- 负责通过轨迹规划和正逆运动学求解，验证各个厂家关节是否符合任务书的要求。

[[轨迹规划]]
选型的指标：位置（未知）、速度、加速度、力矩、功率（关节速度* 力矩）、工作温度、环境湿度
关节模组厂家：科尔摩根、大族、达闼、大象、泰科、轶群

## 4. 粒子群算法优化 BP 神经网络在边坡稳定性中的应用

神经网络的学习过程，就是从训练数据中自动获取最优权重参数的过程，学习的目的通常以损失函数为基准，找出使它的值达到最小的权重参数。为了尽可能找出可能小的损失函数的值，通常传统 BP 神经网络采用反向传播梯度下降更新权重参数。在这里的话通过粒子群算法优化这些权重参数。将权重和偏置（位置）看作是一个粒子，通过多个粒子组成一个粒子群，通过速度、位置公式以及适应度函数（损失函数），不断更新粒子信息，每个粒子都向最优的粒子进行靠齐，经过不断迭代后，选择最优粒子的权重和偏置作为我们最终的神经网络。

> 将边坡土数据进行预处理（归一化、训练数据、测试数据）。

**PSO算法流程**

1.  初始化一群规模为M的粒子，包括随机位置和速度
2.  评价每个粒子的适应度、并得到Pbest、Gbest（位置）
3.  对每个粒子
    -   当前位置的适应度值与Pbest比较，如果较好，用当前位置替换Pbest
    -   比较当前位置适应度值与Gbest比较，如果较好，用当前位置替换Gbest
    -   调整粒子速度和位置
4.  若未达到结束条件则继续步骤3


## 5. 基于 MDL 分割和粒子群优化的轨迹压缩算法

-   MDL 分割算法：类似于数据压缩、近似、拟合，通过一定的判断条件，不断选择满足条件的分割点（垂直距离、角度距离）加入一个**特征点集**，通过这这些特征点集去完成对原始轨迹的近似。

-   改进的MDL分割算法：在MDL分割算法的基础上，增加了一个限制条件，引入了一个平行夹边的概念，作为进一步筛选分割点的条件。    

-   基于粒子群优化的轨迹压缩算法：对原始轨迹中，重要的点进行提取，并实现轨迹的分割，轨迹压缩技术的主要目标就是不影响轨迹精度的条件下，减少轨迹数据的大小。

> 首先人为选定一个角度阈值和距离阈值范围，即在这个范围内找到能代价更小(TAD)的压缩轨迹去近似原轨迹；通过粒子群算法，通过不断调整角度阈值和距离阈值（这两个参数为一个粒子）的大小，得出最后使TAD值（适应度函数；代价函数）最小的那个角度阈值和距离阈值以及K值。


## 6. 谷歌街景门票图像(SVHN)识别

数据集：天池竞赛中谷歌街景图像中门牌号的数据集，大概有8w张左右的照片，大部分的字符都是2-4个字符左右。

难点：因为是变长字符，需要先将变长字符转变成定长字符来处理，字符长度不足的图像可以用“10”填充。

流程：

-  通过DataSet对数据进行封装，提供索引的方式对数据样本进行读取。DataLoder对Dataset进行封装，提供批量读取的迭代读取。
-   学习时：通过CNN/Restnet18 外接五个全连接层进行分类
  
   > 加入 Dropout 层进行防止过拟合的处理。
   
![400](resnet18.png)

(1)这里有虚线和实线，代表什么呢？实线表示残差块中的通道数没有变化，虚线表示通道数变化，例如64->128。
(2)那通道数变化了怎么办？通过1* 1卷积调整一下通道数，然后将步长调整成2就行了呀。

测试时，通过测试数据对测试。

## 7. 基于 Hadoop 的 K-means 算法的并行实现

基于 Hadoop 平台的 MapReduce 模型，在单机环境、伪分布式环境和完全分布式环境下对百万数据实现 K-means 算法，并结合旅游机票数据，分析不同城市游客对旅游城市的偏好，通过Python的Flash框架和百度地图API实现对相关数据的显示。

> K-means 算法的具体步骤包括:
> 
> -   根据特定的方法选择 K 个质心，作为初始的质心文件;
> -   计算每个样本点到质心的距离，并将其归到距离其最近的质心中; 
> -   当每个样本点都分配完成以后，重新计算 K 个质心的位置;
> -   重复步骤 2 和 3，直到所有的样本点不再被分配或是达到最大的迭代次数。
>     
> 在 Map 任务可以计算每个样本点到质心的距离，并进行归类; 在 Reduce 任 务中对归类后的结果重新计算新的质心。


评价聚类效果：轮廓系数（Silhouette Coefficient），是聚类效果好坏的一种评价方式。
-   计算 a(i) = average(i向量到所有它属于的簇中其它点的距离)
-   计算 b (i) = min (i 向量到所有非本身所在簇的点的平均距离)
$$
S(i)=\frac{b(i)−a(i)}{max \{a(i),b(i)\}}
$$
可见轮廓系数的值是介于 \[-1,1] ，越趋近于1代表内聚度和分离度都相对较优。