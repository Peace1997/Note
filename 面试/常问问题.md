您好，我叫 xxx，今年 24 岁，研究生的方向是深度强化学习方向，研一主要做的是智能小车激光对战项目和研二主要做的是未知环境多智能体自主探索建图项目，都有利用单智能体、多智能体强化学习进行决策，*同时也会利用好奇心的内在奖励、循环神经网络、或一些启发式的方法作为辅助任务*。 现在的话也是在中国空间技术研究院杭州中心实习，也是独立负责在仿真环境下将强化学习应用在四足机器人的行走上，目标的话，是希望四足机器人在平坦和复杂环境下，学的更快，走的更好； 研究生期间也做过和其他的专业交叉融合的项目，将粒子群和神经网络应用在交通、土木方向上，接触过图像处理和机械臂的相关任务，

目前也正在学习AutoML、模仿学习内容。

研究生的方向是多智能体深度强化学习方向，目前是在中国空间技术研究院杭州中心实习，做的方向是深度强化学习在仿生机器人中的应用。
研究生期间，接触较多的是强化学习相关的项目。
- 在研一时做过基于深度强化学习的智能小车对战，目标就是智能体通过激光的攻击方式尽可能短的时间击败敌方小车，完成了 1v1 和 2v1 的训练和测试，
- 研二上，完成了未知环境下基于强化学习的多自主体智能决策和协同控制的项目，目标就是合理避障的同时，优化智能体的行走路径，以最小的代价完成未知环境的探索和建图。基于 Linux 下 ROS 框架，通过分层控制结构，Voronoi 分区进行为每个智能体分配目标点，深度强化学习方法引导智能体前往分配的目标点；
- 项目结题后，发现可以进行改进，通过多智能体深度强化学习并结合 Voronoi 分区、基于好奇心驱动的内在奖励进行决策，通过 A* 算法完成规划导航，从而有效率的完成未知环境下多智能体的协同探索和建图。

研究生期间也做过和其他的专业交叉融合的项目，将粒子群和神经网络应用在交通、土木方向上，接触过图像处理和机械臂的相关任务。

**实习的难点**
- 做项目的整体过程，对一个项目，通常调研不是很充分，通常就是边做边规划，而在工作中通常需要有一个完整的调研，然后规划出几个月的任务安排，任务安排到天，这需要对对整个项目有一个清晰的认知。 
- 调研的区别，学校时通常只需要调研当前学术论文；这里调研时，得需要对当前国内外整个行业进展调研，他们完成了哪些内容，达成了哪些指标，我们要做的内容是什么，达成怎样的治标，为什么要这样做，不这样做行不行，多久能做出来。
- 项目开始时的难点是，对一个问题进行数学建模，转换成强化学习的模式。搭建整个系统框架。
- 任务汇报的时候，不同于学校的一些技术、组会、答辩汇报，通常是面向技术的汇报；对领导汇报时，内容要充实，汇报要抓好重点，需要更加简洁、清晰、明了的解释你在做的东西或你将要去的东西，需要更好的总结，不仅面向技术，还要有整体任务的把握。

**强化学习进展介绍**

**人工智能新技术、人工智能新框架与人工智能新应用**三个方面来展开



强化学习 状态、动作、奖励、超参数，设置不好，很难出效果。经验性



**什么是集成学习？**
集成学习算法本身不算一种单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。集百家之所长，能在机器学习算法中拥有较高的准确率，不足之处就是模型的训练过程可能比较复杂，效率不是很高。
目前常见的集成学习算法主要有 2 种：
- 基于 Bagging 的算法：随机森林
- 基于Boosting的算法：Adaboost、GBDT、XGBOOST等。

**Pytorch：model. train () 和 model. eval () 用法和区别 ？**
- `model.train ()` 的作用是启用 Batch Normalization 和 Dropout，用于训练阶段。
- `model.eval ()` 的作用是不启用 Batch Normalization 和 Dropout ，用于测试阶段。
	此时 pytorch 会自动把 BN 和 DropOut 固定住，不会取平均，而是用训练好的值。不然的话，一旦 test 的 batch_size 过小，很容易就会因 BN 层导致模型 performance 损失较大；
- `torch.no_grad()` 也是用于测试阶段，用于关闭梯度计算，节省 eval 的时间。

只进行 inference 时，model. eval () 是必须使用的，否则会影响结果准确性。 而 torch. no_grad () 并不是强制的，只影响运行效率。



**梯度消失和梯度爆炸？**
链式法则计算梯度时，下层梯度计算小于 1，经过层层累积，上层梯度越来越小，即上层权重更新速率低于下层权重更新速率，导致梯度消失，梯度爆炸则是上层权重更新速率高于下层权重更新速率。
产生原因：层数过多，激活函数不正确
解决：选择合适激活函数，BN/LN
[[梯度消失 & 梯度爆炸]]


**BN 和 LN 的区别？**


**强化学习在推荐、广告中的应用？**
购物想法是很容易改变的。假设用户在购物平台停留时间越长，则有可能消费的话，
总体上：监督学习可能希望推荐的每一个商品都停留时间长一点，通过贪婪思想，则整体停留的时间就多一些；强化学习优化目标关注的是整个停留时间，可能有些推荐的商品用户很快划走了，但是最终浏览的累积的时间变长了。所有优化目标就需要去设计相应的状态、动作、奖励。

**什么是 xgboost 算法**

### TCP 和 UDP
1、基于连接与无连接；
2、对系统资源的要求（TCP较多，UDP少）；
3、UDP程序结构较简单；
4、流模式与数据报模式 ；
5、TCP保证数据正确性，UDP可能丢包；
6、TCP 保证数据顺序，UDP 不保证。

### 字典、哈希表、红黑树 
字典由哈希表实现，哈希表是由 数组+链表+红黑树的形式的
散列表（Hash table，也叫哈希表），是根据关键码值(Key)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做**散列函数**，存放记录的数组叫做**散列表**。

哈希表的数据结构
```Python
class HashTable:
    def __init__(self, size):
	    # 使用list数据结构作为哈希表元素保存方法
        self.elem = [None for i in range(size)]  
        self.count = size  # 最大表长

    def hash(self, key):
	    # 散列函数采用除留余数法
        return key % self.count  

    def insert_hash(self, key, value):
        """插入关键字到哈希表内"""
        address = self.hash(key)  # 求散列地址
        while self.elem[address]:  # 当前位置已经有数据了，发生冲突。
            address = (address + 1) % self.count  # 线性探测下一地址是否可用
        self.elem[address] = value  # 没有冲突则直接保存。

    def search_hash(self, key):
        """查找关键字，返回布尔值"""
        star = address = self.hash(key)
        while self.elem[address] != key:
            address = (address + 1) % self.count
            if not self.elem[address] or address == star:  
            # 说明没找到或者循环到了开始的位置
                return False
        return True
```

大体流程：
- **定位**；确定 key 在数组中的对应位置；计算 key 值对应的 hashcode；再对该 hashcode 取一次 hash, 该值用来定位要将这个元素存放到数组中的什么位置。若不同key值对应不同散列地址（存储位置称散列地址），则直接存储即可，
- **冲突解决**：而不同的 key 值可能得到同一散列地址，这种现象称为冲突。如果冲突，则首先判断 value 值是否相同，如果两者相等则直接覆盖，如果不等则在原元素下面使用**链表**的结构存储该元素。
- 数组 + （链表/红黑树）：因为链表中元素太多的时候会影响查找效率，所以当链表的元素个数达到8并且数组长度超过64的时候使用链表存储就转变成了使用红黑树存储，原因就是**红黑树是平衡二叉树（自平衡二叉查找树）**，在查找性能方面比链表要高。
![](img/HashMap.png)
参考
- https://zhuanlan.zhihu.com/p/79507868
- https://www.jianshu.com/p/d04edc8aaf0f
- http://static.kancloud.cn/alex_wsc/java_source/1852265

**多线程和多进程的区别？：**
线程时进程的子集，一个进程有多个线程组成。
多进程数据是分开的，共享复杂，但是同步方便
多现成的共享进程数据，共享简单，但是同步复杂。




### 未知环境下基于强化学习的自主体智能决策与协同控制技术

基于 ROS 机器人操作系统，每个移动机器人装配一个激光雷达，通过一个分层的控制结构（决策层+路径规划层），完成未知环境多机器人的自主探索和建图。在决策模块，通过 Voronoi 的性质和选点公式，选出下一步每个智能体要前往目标点。然后路径规划层，依据每个智能体的目标点，通过深度强化学习方法引导智能体前往目标点。


#### 单智能体目标导航

**奖励函数**：
-   任务完成奖励：与目标点距离小于某个阈值时，则认为完成任务，获得一个较大正奖励
-   安全距离奖励：与障碍距离小于一定距离时，会获得一个负奖励，距离越近，负奖励越大
-   速度控制奖励：控制角速度不宜过大、控制线速度不要过小。
    
**状态空间**：
-   激光雷达距离信息
-   上一时刻速度
-   与目标位置的相对距离（距离+角度）

**动作空间**：
-   线速度 & 角速度
    
**难点**：
1.  如何和ROS环境进行交互
2.  如何尽可能避免重复探索：Voronoi、选点公式超参数选定


#### 多智能体选点

**状态空间**：
-   激光雷达距离信息
-   与上一时刻所选点的相对距离
-   与其他智能体的相对距离
    
**动作空间**：
-   \[0,1]的连续值，用于与可选目标点进行映射，比如0的话就对应第一个可选目标点，1的话就对应最后一个可选目标点。

**奖励空间**：
-   与上一时刻建图差值
-   任务完成奖励
-   内在奖励：当前所选的点与所有智能体的历史选点大于某个阈值则获得内在奖励，分两段，回合数大于3后奖励加倍。
-   惩罚：当前所选的点与其他智能体选择目标点接近、与自身历史目标点接近

**难点**：
- 如何避免重复选点

> Vorono分区的性质：每个移动机器人装配一个激光雷达，会发射多条具有一定长度的激光，如果激光与障碍物发生碰撞会得到相应的距离信息，没有发生碰撞的点就会返回原始长度，，根据激光雷达距离信息以及智能体所处的位置，可以将激光雷达的信息映射为位置点（选择无碰撞的激光雷达信息计算），对于其中任意一个位置点，如果该点到该移动机器人的距离要小于到其他移动机器人的距离，那么就将这个点加入该移动机器人可选目标点。然后可以交由选点公式//强化学习 进行进一步的筛选，最终确定该智能体要前往的目标点。

> 在选择接下来目标点时，之所以不固定每个激光的起始位置，是因为小车在移动的过程中，激光会根据小车的转向，会随时发生变化，如果固定每个激光位置，会发生都需要额外的转换，况且当前是360度的激光，如果小于360度时，情况处理更麻烦。因此我们通过actor神经网络输出动作后与可选目标点映射时，也是与小车的激光相对应的。
> 
> 之所以采用连续动作，是当激光雷达的数量增加时，采用离散动作时，动作空间就会比较大。

#### 基于深度强化学习的智能小车对战

基于Box 2d环境，根据实体小车，对仿真小车进行设计，每辆小车搭载一个固定激光发射装置和激光接收器。在1v1对战时，智能小车采用深度强化学习方法（TD3）进行训练，可以在无障碍物环境、有障碍环境、人机对战下完成测试；在2v1中，将1v1中训练较好的版本的actor网络通过迁移选择，作为MADDPG算法的起始actor网络，经测试，两个智能体可以旋转攻击，在避免队友伤害的同时攻击到敌方小车


**状态空间**：
- health —— 生命值
- pos —— 小车所处位置
- angle —— 小车当前转动的角度
- scan —— 通过射线投射检测发生碰撞物体的类型以及碰撞距离百分比 fraction
- detect —— 检测敌方小车
>与枪口位置(水平)成大约的范围检测到小车，并且两小车的距离小于4，则设为True
>检测范围一般大于射线投射的范围.

**动作设置**
- v_t —— 前后移动
- v_n —— 左右移动
- angular —— 左右旋转角度
- shoot —— 发射子弹
>只要detect到就可以发射子弹

**1v1 奖励函数**：
- 越远距离攻击到敌方小车获得的奖励越多：
$$  
    (Distance - 0.8)*0.1  
$$
> 小车的坐标是由小车的中心点确定的，小车的长度为0.6m x 0.42m 因此两辆小车接触在一起时他们之间的距离为0.42m或0.51m或0.6m
-   攻击敌方小车使其生命值降为0获得一个较大正奖励 （+3）
-   检测到敌方小车时会获得一个较小正奖励（+0.001）
-   两小车相撞获得一个负奖励 （ -0.01）
-   小车撞倒墙体获得一个负奖励（ -0.01）
-   敌方小车攻击智能体使其生命值降为0获得一个较大负奖励 （-3）

  
**2v1 奖励函数**
- 任何一个红色智能体攻击到敌方小车获得一个正奖励：
$$  
    (Distance - 0.4)*0.2  
$$
-   任何一个智能体攻击敌方小车使其生命值降为0获得一个较大正奖励 （+5）
-   任何一个智能体受到攻击都将获得一个较小正奖励（-0.1）
> 任何一个智能体受到的攻击可能来自于敌方小车也有可能来自于友方小车
-   任何一个智能体检测到敌方小车时会获得一个较小正奖励（+0.001）
-   任何一个智能体和其他任何两个小车相撞相撞获得一个负奖励 （ -0.01）
-   任何一个智能体撞到墙体都将获得一个负奖励（ -0.01）
-   任何一个智能体其生命值降为0都将获得一个较大负奖励 （-3）

#### 在轨维修精细操控机构设计

空间人工智能体控制中心的一个项目。航天机械臂的一个地面试验，主要的任务就是通过单个或两个机械臂协同合作完成一个精细维修任务。这个项目大体可以分为三个部分：1. 调研机械臂公司，通过Matlab机器人工具箱完成机械臂的空间仿真。2. 设计一个上位机系统，可以控制机械臂，并完成可视化的显示。3. 通过手眼相机，利用图像处理控制机械臂完成避障、协同控制、维修等任务。

#### 粒子群算法优化 BP 神经网络在边坡稳定性中的应用

神经网络的学习过程，就是从训练数据中自动获取最优权重参数的过程，学习的目的通常以损失函数为基准，找出使它的值达到最小的权重参数。为了尽可能找出可能小的损失函数的值，通常传统 BP 神经网络采用梯度下降的反向传播更新权重参数。在这里的话通过粒子群算法优化这些权重参数。将权重和偏置（位置）看作是一个粒子，通过多个粒子组成一个粒子群，通过速度、位置公式以及适应度函数（损失函数），不断更新粒子信息，每个粒子都向最优的粒子进行靠齐，经过不断迭代后，选择最优粒子的权重和偏置作为我们最终的神经网络。

> 将边坡土数据进行预处理（归一化、训练数据、测试数据）。

#### 基于 MDL 分割和粒子群优化的轨迹压缩算法

-   MDL分割算法：类似于数据压缩、近似、拟合，通过一定的判断条件，不断选择满足条件的分割点（垂直距离、角度距离）加入一个特征点集，通过这这些特征点集去完成对原始轨迹的近似。
    
-   改进的MDL分割算法：在MDL分割算法的基础上，增加了一个限制条件，引入了一个平行夹边的概念，作为进一步筛选分割点的条件。
    
-   基于粒子群优化的轨迹压缩算法：对原始轨迹中，重要的点进行提取，并实现轨迹的分割，轨迹压缩技术的主要目标就是不影响轨迹精度的条件下，减少轨迹数据的大小。
    
> 首先人为选定一个角度阈值和距离阈值范围，即在这个范围内找到能代价更小(TAD)的压缩轨迹去近似原轨迹；通过粒子群算法，通过不断调整角度阈值和距离阈值（这两个参数为一个粒子）的大小，得出最后使TAD值（适应度函数；代价函数）最小的那个角度阈值和距离阈值以及K值。
    

#### 谷歌街景门票图像(SVHN)识别

数据集：天池竞赛中谷歌街景图像中门牌号的数据集，大概有8w张左右的照片，大部分的字符都是2-4个字符左右。

难点：因为是变长字符，需要先将变长字符转变成定长字符来处理，字符长度不足的图像可以用“10”填充。

流程：

-  通过DataSet对数据进行封装，提供索引的方式对数据样本进行读取。DataLoder对Dataset进行封装，提供批量读取的迭代读取。
-   学习时：通过CNN/Restnet18 外接五个全连接层进行分类
    
    > 加入Dropout层进行防止过拟合的处理。
    
测试时，通过测试数据对测试。

#### 基于 Hadoop 的 K-means 算法的并行实现

基于 Hadoop 平台的 MapReduce 模型，在单机环境、伪分布式环境和完全分布式环境下对百万数据实现 K-means 算法，并结合旅游机票数据，分析不同城市游客对旅游城市的偏好，通过Python的Flash框架和百度地图API实现对相关数据的显示。

> K-means 算法的具体步骤包括:
> 
> -   根据特定的方法选择 K 个质心，作为初始的质心文件;
> -   计算每个样本点到质心的距离，并将其归到距离其最近的质心中; 
> -   当每个样本点都分配完成以后，重新计算 K 个质心的位置;
> -   重复步骤 2 和 3，直到所有的样本点不再被分配或是达到最大的迭代次数。
>     
> 在 Map 任务可以计算每个样本点到质心的距离，并进行归类;在 Reduce 任 务中对归类后的结果重新计算新的质心。