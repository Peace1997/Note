面试官您好，我是来自浙江理工大学，计算机技术专业的硕士马佩鑫 ，很高兴能参加此次（崭珀po科技）面试，我面试的岗位是自动驾驶决策规划算法工程师，我研究生期间主要的研究方向是多智能体深度强化学习方向和机器人决策规划方向，目前对决策、规划、运动控制都有一定的应用经验，在做未知环境下多机器人的协同自主探索、协同导航项目时，都有涉及决策规划的相关任务，在中国空间技术研究院（航天五院）杭州中心实习期间，独立负责将强化学习应用在四足机器人运动控制。我未来也是期望从事自动驾驶相关工作，以上是我的自我介绍情况。

***自动驾驶简述：***

基于我此前的调研，自动驾驶主要分为：感知定位、决策规划、控制执行。
自动驾驶一般指车辆利用车载传感器来感知车辆周围的环境，根据感知获得道路、车辆位置和障碍物信息，决定驾驶行为、路径规划、速度规划等问题，控制车辆的转向和速度，从而使车辆能够安全、可靠、舒适地在道路上行驶。

而对于决策模块通常包含三个层次：
- 全局路径规划
- 行为决策
- 运动规划
***行为决策的难点**

1. 首先，真实的驾驶场景千变万化，如何覆盖？

2. 其次，真实的驾驶场景是一个多智能体决策环境，包括主车在内的每一个参与者所做出的行为，都会对环境中的其他参与者带来影响，因此我们需要对环境中其他参与者的行为进行预测；如何更加准确的预测其他参与者的行为或行驶轨迹。

3. 自动驾驶车辆对于环境信息不可能做到100%的 感知，例如存在许多被障碍物遮挡的可能危险情形。这时如何决策。

综合以上几点，在自动驾驶行为决策层，我们需要解决的是在**多智能体决策的复杂环境中，存在感知不确定性情况的规划问题**。可以说这一难题是真正实现 L4、L5 级别 自动驾驶技术的核心瓶颈之一，近年来随着 深度强化学习等领域的快速发展，为解决这一问题带来了新的思路和曙光。

***轨迹预测***

轨迹预测可分为短期预测与长期预测。

1.  **短期预测**一般根据运动学模型(CV/CA/CTRV/CTRA)基于当前的目标state信息预测未来一段时间的轨迹，一般<1s是合适的，如果时间过长，那目标仅与运动学相关的假设就不成立了。短期预测可以建一个运动模型专门去预测，同样的也可以使用前面感知模块滤波中的预测模块，只不过不调用测量进行滤波更新，这样的好处是可以传播不确定度。
2.  **长期预测**是当前业界主要在做的。这种预测仅基于运动模型就不合适了，一般需要做意图预测，并结合一些上下文信息(地图、目标间交互信息)才能得到不错的结果。此时业界有很多不同的输出形式，比如输出轨迹的概率分布、输出多条预测轨迹、输出一条可能性最大的预测轨迹。
>**Uber**：LaneRCNN[5]、**Google**：VectorNet[6]、**美团**：StarNet[12]。行人。


## 强化学习
**简介**：
强化学习通常处理的是序列决策问题，它既没有非常准确的监督信号，也不完全是无监督地发现数据中潜在结构，它通过不断与环境交互去学习一系列的决策，使得模型最终能在环境中获得最大的收益。强化学习与深度学习相结合，更好地增强智能体决策能力，
**应用**：
目前强化学习主要广泛应用场景是游戏和机器人，在其他领域也有落地应用，例如推荐系统、金融交易、无人驾驶、 交通控制、自动调参等需要决策的经常。
**问题与挑战**：
- 样本利用率低
- 奖赏函数难以设计和推广
- 实验效果难以复现
**潜力方向**：
- 提出新的有模型的算法，以及尝试搜索与监督的方法，以提升样本的利用效率；
- 迁移学习、元学习、引入 DRL 中，适用多个任务，利用之前任务经验，加速训练，快速适应新任务。
- 模仿学习、分层强化学习，组合多个子任务形成有效的全局策略，加入人类经验，使得模型更可控。
- 将博弈论和多智能体引入 DRL，以适应真实环境中更加复杂的问题。

**自动驾驶简述：**
自动驾驶一般指车辆利用车载传感器来感知车辆周围的环境，根据感知获得道路、车辆位置和障碍物信息，控制车辆的转向和速度，从而使车辆能够安全、可靠地在道路上行驶，具体包括感知、定位、决策、控制等多个关联子系统。


MDP  & POMDP
如果序列决策过程所有状态都具有**Markov性**，且环境状态是**完全可观测的**，则称该过程为**Markov决策过程**。如果环境状态是**部分可观测**的，则称其为**部分可观测Markov决策过程**。

==**马尔可夫性质（Markov property）是指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态