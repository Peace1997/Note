
回归问题用于预测输入变量（自变量）和输出变量（因变量）之间的关系。
回归模型

# 常见问题

***线性回归的目的是什么？***
线性回归的目的 就是找到和样本**拟合程度最佳**的线性模型，即找到了最优的系数向量和干扰项就找到了最优模型。

***逻辑回归的目的是什么***
利用线性模型解决分类问题

***逻辑回归的“Logistic”应该怎么解释？***
并非逻辑的意思，其语义来自Logarithm：对数，恰当的中文名称：**对数几率回归**，或逻辑斯谛回归

***为什么逻辑回归是分类算法***



***线性回归和逻辑回归的区别与联系***
-   线性回归和逻辑回归都是**广义线性回归模型的特例**
-   线性回归只能用于**回归问题**，逻辑回归用于**分类问题**（可由二分类推广至多分类）
-   线性回归无激活函数或不起作用，逻辑回归的激活函数是**对数几率函数**，属于Sigmoid函数
-   线性回归使用**最小二乘法**作为参数估计方法，逻辑回归使用**极大似然法**作为参数估计方法

***逻辑回归中的连续特征为什么要离散化处理？***

模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了

工业界很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列 0、1 特征交给逻辑回归模型

- 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为 N 个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
- 离散化后的特征对异常数据有很强的鲁棒性。
- 特征离散化后，模型会更稳定，特征精度过高也可能是噪声；
- 离散特征的增加和减少都很容易，易于模型的快速迭代；
- 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

***线性回归、对数回归有什么联系***

显然，前面的线性回归和对数回归都是**广义线性回归的特例**，根据联系函数的不同，以不同的方式映射，可以是对数，可以是指数，也可以是其他更复杂的函数。


# 一、线形回归
Linear Regression

## 1. 问题建模

*线性回归模型*

假设因变量和自变量之间是线性关系的，可以通过一条直线表示。
在有监督学习中，一份数据集，由一列观测（**y**，即因变量）和多列特征（**X**，即自变量）组成，转换成**数学模型**就是：
$$
y=X \beta+\varepsilon
$$

*参数描述*
因变量与自变量一一对应，一个自变量的值可能包含 p 个特征。
$$
y=\left(\begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{array}\right) \quad, \quad X=\left(\begin{array}{c}
X_1^T \\
X_2^T \\
\vdots \\
X_n^T
\end{array}\right)=\left(\begin{array}{cccc}
1 & x_{11} & \cdots & x_{1 p} \\
1 & x_{21} & \cdots & x_{2 p} \\
\vdots & \vdots & \ddots & \vdots \\
1 & x_{n 1} & \cdots & x_{n p}
\end{array}\right)
$$
$\beta$ 是系数向量， $\varepsilon$ 是干扰项（disturbance term），或称错误项（error term）:
$$
\beta=\left(\begin{array}{c}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p
\end{array}\right) \quad, \quad \varepsilon=\left(\begin{array}{c}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{array}\right)
$$
第 i 个 y（观测值）是这样的：
$$
y_i=\beta_0 1+\beta_1 x_{i 1}+\cdots+\beta_p x_{i p}+\varepsilon_i\left(i=1, \cdots, n, \beta_0 \text { 为截距 }\right)
$$

## 2. 模型求解

既然我们想找到一个最优的模型来描述数据，有两个问题需要解决，**如何定义最优**、**如何寻找最优**；
定义最优：就是如何设置合适的**损失函数**
寻找最优：选择合适的方法进行**参数估计**

### 2.1 定义最优

想要评价一个模型的优良，就需要一个**度量标准**。 
对于回归问题，最常用的度量标准就是**均方差**, 即预测值与实际值之间的平方差。
$$
\left(\beta^*, \varepsilon^*\right)=\operatorname{argmin} \sum_{i=1}^m\left(f\left(x_i\right)-y_i\right)^2
$$
即找到了最优的系数向量 $\beta^*$ 和干扰项 $\varepsilon^*$ 就找到了最优模型。

### 2.2 寻找最优

寻找最优就是如何找到这个直线，一般有两种方法，最小二乘法和梯度下降法；

#### 最小二乘法
寻找一条直线，使得样本点和直线的欧式距离之和最小。
简单描述为：根据凸函数的性质，求解所有关于$\beta$和$\varepsilon$的二阶导的零点，选择使目标函数（均方差）最小的零点作为目标值，即求导找极值，选最小极值。

#### 梯度下降
梯度下降法，沿着梯度方向进行寻优，通过学习率控制学习步幅。


# 二、 广义线性回归

*线形回归模型 & 对数线性回归模型*
数学上的一个特例经常归属于一个更普遍或更一般的原型。
$$
y=\beta X+\varepsilon \quad, \quad \ln y=\beta X+\varepsilon
$$
左边是我们之前得到的线性回归模型，右边是**对数线性回归模型**（log-Linear Regression）

对数线性回归与线性回归区别仅仅在于等式左部，形式依旧是线性回归，但实质上是完成了输入空间 X 到输出空间 y 的**非线性映射**。

>原本线性回归模型无法描述的非线性 y，套上了一个非线性函数 ln (·)，就可以描述对数形式的 y 了，即将线性回归模型和真实观测**联系**起来。

***广义线性回归模型***
综合以上两个式子综合，写成更一般的形式就是**广义线性回归模型**（GLM，Generalized Linear Model）了：
$$
y=g^{-1}(\beta X+\varepsilon)
$$
这里的g(·)，即**ln(·)**，是一个单调可微函数，称为**联系函数 或 激活函数**


显然，*前面的线性回归和对数回归都是广义线性回归的特例*，根据联系函数的不同，以不同的方式映射，可以是对数，可以是指数，也可以是其他更复杂的函数。


# 三、逻辑回归
Logistic Regression

## 1. 问题建模

问题解决：**将线性回归应用到分类问题**。 虽然被称为回归，但其实际上是分类模型，并常用于二分类。

逻辑回归的本质是：假设数据服从这个分布，然后使用极大似然估计做参数的估计。？

### 1.1 逻辑分布
Logistic 分布是一种连续型的概率分布, 其分布函数和密度函数分别为:
$$
\begin{gathered}
F(x)=P(X \leq x)=\frac{1}{1+e^{-(x-\mu) / \gamma}} \\
f(x)=F^{\prime}(X \leq x)=\frac{e^{-(x-\mu) / \gamma}}{\gamma\left(1+e^{-(x-\mu) / \gamma}\right)^2}
\end{gathered}
$$
![500](LR2.png)
左为密度函数，右为分布函数

逻辑分布的形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。

在深度学习中常用到的 **Sigmoid 函数**就是 逻辑分布 的分布函数在 $\mu$ =0，$\gamma$ =1 的特殊形式。

### 1.2 逻辑回归模型

逻辑回归主要用于分类问题，我们以二分类为例，对于所给数据集假设存在这样的一条直线可以将数据完成**线性可分**。

对于随机变量 X 取值为实数，随机变量 Y 取值为 1 或 0。我们通过监督学习的方法来估计模型参数

二项逻辑回归模型是一个条件概率分布
$$
\begin{aligned}
&\mathrm{P}(\mathrm{Y}=1 \mid \mathrm{x})=\frac{\exp (\mathrm{w} \cdot \mathrm{x}+\mathrm{b})}{1+\exp (\mathrm{w} \cdot \mathrm{x}+\mathrm{b})} \\
&\mathrm{P}(\mathrm{Y}=0 \mid \mathrm{x})=\frac{1}{1+\exp (\mathrm{w} \cdot \mathrm{x}+\mathrm{b})}
\end{aligned}
$$
逻辑回归通过**比较两个条件概率值大小**，将实例 x 分到概率值较大的那一类。

线性函数的值越接近正无穷，概率值越接近于1；线性函数的值越接近负无穷，概率值越接近0。

#### 激活函数的选择

我们知道，线性回归本身的输出是连续的，也就是说要将连续的值分为离散的 0 和 1。答案很容易想到，找到一个联系函数，将 X 映射到 y∈{0，1}——单位**阶跃函数**。

通过阶跃函数虽然可以对输出的 y 值进行在分类，但一个原则性问题，联系函数必须是**单调可微**的函数，也就说必须是**连续**的。那应该选用哪个联系函数呢。

> 阶跃函数只能返回 0 或 1，而我们希望神经网络中流动的是连续的实数信号

逻辑回归使用的联系函数是 Sigmoid 函数（S 形函数）中的最佳代表，即**对数几率函数**（Logistic Function），其整体构型是与阶跃函数相似的,比阶跃函数更具平滑性。
![400](LR1.png)

    >补：激活函数的选择要求
    >1. 激活函数首先要使用非线性函数，如果使用线性函数的话，加深神经网络的层数就没有意义了
    >2. 激活函数单调可微，阶跃函数虽然也属于非线性函数，但是其不可微。


对于二分类常采用sigmoid激活函数（基于线性模型），而对于多分类问题，我们常采用激活函数是softmax（基于对数线性模型），所有输出概率加和为1。

*Sigmoid 函数原型*

函数原型如下:
$$
y=\frac{1}{\left(1+e^{-z}\right)}
$$
为什么叫对数几率呢, 因为它本来是长这样子的:
$$
\ln \frac{y}{1-y}=z
$$
式子中的 $\frac{y}{1-y}$ 就是所谓的几率 (odds)

>Odds（几率）和 Probability（概率）之间是有区别的 
>- Probability 是指，期望的结果/所有可能出现的结果  
>- Odds是指，期望的结果/不期望出现的结果

至于为什么要使用Sigmoid函数中的对数几率函数，这涉及到伯努利分布的指数族形式，最大熵理论等

#### 逻辑回归的数学原型
$$
y=\frac{1}{1+e^{-(\beta X+\varepsilon)}}
$$

逻辑回归也是广义线性回归中的一种以对数几率函数为联系函数的**特例**。

 Logistic 回归实际上是使用线性回归模型的预测值逼近分类任务真实标记的对数几率

## 2. 模型求解

既然有了数学模型，我们就需要通过两步走（定义最优、寻找最优参数）策略，对模型进行求解了，其优点有：
1.  直接对**分类的概率**建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题（区别于生成式模型）；
2.  不仅可预测出类别，还能得到该**预测的概率**，这对一些利用概率辅助决策的任务很有用；
3.  对数几率函数是**任意阶可导的凸函数**，有许多数值优化算法都可以求出最优解。


### 2.1 定义最优
交叉熵作为我们的损失函数，
[[常用解释#交叉熵]]

与线性回归不同的是，逻辑回归由于其激活函数为非线性函数，参数估计方法不再使用最小二乘法，而是极大似然估计。
最小二乘法是**最小化预测和实际之间的欧氏距离**，极大似然法的思想也是如出一辙的，但是它是通过**最大化预测属于实际的概率**来最小化预测和实际之间的“距离”

逻辑回归模型的数学形式确定后，剩下就是如何去求解模型中的参数。在统计学中，常常使用极大似然估计法来求解，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）最大。
$$
\begin{gathered}
P(Y=1 \mid x)=p(x) \\
P(Y=0 \mid x)=1-p(x) \\
\end{gathered}
$$
似然函数：
$$
L(w)=\prod\left[p\left(x_i\right)\right]^{y_i}\left[1-p\left(x_i\right)\right]^{1-y_i}
$$
为了更方便求解，我们对等式两边同取对数，写成对数似然函数：
$$
\begin{aligned}
L(w) &=\sum\left[y_i \ln p\left(x_i\right)+\left(1-y_i\right) \ln \left(1-p\left(x_i\right)\right)\right] \\
&=\sum\left[y_i \ln \frac{p\left(x_i\right)}{1-p\left(x_i\right)}+\ln \left(1-p\left(x_i\right)\right)\right] \\
&=\sum\left[y_i\left(w \cdot x_i\right)-\ln \left(1+e^{w \cdot x_i}\right)\right]
\end{aligned}
$$
对 $L(w)$ 求极大值，得到 $w$ 的估计值
在机器学习中我们有**损失函数**的概念，其衡量的是模型预测错误的程度。如果取整个数据集上的平均对数似然损失，我们可以得到:
$$
J(w)=-\frac{1}{N} \ln L(w)
$$

**最大化似然函数**和**最小化损失函数（交叉熵）** 实际上是等价的。
使用交叉熵误差作为 softmax函数的损失函数后，反向传播得到 （y1 − t1, y2 − t2, y3 − t3）

### 2.2 寻找最优


#### 梯度下降
梯度下降是通过 $\mathrm{J}(\mathrm{w})$ 对 $\mathrm{w}$ 的一阶导数来找下降方向, 并且以迭代的方式来更新参数, 更新方式为 :
$$
\begin{aligned}
&g_i=\frac{\partial J(w)}{\partial w_i}=\left(p\left(x_i\right)-y_i\right) x_i \\
&w_i^{k+1}=w_i^k-\alpha g_i
\end{aligned}
$$
其中 $\mathrm{k}$ 为迭代次数。每次更新参数后, 可以通过比较 $\left\|J\left(w^{k+1}\right)-J\left(w^k\right)\right\|$ 小于阈值或者到达最 大迭代次数来停止迭代。


#### 牛顿法

牛顿法的基本思路是, 在现有极小点估计值的附近对 $f(x)$ 做二阶泰勒展开, 进而找到极小点的下 一个估计值。假设 $w^k$ 为当前的极小值估计值，那么有：
$$
\varphi(w)=J\left(w^k\right)+J^{\prime}\left(w^k\right)\left(w-w^k\right)+\frac{1}{2} J^{\prime \prime}\left(w^k\right)\left(w-w^k\right)^2
$$
然后令 $\varphi^{\prime}(w)=0$, 得到了 $w^{k+1}=w^k-\frac{J^{\prime}\left(w^k\right)}{J^{\prime \prime}\left(w^k\right)}$ 。因此有迭代更新式:
$$
w^{k+1}=w^k-\frac{J^{\prime}\left(w^k\right)}{J^{\prime \prime}\left(w^k\right)}=w^k-H_k^{-1} \cdot g_k
$$
其中 $H_k^{-1}$ 为海森矩阵:
$$
H_{m n}=\frac{\partial^2 J(w)}{\partial w_m \partial w_n}=h_w\left(x^{(i)}\right)\left(1-p_w\left(x^{(i)}\right)\right) x_m^{(i)} x_n^{(i)}
$$
此外, 这个方法需要目标函数是二阶连续可微的, 本文中的 $\mathrm{J}(\mathrm{w})$ 是符合要求的。


