# Introduction

***什么是生成对抗网络？***

生成对抗神经网络是一种深度学习模型，用于**生成新的样本**。它由两个网络组成：**生成器**（generator）和**判别器**（discriminator）。

生成器用于生成新的样本，判别器用于识别真实样本和生成样本。两个神经网络并非合作关系而是一种博弈与对抗的关系。在训练过程，生成器和判别器模型不断学习，最终达到一个平衡点，最终生成模型能够生成以假乱真的数据，让判别模型无法区分是生成的数据还是真实的样本，判别器也可以识别出大部分生成样本，


***GAN 与 Actor-Critic 的区别***

- Actor-Critic 网络模型，Actor 产生动作，Critic 网络对 Actor 产生的动作进行评价，两者是一种合作关系。GAN 中的两个网络生成器和判别器，是一种对抗关系。
- AC网络模型主要用于解决决策问题，GAN主要用于生成样本（）



*生成对抗神经网络结构*

包含一个生成模型和一个判别模型
- **生成模型**：生成与样本分布一致的数据，目标是欺骗判别模型，让判别模型认为生成的数据是真实的。（输入低维度的随机噪声（向量），输出的是高维度的张量）
- **判别模型**：试图将生成的数据与真实的样本区分开。（）


*生成抗神经网络的应用？*

由于 GAN 能生成复杂的高维度数据，因此常应用于：
- 图像处理：图像合成、图像转换、图像超分辨率、对象检测
- 序列数据生成：音乐生成、语音生成
- 半监督学习
- 域自适应


*生成模型的数据生成方式*

生成模型希望机器学习大量的训练数据，从而具备产生同类型新数据的能力。

方法；
- 自动编码器
- 变自动编码器（VAE）


**自动编码器**（Auto-Encoder）：它是一个多层感知器的神经网络，包含两个部分：编码器和解码器
- 编码器：数据压缩，生成隐含编码
- 解码器：压缩数据还原成原始数据

缺点：不能通过新编码生成数据。

**变自动编码器**（Variational Auto-Encoder）：在自动编码器基础上加入了限制，要求产生的隐含向量能够遵循高斯分布，从而帮助自动编码器理解训练数据的潜在规律，从而学习到输入数据的隐含变量模型。
- 编码器：生成隐含变量模型（均值向量+标准差向量），对这个概率模型随机采样，生成隐含编码向量。
- 解码器：将采样获得的隐含编码向量换元成原始数据

评价标准：
- 网络整体准确程度：输出数据与输入数据之间的均方距离
- 隐含变量是否可以很好逼近高斯分布：隐含变量与高斯分布相近程度（KL 散度）

缺点：VAE更倾向于生成与真实数据更为接近的数据，而不是学会如何生成数据。




Floyd 深度学习云平台


# Modeling

## 目标

GAN 的目标是优化以下函数：
$$\min_G\max_DV(D,G)=\mathbb{E}_{x\sim p_{\mathrm{dan}}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z)))]$$
- 判别器$D$的目标是最大化这个函数：区分真实样本$x$和生成样本$G(z)$。·
- 生成器$G$的目标是最小化这个函数：欺骗判别器，使$D(G(z))$接近真实。

GAN 目标是一个**极小极大（min-max）博弈问题**，生成器和判别器的目标函数彼此对抗，导致整体优化函数的复杂性。

## JS 散度
[[常用解释#JS 散度]]
在理论上，GAN 的目标可以解释为优化$P_\mathrm{data}$和$P_g$的 JS 散度，
$$D_{\mathrm{JS}}(P_{\mathrm{data}}\|P_{g})=\frac{1}{2}D_{\mathrm{KL}}(P_{\mathrm{data}}\|M)+\frac{1}{2}D_{\mathrm{KL}}(P_{g}\|M)$$

其中$M=\frac12(P_{\mathrm{data}}+P_{g}).$


***GAN 优化目标的核心是最小化$D_\mathrm{JS}(P_\mathrm{data}\|P_g)$：***
- 判别器通过最大化自身目标$V(D,G)$, 在固定$P_g$ 时，学习$P_\mathrm{data}$ 和 $P_{g}$ 的差异，并达到最优状态$D^*(x)$。
$$D^*(x)=\frac{P_\text{data}(x)}{P_\text{data}(x)+P_g(x)}$$
-  在判别器达到最优$D^*$的条件下，GAN 的目标函数变为：
$$V(D^*,G)=-\log(4)+2\cdot D_{\mathrm{JS}}(P_{\mathrm{data}}\|P_g)$$
- 因此，GAN 的生成器在训练过程中，实际上是在试图使生成分布$P_{g}$与真实数据分布$P_\mathrm{data}$的 JS 散度$D_\mathrm{JS}(P_\mathrm{data}\|P_g)$最小化。


***JS 散度在 GAN 中有以下几个主要问题：***
1. **对不重叠区域不敏感**：当$P_\mathrm{data}$ 和$P_g$ 没有重叠时，JS 散度的值为最大常数，无法反映两者之间的真实差异。
2. **梯度消失问题**：当$P_g$远离$P_\mathrm{data}$时，生成器的梯度非常小，导致生成器难以有效学习。
3. **判别器过强时的不稳定性**：如果判别器非常接近最优，会将生成器的梯度压得很小，进一步加剧梯度消失问题。

## 问题

### 不稳定性

判别器和生成器的相互博弈会形成动态优化问题，很难保证收敛。

**判别器过强**：
如果判别器在训练初期很快学会区分真实样本和生成样本，那么生成器的梯度会变得非常小（梯度消失）。
- 原因是当 $D(G(z)) \approx 0$ 时，生成器的损失函数的梯度接近于零，导致生成器无法改进
**判别器过弱**：
如果判别器无法有效区分真实样本和生成样本，生成器无法获得准确的**梯度信号**。结果是生成器的更新方向可能不正确，无法逐步生成更逼真的样本。


### 模式崩溃

生成器无法很好地捕捉真实数据分布的多样性，仅生成少量有限模式的样本（即分布的一个或几个高概率区域）。这种现象导致生成的样本缺乏多样性，与真实数据分布不一致。

***产生原因***：
由于**判别器并没有强制生成器去探索真实数据分布中的多样性**，生成器通过取巧的方式获得判别器较高的分数，则生成器只能生成真实数据分布中一小部分模式，从而忽略真实样本中的其他模式数据，从而在生成器更新时，只学习特定模型（取巧的）的梯度信息，从而最终导致生成器逐渐收敛于某个模式。

***解决方法***
使用改进的损失函数
- **WGAN (Wasserstein GAN)**：用 Wasserstein 距离替代 JS 散度，使得分布之间的差异能够以更平滑的方式反映到生成器的梯度中，从而促进生成器捕获更多模式。
- **Least Squares GAN (LSGAN)**：使用均方误差代替对数损失函数，平滑梯度并提高训练稳定性。
