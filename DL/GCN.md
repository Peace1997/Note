
图卷积网络（GCN，Graph Convolutional Network）是一种专门处理**图结构**的神经网络模型，GCN 就是将卷积思想推广到图结构上的一种方法，通过聚合邻居节点的信息来更新每个节点的表示。

***为什么要采用 GCN？***
传统的 CNN 只能处理欧式结构数据（图像、序列），但现实很多数据是图结构的，图数据的最大特点是：不规则结构、邻接关系复杂，不能直接采用传统的卷积操作。
> 例如人类和人形机器人的骨骼结构天然地可以表示为图，其中关节可以看作是图的节点，骨骼连接/连杆可以被看作是图的边。



我们一起将 GCN 的核心公式梳理一遍吧。

## GCN 层

一个 GCN 层通过以下三个主要步骤来转换节点的特征：

1. **添加自环（Self-Loop）：** GCN 首先通过为每个节点添加**自环**来修改图的**邻接矩阵 (A)**。我们通过将单位矩阵 (I) 添加到 A 中，得到一个新的矩阵 $\widetilde{A} = A+I$。这样做是为了确保在计算节点的新特征时，不仅包括其邻居的特征，也包括其自身的特征。

2. **对称归一化（Symmetric Normalization）：** 为了防止拥有大量邻居的节点主导聚合过程，GCN 采用了一种特殊的归一化方法。它使用$\widetilde{A}$的**度矩阵**$\widetilde{D}$ 来归一化邻接矩阵。最终的归一化矩阵是 $\widetilde{D}^{-\frac{1}{2}} \widetilde{A} \widetilde{D}^{-\frac{1}{2}}$。这种对称形式至关重要，因为它能稳定学习过程，并提供了一种数学上严谨的方法，通过图的**特征向量**和**特征值**来分析图的结构。
    
3. **特征转换和激活：** GCN 层通过将归一化后的邻接矩阵与输入特征矩阵 (H (l)) 和一个可学习的权重矩阵 (W (l)) 相乘，来聚合特征并学习新的表示。然后，这个结果会通过一个激活函数 (σ)（如 ReLU）。一个层的完整过程可以用以下公式表示：

$$H^{(l+1)}=\sigma(\widetilde{D}^{-\frac{1}{2}} \widetilde{A} \widetilde{D}^{-\frac{1}{2}}H^{(l)}W(l))$$
### 对称性约束

对称性约束是一种限制行为的方式，通过构造一个对称矩阵来表示图，我们给图的数学性质施加了严格的限制，从而使得 GCN 的学习过程更加稳定、高效和可解释。

- **梯度稳定：** 在训练神经网络时，我们使用梯度下降来更新权重。如果我们的矩阵是对称的，它的特征值都是实数（使用度矩阵的逆进行限制在 0-1 之间），这能确保在反向传播计算梯度时，不会出现爆炸性增长或消失的情况。这让整个学习过程变得更加稳定和可控。
    
- **过滤噪声：** 你可以把每个**特征向量**想象成一个“滤波器”，它专门捕捉图中的某种**特定模式**。这些模式可以是局部的小结构，也可以是全局的大趋势。如果我们的特征向量是**正交**的，它们就能更好地独立捕捉这些模式，而不会互相干扰。这就像我们有一套非常精准的滤镜，可以有效地从图数据中分离出有用的信号，并过滤掉无用的噪声。