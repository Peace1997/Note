# 具身通用智能
2024/1/02  
#具身智能 #零样本学习
- https://swarma.org/?p=47084

## 具身智能
**具身智能**，通俗来说，是指研究在环境中具有实体的智能体（如现实或仿真环境下的机器人，能够直接与环境进行物理交互）如何通过与环境的交互来取得认知能力，学习并掌握新技能新知识的一个人工智能的子领域。

在语言任务中存在某种结构对于许多其他类别的任务也能通用，即不同数据形式（视觉，蛋白质折叠，数值计算）具备一定的同构性，近期的具身智能大模型 (foundation model) 其实也是沿着这个方向进一步地规模化。

在深度学习范式下，具身智能的研究主要集中在模仿学习(Imitation Learning)和强化学习 (Reinforcement Learning, RL) 两大块。**模仿学习**通过采集特定任务的轨迹数据集并用深度神经网络来拟合状态(state)或观测(observation) 的时间序列到动作 (action) 的映射来实现技能的学习，一般来说*数据采集成本较高*。**强化学习**则是通过让智能体与环境直接交互，在交互的过程中优化预先定义好的与特定任务相关的奖励函数 (reward function) 来学习新技能，一般来说设计奖励函数需要反复迭代，且强化学习的样本效率 (sample-efficiency) 相比于模仿学习来说会低得多。

*目前没有证据表明基于强化学习的方法训练出的智能体能涌现出对其所解决的任务和环境的认知能力*，比如训练机械狗时，训练需要频繁地人为重置机械狗的位置，因为机械狗的奖励函数只鼓励它向前走，即使碰到墙也会反复向前冲撞。同时，*没有任何证据表明目前基于模仿学习的方法能够通过大规模的预训练涌现出训练集中从未出现过的技能。

>在早期具身智能的研究中使智能体学到的策略泛化到与训练数据非常相似的任务是一件非常困难的事情，神经网络并没有数据来学习到这两者在“更加抽象的层面上是相类似的”。因此**一个巨大的挑战是任务数量的组合爆炸**。

### 具身智能研究差距
- 为方便实现，机器人具体形态可能会被人为抽象，使模型不在关注机器人具体形态。
- 目前的**视觉自监督学习** (vision self-supervised learning) 还没能学习到对于世界的结构化表征，不具备足够视觉认知能力。

#视觉自监督学习
>视觉自监督学习是一种机器学习方法，旨在通过利用图像或视频数据的内在结构进行训练，而无需人工标记的监督信号。


### 解决强化学习低效学习方法
强化学习 (尤其是无模型强化学习，model-free RL) 由于无法直接获取环境动力学的梯度信息，在样本效率方面往往比模仿学习低几个量级，对于学习在物理环境中运行的策略，这样低效的学习方式是不现实的。一般来说有两种解决方案：

- 一种是通过构建一个与**真实环境类似的模拟器**，在模拟器中使用大量数据学习到一个策略，然后在真实环境中零样本泛化或在线微调；
- 另一种方案是**学习一个关于环境的模型**，并利用学到的模型来生成学习数据 ，从而极大得减少对真实环境数据的需求，基于这一想法，通过与真实物理世界进行1小时的交互就能让机械狗学会走路并抵抗外界的干扰。


### 基于深度学习范式来实现通用具身智能所面临的一些根本性挑战
- **目前的学习系统本质上仍是一个开环系统，需要人类智能的介入**(如根据学习结果，有针对性地采集更多更好的数据，调整数据的概率分布，反复迭代优化奖励函数等)**来实现闭环**，用Yann Lecun的话来说就是，目前的机器学习系统是Assisted Intelligence，而实现通用具身智能需要的是Autonomous Intelligence [10]; 
- **目前的方法还不具备从自然模态中学习到关于世界的结构化表征与抽象**(或者说世界模型)**的能力**，相对地，人类和动物在婴儿时期就能从自然模态（如视觉，听觉等直接来源于外部世界的信号）中学习并基于直觉理解物理世界的结构和运作规律(intuitive physics )，这种自然习得的认知能力是实现通用具身智能的关键。


### 实现通用具身智能的关键
目前的方法构建学习系统面临一个非常困难的实际问题：**任务指定问题**（其本质也是某种对齐问题），通俗来讲就是说想要训练一个模型来精确地完成一项人类工程师心里希望其完成的任务，是一件极其困难的事。**我们希望机器解决的任务和我们对任务的数学描述之间存在差异性**。

实现通用具身智能的一个关键问题，是如何使机器学习系统从自然模态中（如视觉，听觉）学习到关于世界的层级化抽象（或者说是世界模型，认知地图），即**如何构建一个机器学习系统使其能从自然模态学习到世界模型**。

（1）认知能力的自然涌现
感知和认知是一整体，要完美解决感知问题必然涉及到认知，反之亦然，并且认知能力并不是凭空产生的。因此合理的做法并不是通过将认知模块直接设计进智能系统，而是在同一个框架下通过**对更低层级的感知任务进行优化**，使得认知能力自然地涌现出来，这看起来也是人类和动物学习认知世界的方式。

（2）计算可行性
提出的学习方式应当是实际可行的，其中一个重要指标就是**计算可行性**(computational tractability)，**世界模型让智能体以计算可行的（computationally-tractable）方式对未来进行长期预测和规划。** 这种在**合适粗粒度**上进行规划的能力使得我们可以利用有限的计算资源有效地寻找到一个高度优化的方案，这对时间和空间上的长远期规划是至关重要的。

（3）反事实推理
人类和动物学习技能相比于目前的人工智能系统具有高得多的样本效率，其中一个很重要的原因应该是我们能够通过世界模型构想出行为和结果之间的对应关系（即因果关系）。当我们重复一个任务，我们会不断做**反事实推理**来反思中间的哪些步骤可以做得更好，这个循环往复的过程就是我们学习新技能的过程。

世界模型包含了事件间的因果关系（一种特殊的对时序的抽象），使得智能体能够进行反事实推理（counterfactual reasoning），并据此高效且自动地学习新知识和新技能。

>反事实推理是一种逻辑推理方式，它从一个已知的假设出发，假设该假设不成立，然后推断由此产生的结果。已经发生的事件或情况，但如果某些条件发生了变化，会导致结果的不同。

### 实现通用具身智能的关键
**如何构建一个机器学习系统使其能从自然模态学习到世界模型**

*（1）神经网络架构*
大脑是一个具有记忆和反馈连接的动力系统，且时刻都在预测感官信号并纠正其预测，这种层级化的预测编码架构(predictive-coding)连接了感知与认知。我们第一视角能感知到的关于世界的一切信息都是存在于大脑状态中的信息流。相比之下，当今主流的**人工神经网络架构是前馈的（feedforward），没有状态或信息反馈流**，这种架构似乎不太可能编码表征世界的状态信息。
**在多个尺度上的信息反馈循环**（feedback loop）**可能对autonomous intelligence的涌现至关重要**，而这在当前系统中是不具备的。
>transformer的上下文学习能力与其前向展开（forward pass）过程中的记忆容量（或等效于中间状态的大小）存在因果关系，而不是与模型参数的数量。记忆容量对于通用学习能力的涌现起着至关重要的作用，而如何有效地**带有记忆和反馈链接**的神经网络架构是目前还未解决的问题，其本质是一个极具挑战性的temporal credit assignment问题(即如何衡量当前时刻的某一变化对未来某一时刻结果的影响，在神经网络训练的语境下对应于根据未来某个目标函数的值如何反推以调整神经网络的参数；目前深度学习中最主流的反向传播算法就是一种credit assignment机制)

*（2）学习法则*
大脑的学习机制很有可能是局部（local）的，局部学习机制的好处在于不需要同步全局的信息，并且对噪声（大脑作为一个物理系统）有更好的鲁棒性。
局部学习规则有望解决当前AI系统与自然智能在学习方面的一些显著差别：
1）**训练和推理阶段互相分离**，在训练过程中，权重通过随机梯度下降（SGD）的反向传播进行更新，而在推理阶段通常保持不变；
2）基于统计学习的方法通常假设环境的**分布是静态的**（stationarity）（比如训练是在randomly shuffle过的封闭数据集上进行的），这一假定对于自然数据流几乎不可能得到满足；
3）目前的学习方法对于非静态的环境/任务分布无法实现**持续终身学习**（continual lifelong learning），而会碰到灾难性遗忘（catastrophic forgetting）的问题。而对于通用具身智能来说，具备持续学习能力至关重要，因为世界是不断演化的，通用智能体只有通过不断改变自己内部的状态来适应变化的环境，才能确保其能力的通用性。

*（3）目标函数*
假设世界模型的学习可以归结为优化一个目标函数，那么这个目标函数可能是什么呢，是否存在一个普遍的目标，使得这些行为得以涌现呢：
1）**自由能原理**在主动感知（active perception）的框架下统一了行动和感知，通过最小化变分（variational）自由能，鼓励智能体主动获取信息来学习世界模型，以减少其对未来观测的意外程度（surprise）。与强化学习不同的是，在主动感知的框架下智能体并不需要外部提供的与任务相关的奖励，这就避免了人为设计奖励函数会碰到的困难。
2）**通过对抗训练的方式来学习世界模型**，一个名为世界模型的神经网络和一个名为控制器的神经网络之间进行对抗性博弈，其中控制器的内在奖励可以是世界模型的信息增益、算法压缩进度或在具有可计算答案的更抽象问题上的正确性等等，他们展示了通过神经网络权重编码的自我发明思维实验进行学习的可能。

*（4）数据/环境*
训练智能体的数据和环境也对通用智能的产生起到关键的作用。目前的主流的做法在采集数据和训练模型上是分离的，而我们也看到这对于复杂的实际问题是有局限性的（自动驾驶就需要不断用当前学到的驾驶策略去采集新数据，并用新数据进一步更新驾驶策略，实现反馈闭环）
**一个适合通用智能涌现的环境应当至少具有开放式（opened-endness）非平稳性（non-stationary）的特点，比如说我们所生存的这个世界，不断地会有新的复杂性的出现**。
>目前的绝大多数强化学习的环境都是有限的，类似于单机游戏总有通关的时候，在通关以后没有任何进一步学习适应新环境的动机和必要性了，那么自然不需要涌现出适应性和通用学习能力。

## 零样本学习
**零样本学习**的目标是让模型具备对未见过的数据进行有效推断的能力。为了实现零样本学习，通常会利用**语义嵌入**（semantic embeddings  [[NLP基础#2.2 词嵌入]]，高维的符号型数据（比如单词、短语、图像等）映射到低维连续向量空间）和**元学习**（meta-learning，让模型学会如何学习）等技术，以便让模型在没有标记数据的情况下也能做出准确的预测。

> - 通过语言嵌入，学习机器人多任务策略，从而在测试集中零样本泛化解决这个任务。
> - 为降低具身智能数据采集成本，可以在大规模文本、语音和视觉任务数据集上预训练的模型，在具身任务数据集上微调，从而泛化到具身任务中








