# 自动驾驶：World Model
`CVPR 2023 自动驾驶workshop上Tesla和Wayve都提到了他们在利用生成大模型方面的最新探索方向，即大模型来生成自动驾驶相关的连续视频场景，Wayve将其命名为GAIA-1，并于前段时间发布，而Tesla则将自己的尝试命名为World Model。`

自动驾驶的目标是开发无需人工干预、减少事件和提高交通效率的车辆。自动驾驶车辆需要对环境有着越来越深刻的理解。而**World Model正是打开自动驾驶未来大门的钥匙！**

（1）大规模生成仿真数据
world model凭借自动驾驶车辆采集的大量实景视频数据，利用**生成模型**生成未来场景，并和真实的未来时刻数据对比，从而构建损失，这样就可以不依赖标注信息对模型进行训练，即视觉自监督学习。世界模型可以用作仿真工具来大规模生成仿真数据，尤其是极其罕见的边缘场景（Corner Case）。
> GAIA-1生成的场景虽然很丰富，但是还是偏模糊，另外仔细观察连续帧会发现有些车子或背景会在推演过程发生形状颜色类型的跳边，这可能表明了现阶段GAIA模型还没有很好的理解事物变化的连续性。

（2） 语义信息理解
Tesla World Model不仅能够生成RGB空间图像，还能够生成类似标注的**语义信息**，而这既表明了这项技术未来被利用在标注数据生成的潜力，也说明了模型具备了一定的对于语义的理解推演能力。

（2） 未来展望
World Model更有潜力的应用方向我认为是World Model可能会成为像GPT一样的自动驾驶领域的**基础模型**，而其他自动驾驶具体任务都会围绕这个基础模型进行研发构建。
GPT利用自监督的方式通过海量数据得到一个能力非常强大的模型，这个训练好的模型直接在推理阶段被用来执行后续的任务。那么自动驾驶Foundation Model使用的是图像生成技术，而现在最成功的图像生成技术无疑是Diffusion Model。

#生成模型 #自监督训练 #视觉自监督学习  #扩散模型（Diffusion_Models）

>自监督学习（Self-Supervised Learning）是一种无需人工标签的训练方法，通过使用数据**本身的结构或内在信息**来指导模型的学习过程。传统的监督学习中，需要为每个样本提供人工标签作为训练信号。而自监督学习则通过设计一些任务，使得模型能够自行生成“伪标签”，从而进行学习。这些任务的目标是根据原始数据中的某种特定属性或关系来预测缺失或隐藏的部分。自监督学习的优势在于不依赖人工标签，可以利用大规模未标记数据进行训练，提高数据利用效率，并且可以预训练一个良好的初始模型，再进行有监督任务的微调。这对于数据稀缺或标签困难的场景具有重要意义。

# 可探索的预训练强化学习Transformer
`DeepMind提出了算法蒸馏(Algorithm Distillation, AD) ，通过建立因果序列模型将强化学习算法提取到神经网络中。 REF：IN-CONTEXT REINFORCEMENT LEARNING WITH ALGORITHM DISTILLATION`

（1）背景介绍：
使用Transformer将离线强化学习（offline RL）问题转化为一个序列预测问题，训练得到的Transformer模型学到的不是离线数据库中的策略而是产生这个数据库中数据的强化学习算法。所以他们的方法被称为算法蒸馏（Algorithm Distillation），以此与传统的策略蒸馏（policy distillation）相区分。

（2）离线强化学习方法：
- 从不包含学习的数据中学习策略，如通过蒸馏固定的专家策略。
- 从包含学习的数据中学习，如智能体的经验缓存区

（3）研究瓶颈：目前离线强化学习的的内容（context）太小，无法捕捉到策略提升。

#策略蒸馏 #算法蒸馏  #预训练模型


【算法蒸馏 & 策略蒸馏】


Transformer可以通过模仿学习从离线RL数据中学习单任务策略，随后又被扩展为可以在同域和跨域设置中提取多任务策略。这些工作为提取通用的多任务策略提出了一个很有前景的范式：首先收集大量不同的环境互动数据集，然后通过序列建模从数据中提取一个策略。


# PPO x Family 决策智能入门公开课


（1）从感知AI到决策AI
人工智能三要素：算力、算法、数据
算法通常分为：感知型AI和决策型AI
感知型AI：多种模态信息的提取和融和

（2）为什么使用深度强化学习做决策AI

根本目标：**搜索最优解**
实现方式：模仿学习和试错学习
- 模仿学习：
	- 间接、直观
	- 数据要求高、可迁移性差
- 试错学习：
	- 可以不断提升与强化
	- 过程复杂，效率和稳定性有待提高
传统试错学习难点
- 搜索效率
- 随机性
- 非完全信息

深度强化学习：一种更强大、更通用、更稳定的搜索最优解的方法。

（3）强化学习
环境：是对决策问题的抽象和标准化。
智能体的目标：找到该环境上的最优解
智能体和环境通过交互来进行在线学习不断强化自身，演化出无限可能。

特点：
- 类比传统搜索算法：RL可以建模环境的未知性和不确定性，自主学到更抽象的搜索此策略。
- 类比监督学习（从固定标签中学习）：RL需要从延迟性的、间接的奖励中学习。
- 类比离线学习：RL是在线学习，需要平衡探索和利用，需要从非独立同分布的数据中学习。
综上，强化学习有三个特点：**建模环境、从奖励中学习、平衡探索和利用。**

（4）为什么要结合深度学习
深度神经网络拥有非常强大的**非线性建模和表征能力**，可以处理多种决策场景中不同模态的输入和输出（即多模态观察空间和混合动作空间）。


（5）形式化定义RL

1. 问题环境：自然决策问题抽象成适用于强化学习设定MDP
2. 优化目标：稳定且优化效率足够高的优化目标。
3. 马尔科夫性：t时刻的决策仅与t-1时刻状态有关。

强化学习的目标就是寻找能够最大化期望回报的策略。


（6） 为什么聚焦于PPO

1）收集数据
不断探索到新的有价值的信息，才能不断强化智能体。
2）设计目标
策略梯度定理：增大策略选择高回报动作$P_\theta(a_t,s_t)$的概率，减少策略选择低回报值动作的概率 。

梯度项类似于最大似然估计，$G_t(\tau^n)$作为系数，这个系数就隐含了增大高回报动作选择的概率，高回报就会给更大的权重。

$$
\begin{gathered}
\theta \leftarrow \theta+l r \cdot \nabla J_\theta \\
\nabla J_\theta=\frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} G_t\left(\tau^n\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)
\end{gathered}
$$
3）优化策略
如何利用神经网络去实现设计目标
![[Pasted image 20240105164040.png]]


REINFORCE方法高方差，AC存在偏差。
> AC不必收集一整个回合的数据才去更新

**A2C**：学习一个优势函数。优势函数结合两者的优势，并减去一个基线函数来标准化，从而在保持无偏估计的同时减少方差。

**TRPO**：通过可靠的方向和合适的步长，解决如何让策略持久稳定提升问题。


当新旧策略之间通过KL散度控制在一定范围之内（信赖域），就可以利用旧策略来更新新策略。
![[Pasted image 20240105172308.png]]
对于动作这一部分：仍然存在新策略的期望这一项，通过重要性采样（IS）解决。重要性采样就是解决两个分布间存在差异的情况，而又需要求期望时，就需要通过重要性采样系数来进行校正。
![[Pasted image 20240105172654.png]]

综上，采用当前策略（更新前的策略，旧策略）去收集数据，用收集到的数据去计算一个更新方向，这个更新方向因为有重要性采样的校正，因为有信赖域的约束，因此可以保证策略的提升。


**PPO**：解决TRPO求解约束优化问题复杂度过高，IS计算方差很大的问题。解决方式为：
- 截断式优化目标
- 悲观的约束：当优势值大于0时，保持足够谨慎的更新；当优势值小于0时，保持足够的惩罚力度。
- 结合其他改进方法（GAE）


数据：收集序列轨迹，预计算所需值（Return、优势估计）
两重训练循环：提高数据利用效率
- 第一重循环：将收集到的数据训练多个Epoch
- 第二重循环：在某一个Epoch内，将数据划分为多个mini-batch进行更新。

策略更新：截断式优化目标+悲观约束
价值函数更新：结合多步时序差分方法