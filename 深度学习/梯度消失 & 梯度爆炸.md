## 一、引入
### 1. 梯度不稳定

#### 什么是梯度不稳定？

神经网络的梯度是不稳定的，越**靠近输入层**的隐藏层中越容易发生梯度消失或爆炸， 这也是基于梯度学习的深度神经网络的根本问题。

#### 产生梯度不稳定的原始？

由于神经网络反向传播（**链式法则**）计算梯度，上层梯度（靠近输入层）来自于下层（靠近输出层）梯度的乘积，当存在过多层时，经过层层累积，就会出现梯度不稳定的情况，比如梯度爆炸或梯度消失的情况。

所以梯度消失和梯度爆炸都属于梯度不稳定

### 2. 梯度计算

正向传播，输入数据 $x$ 首先经过权重和偏置的计算然后输入激活函数 $f_1= f(x * w_1 +b1)$，得到的结果继续经权重和偏置计算后，传入下一层的激活函数，如此不断计算，得到最后的输出：
$$
f_{i+1} = f(f_i * w_{i+1} + b_{i+1})
$$
反向传播，若基于梯度下降的策略，权重参数的更新为：$w \leftarrow w+\Delta w$，给定学习率 $\alpha$，得出 $\nabla w=-\alpha \frac{\partial \operatorname{Loss}}{\partial w}$，即损失函数对权重进行求导。
如果要更新第二层的权重，根据上面可知，无法直接求解$\frac{\partial L o s s}{\partial w_2}$，需要通过链式法则进行转换:
$$
\nabla w_2=\frac{\partial L o s s}{\partial w_2}=\frac{\partial L o s s}{\partial f_4} \frac{\partial f_4}{\partial f_3} \frac{\partial f_3}{\partial f_2} \frac{\partial f_2}{\partial w_2}
$$
其中 $\frac{\partial f_2}{\partial w_2} = f_1$，即第一个隐藏层的输入。$\frac{\partial f_4}{\partial f_3}$ 则是对激活函数进行求导即 $\nabla f(f_3*w_4+b_4)*w_4$。同理$\frac{\partial L o s s}{\partial f_4}$则是对损失函数求导。


## 二、简述
#### 1. 什么是梯度消失和梯度爆炸？
**直观理解**：
根据上面链式法则我们可知，在更新各层权重，随着层数的增多，上层梯度容易受下层梯度的影响，如果下层梯度($\frac{\partial f_{i+1}}{\partial f_{i}}$)小于1，即激活函数在$w_{i+1}$的导数值小于1，经过层层累乘，上层梯度会越来越小，从而发生**梯度消失**（gradient vanishing），相反如果下层梯度值大于1，经过层层传递，上层梯度越来越大，从而发生**梯度爆炸**（gradient explored）。

**进阶理解**：
- 梯度下降：上层权重更新速率低于下层学习速率，随着层数越多，差距越明显。
- 梯度上升：上层权重更新速度高于下层学习速率，随着层数越多，差距越明显。

权重更新速度：
![500](gradient_vansishing.png)

#### 2. 梯度消失和梯度爆炸产生的原因？
- **层数过多**；随着神经网络层数不断增加，不同层间权重更新速度差异较大，而靠近输入层的权重变化会很小（梯度消失）、很大（梯度爆炸）。这是反向传播更新参数的固有缺陷。
- **激活函数**；例如 sigmoid 函数求导后，最大值为 0.25，初始化的网络权重通常小于 1，则 $\nabla f*w<1$. 那么随着层数越多，求导会越来越小，从而导致最后发生梯度消失。
- **初始权重**；同样初始权重过大，$\nabla f*w>1$，那么随着层数越多，求导会越来越大，上层权重更新速度增快， 引起梯度爆炸，所以通常权重初始化都利用高斯分布。

#### 3. 怎么样避免梯度消失和梯度爆炸？
- **选用合适的激活函数**
- **Batch Normalization**




[详解深度学习中的梯度消失、爆炸原因及其解决方法](https://zhuanlan.zhihu.com/p/33006526)