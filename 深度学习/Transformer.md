#多头注意力机制 #残差结构

# 一、简述

*Transformer 优势是什么？*

利用注意力机制（多头注意力机制）来提高训练速度的模型，支持并行化计算，由于增加了模型的复杂度，使得它在精度和性能上都要优于 RNN 循环神经网络

> 加强了对注意力的使用，使用多头注意力机制
> 它不像RNN一样对每次都对整个句子加深理解，而是每次都注意到句中不同的部分，提炼对句子更深层的理解。


*Transformer 的结构是什么样？*

包含一个编码层 Encoders 和解码层 Decoders，在 Encoders、Decoders 中存在多个小编码器 Encode、小解码器 Decode。

对于每个编码器 encode，它的输入是前一个编码器的输出。
对于每个解码器 decode，它的输入 包括两个部分：前一个解码器的输出+整个编码层 Encoders 的输出。


**Encode 模块**： 自注意力机制+前馈神经网络
- 自注意力机制（多头注意力机制）：每个自注意模块，经过矩阵计算（Query、Key、Value），得到一个权重矩阵，所有注意力模块拼接为一个多重权重矩阵。
- 前馈神经网络：由于 transformer 中使用了多个 Encoder、Decoder，为了解决**梯度消失**的问题，每个 Encoder、Decoder 都加入**残差**神经网络的结构，所以前馈神经网络的输入包括自注意力机制和原始数据输入。 

**Decode 模块**：自注意力机制 + 注意力 + 前馈神经网络
- 自注意力机制和前馈神经网络与 Encode 模型是相似的。
- Encoder-Decoder 注意力：利用 Encoder 的输出与 Decoder 的自注意力输出做一次注意力，这样 Decoder 就能参考 Encoder 自注意力的结果，这样自注意力才有意义。

预测模块：Softmax
- 将Decoder模块产生的向量映射到更高维度的向量（logits），用于单词的选择


简略版
![[transformer.png]]
完整版：
![[transformer2.png]]


>  注意力机制的核心：通过Query、Key、Value，快速准确地找到核心内容，换句话说：**用我的搜索（Query）找到关键内容（Key），在关键内容上花时间花功夫（Value）。**


**什么是多头注意力？**
Multi-head Attention
在同一层做注意力计算的时候，同时多做几次注意力（独立）。他扩展了模型关注不同位置的能力，提高自注意力机制层的性能。
>就是多个人帮我同时理解一句话，我在他们理解的基础上，给出自己的答案


