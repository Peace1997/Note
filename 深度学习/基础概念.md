
# 前向传播

![[Pasted image 20240104111120.png]]


我们期望通过多个激活函数去拟合目标函数，激活函数是需要通过变换移才可以完成这一目标，那如何使激活函数到任意位置呢$\rightarrow$ 对激活函数前加斜率和偏执（$k(r(ax+b)+d)$）。
输入值与权重偏置的矩阵运算可以看作是线性操作($ax+b$)；对线性操作的结果进行激活函数的处理，可以看作是非线性运算$r(ax+b)$。同理，对非线性运算的结果继续进行权重偏执的矩阵运算可以看作是第二次线性操作$k(r(ax+b)+d)$，即可得到任意位置的激活函数，从而通过激活函数去拟合目标函数。

# 梯度下降

2024/1/4

在反向传播计算梯度时，通过链式求导法则就可以知道求出输出对每个参数的偏导，如果偏导为正，表明沿着减少该参数的方向走一点可以使得输出减少，偏导越大表示输出减少的趋势越大。梯度可以看作是偏导数的向量形式。梯度的大小和方向都给出了函数在该点处增长最快的方向。神经网络更新采用沿着**梯度方向**更新的方法，通过学习率控制学习的步幅，更快的完成学习。

在神经网络梯度下降中，梯度的正负值会指示参数更新的方向。
- 当梯度为正时，表示当前参数对于损失函数具有正向影响。也就是说，增加该参数的值可能会导致输出增加，而减少该参数的值可能会导致输出（损失函数）减少。因此，为了最小化损失函数，我们应该沿着梯度的负方向更新该参数，即减小它的值。
    
- 当梯度为负时，表示当前参数对于损失函数具有负向影响。也就是说，增加该参数的值可能会导致输出减少，而减少该参数的值可能会导致输出增加。因此，为了最小化损失函数，我们应该沿着梯度的正方向更新该参数，即增加它的值。

梯度的正负值告诉我们参数更新的方向，以使得损失函数逐渐减小。梯度下降算法利用这个信息来迭代地更新神经网络参数，直到达到损失函数的最小值或接近最小值。