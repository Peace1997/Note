---
aka description: 《深度学习入门：基于Python的理论与实践》
---

# 《深度学习入门：基于Python的理论与实践》上

  

## 一、 感知机（Perceptron）

**基本结构**：感知机接收多个输入信号，输出一个信号。

**组成：**
* 权重$w$：控制输入信号的重要性的参数
* 偏置$b$：调整神经元被激活的容易成都的参数
* 神经元：输入信号送往神经元时，会被乘以固定的权重($w_1x_1$,$w_2x_2$)。神经会计算传送过来的信号总和，总和超过某个界限($\theta$;阈值)时，神经元被激活。

**单层感知机与多层感知机**：
* 单层感知机可以实现与门、非门、或门三种逻辑电路。感知机的局限性在于它只能表示由一条直线分割的空间（**线性空间**），而对于由于曲线分割而成的空间（**非线形空间**），例如异或门，无法用单层感知机表示，需要由多层感知机表示。
* 多层感知机：叠加多层的感知机称为多层感知机（异或门由两层感知机组成）。 $\rightarrow$ 引出接下来的神经网络
* 单层感知机只能表示线性空间，而多层感知机可以表示非线形空间。
* **感知机局限**:需要人工设定合适的、符合预期的输入与输出的权重。

  
## 二、 神经网络基础

### 2.1 基本结构：

最左边一列称为**输入层**，中间的一/几列称为**中间层（隐藏层）** ，最右边一列称为**输出层**。

### 2.2 激活函数：
**作用**：将输入信号的总和转换为输出信号的函数h(x)
**描述**：
$$
\begin{align*}
& a = b + w_1x_1 + w_2x_2 \\
& y=h(a)
\end{align*}
$$

> 感知机之间流动的是0或1的二元信号，而神经网络中流动的是连续的实数值信号。


**激活函数为什么不能采用线形函数？**
使用线性函数的话，加深神经网络的层数就没有意义了，因为使用线性函数时，无论如何加深层数，总可以找到一个与之等效的“无隐藏层的神经网络”。
> 例如激活函数为h(x) = cx ; 三层神经网络计算后可以得到y(x) =h(h(h(x))) = $c^3x$同样的处理，可以由y(x) = ax (a=$c^3$)代替

 **常用激活函数**：（感知机中神经元采用阶跃函数）
* Sigmoid函数：
  $$
h(x) = \frac{1}{1+exp(-x)}
$$
* ReLU 函数：
$$

h(x)= \begin{cases}x & \text { if } x>0 \\ 0 & \text { if } x \leq 0\end{cases}

$$

  

### **2.3 各层之间矩阵计算**
![第一层矩阵相乘](img/ANN1.png)

 >其中偏置的数量与下一层神经元个数相对应

  

### **2.4 输出层设计**

* **回归问题**：恒等函数
对于输入信息，不加以任何改动直接输出。

* **分类问题**：softmax函数

$$

y_{k}=\frac{\exp \left(a_{k}\right)}{\sum_{i=1}^{n} \exp \left(a_{i}\right)}

$$

> 特点：softmax的输出是0.0到1.0之间的实数，且输出值的总和是1；因此可以将其输出解释为概率。通常在推理阶段会省略输出层的softmax函数。
> 缺点：softmax函数在计算机的运算中存在一定的缺陷，即会发生**溢出问题**
> 改进： $C'$通常是输入信号中的最大值，可以通过减去这个最大值去防止溢出。 $$
 y_k=\frac{\exp \left(a_{k}+\mathrm{C}^{\prime}\right)}{\sum_{i=1}^{n} \exp \left(a_{i}+\mathrm{C}^{\prime}\right)}
$$


### 总结

* 神经网络问题的步骤可以分为**学习**和**推理**两个阶段；学习阶段使用训练数据（学习数据）进行权重、偏置参数的学习，推理阶段利用刚才学习到的参数，对测试数据进行测试。
* 神经网络的输入时通常需要进行预处理操作：
	* **正规化**（normalization）：把输入数据限定到某个范围内
	* **白化**（whitening）：把数据整体的分布形状均匀化


## 三、 神经网络的学习

神经网络的学习是指以损失函数为基准，找到能使它的值达到最小的权重参数，从而在训练数据中自动获取最优权重参数。

### 3.1 从数据中学习

神经网络的特征就是可以从数据中进行学习，可以利用数据自动决定权重参数的值；尽可能减少人为介入，从数据中发现答案。

  
#### 3.1.1 数据处理方法

* **传统机器学习**：例如在计算机视觉领域，可以先从图像中提出**特征量**（SIFT、SURF、HOG等），在用机器学习技术（SVM、KNN等）学习这些特征量。

> 将图像转为向量时使用的特征量仍是人为设计的，对于不同的问题，仍然需要设计合适的特征量，才可以得到较好的结果。

* **神经网络**(深度学习)：神经网络中对于上述的特征量可以直接由机器来学习，因此对于相似的问题可以用同样的流程来解决。对于**端到端（end-to-end）** 的学习方式，可以直接从原属数据（输入）中获得目标结果（输出）。

#### 3.1.2 训练数据和测试数据：

* 训练数据（监督数据）：使用训练数据，寻找最优的参数。
* 验证集：当我们的模型训练好之后，我们并不知道他的表现如何。这个时候就可以使用验证集（Validation Dataset）来看看模型在新数据（验证集和测试集是不同的数据）上的表现如何。**同时通过调整超参数，让模型处于最好的状态**
* 测试数据：使用测试数据评价训练得到的模型的实际能力，即测试模型的泛化能力。对某个数据集过度拟合的状态称为过拟合（over fitting）**。**

### 3.2 损失函数

损失函数是描述神经网络模型“优良程度”的**指标**，神经网络可以以该指标为线索寻找最优权重参数。


#### 3.2.1 均方误差（mean squared error）：

$$
E=\frac{1}{2} \sum_{k}\left(y_{k}-t_{k}\right)^{2}
$$
$y_k$为神经网络的输出，$t_k$为正确解标签，均方误差会计算神经网络的输出和正确解监督数据各个元素之差的平方，在求总和。

#### 3.2.2 交叉熵误差（cross entropy error）

$$

E=-\sum_{k} t_{k} \log y_{k}

$$
交叉熵误差的值是由正确解标签所对应的输出结果决定的，也就是说正确解标签对应的输出($y_k$)越小，则误差越多。

#### 3.2.3 mini-batch 学习

我们从训练数据中选出一批数据（mini-batch），作为全部数据的“近似“，然后对每个mini-batch进行学习，这种学习方式称为mini-batch学习。

#### 3.2.4 损失函数的设定

*为什么需要设定损失函数作为指标？

在反向传播求导时，如果选用识别精度作为指标时，参数的导数在绝大多数地方都会变成0，导致参数无法更新。稍微改变一下参数值，识别精度的变化是**不连续的、离散**的（32%、33%），而损失函变化是**连续性**的(0.9253、0.9342)。因此识别精度对微小的参数变化几乎没什么反应，即使有反应也是不连续的，突然地变化。

> 类似的，之所以不采用**阶跃函数**作为激活函数，就是因为微小的参数变化会被阶跃函数忽略，导致损失函数的值不会产生任何变化。

  

## 四、 数值微分和梯度

#### 4.1 数值微分

利用微小的差分求导数的过程称为**数值微分**，利用数值微分可以计算权重参数的梯度（严格来说，是损失函数关于权重参数的梯度）。数值微分实现起来很简单，但是比较费时间（存在大量参数）。  

> 而基于数学式的推导强求导数的过程，称为**解析性**求导（误差反向传播）。

#### 4. 2 梯度

* **梯度**（gradient）：由全部变量的**偏导数**汇总而成的向量称为梯度。梯度表示的是各点处的函数值减小（增加）最多的方向，因此无法保证梯度所指的方向就是函数的最小值或者真正前进的方向。
* **梯度法**（gradient）：通过不断的沿梯度方向前进，逐渐减少函数值的过程。虽然梯度的反向并不一定指向最小值，但沿着它的方向能够最大限度地减少函数的值。
* **梯度下降**（gradient descent method）：寻找最小值的梯度法。一般来说，神经网络中，梯度法主要只得是梯度下降
* **梯度上升**（gradient ascent method）：寻找最大值的梯度法。
* **神经网络梯度**：

$$
\begin{gathered} \mathbf{W}=\left(\begin{array}{lll} w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \end{array}\right) \\ \frac{\partial L}{\partial \boldsymbol{W}}=\left(\begin{array}{ccc} \frac{\partial L}{\partial w_{11}} & \frac{\partial L}{\partial w_{12}} & \frac{\partial L}{\partial w_{13}} \\ \frac{\partial L}{\partial w_{21}} & \frac{\partial L}{\partial w_{22}} & \frac{\partial L}{\partial w_{23}} \end{array}\right) \end{gathered}
$$
神经网络的梯度，是指损失函数关于权重参数的梯度；$\frac{\partial L}{\partial \boldsymbol{W}}$的元素由各个元素关于 W 的偏导数构成。例如$\frac{\partial L}{\partial \boldsymbol{w_{11}}}$表示当$w_{11}$稍微变化时，损失函数 L 会发生多大变化。

  
#### 4. 3 学习率

学习率决定在一次学习中，应该学多少，以及在多大程度上更新参数。如果学习率过大，会发散成一个很大的值，反过来，如果学习率过小的话，基本上没怎么更新就结束了。

#### 4. 4 学习算法的实现

1. **mini-batch**：从训练数据中随机选出一部分数据（mini-batch），我们的目标是减少mini-batch的损失函数的值

2. **计算梯度**：为了减少mini-batch的损失函数的值，需要求出各个权重参数的梯度，梯度表示损失函数的值减少最多的方向。

3. **更新参数**：将权重参数沿梯度方向进行微小更新。然后不断重复迭代。

> **随机下降法（stochastic gradient descent；SGD）**：在这里使用的数据是随机选择的mini batch 数据，然后通过梯度下降法更新参数。

  

## 五、 误差反向传播法

### 5.1 计算图

在**计算图**（computational graph）中，从“左向右进行计算”是一种正方向的传播，称为**正向传播**（forward propagation），从“右向左进行计算”是一种反方向的传播，称为**反向传播**（backward propagation）。

* **局部计算**：计算图的特征是可以通过传递“局部计算”获得最终结果。其中局部计算是指，无论全局的计算多复杂，都能只根据与自己相关的信息输出接下来的结果。虽然局部计算非常简单，但是通过传递它的计算结果，就可以获得全局的复杂计算的结果。

* **计算图优点**：可以通过正向传播和反向传播高效地计算各个变量的导数值。
	- 计算图可以集中精力于局部计算，从而简化问题。
	- 利用计算图可以将中间的计算结果全部保存起来。


### 5.2 反向传播

反向传播将局部导数向正方向的反方向传递局部导数的原理，是基于**链式法则**（chain rule）的。其中链式法则是关于复合函数的导数性质。

#### 5.2.1 加法节点 & 乘法节点

* **加法节点**的反向传播只是将输入信号输出到下一个节点。
* **乘法的反向**传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。

#### 5.2.2 举例

对于Sigmoid层反向传播计算：神经网络的反向传播会把这个差分表示的误差传递给前面的层，在每个子层计算时采用链式法则进行局部计算。

  
![](img/backward.png)

  

#### 5.2.3 损失函数设计对反向传播的影响

* **分类**问题中：使用交叉熵误差作为输出层softmax函数的损失函数后，反向传播会得到($y_1-t_1$,$y_2-t_2$,$y_3-t_3$) 这样可以直接看出与标签差值的结果。

* **回归**问题中：使用平方和误差作为输出层“恒等函数”的损失函数，反向传播仍然会得到上述相同的结果。

> 如果在方向传播过程中，存在一个较大的误差，那么前面的层学到的内容就会比较“大”，反之亦然。

  

#### 5.2.4 误差反向传播梯度确认

相较于数值微分，即使存在大量的参数没，误差反向传播法也可以高效的计算梯度。而误差反向传播的实现比较复杂，容易出错，因此在确定反向传播法是否正确时，是需要用数值微分的，通过数值分析法，可以确定数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（非常相近），这种操作称为**梯度确认**（gradient check）。

  

## 六、 与学习相关的技巧

  

### 6.1 参数的更新

神经网络学习目的是找到使损失函数的值尽可能小的参数，这种寻找最优参数的过程称为最优化（optimization）。

#### 6.1.1 SGD （随机梯度下降）

$$

\boldsymbol{W} \leftarrow \boldsymbol{W}-\eta \frac{\partial L}{\partial \boldsymbol{W}}

$$

其中$\eta$表示学习率，上面式子直接根据梯度下降更新权重参数。SGD虽然计算较为简单，但是存在一定的缺点，如果函数的形状非均向，比如延伸状（$f(x,y)= x^2 + y^2$），更新路径（“之字形路径”）就会非常低效，根本原因是，梯度的方向并没有指向最小值的方向。

> SGD可以类比为在复杂地形中，没有地图，不能睁眼的情况下，需要尽快完成“下山任务”，虽然看不到周围情况，但是能够感受当前所在位置不同地面的坡度，只要朝着当前所在位置的坡度最大方向前进，不断重复这一策略，就可以下山。

#### 6.1.2 Momentum（动量）

$$

\begin{gathered} \boldsymbol{v} \leftarrow \alpha \boldsymbol{v}-\eta \frac{\partial L}{\partial \boldsymbol{W}} \\ \boldsymbol{W} \leftarrow \boldsymbol{W}+\boldsymbol{v} \end{gathered}

$$
加入了一个新的变量$v$,对应于物理上的速度。第一个公式表示了物体在梯度方向上受力，在这个力的作用下，物体的速度不断增加。$\alpha v$这一项，在物体不受任何力时，该项承担使得物体逐渐减速的任务（$\alpha$通常设为0.9左右），对应于物理上的地面摩擦力或空气阻力。其更新路径与SGD相比，减弱了“之”字形的变动程度。

  

#### 6.1.3 AdaGrad
$$
\begin{aligned} &\boldsymbol{h} \leftarrow \boldsymbol{h}+\frac{\partial L}{\partial \boldsymbol{W}} \odot \frac{\partial L}{\partial \boldsymbol{W}} \\ &\boldsymbol{W} \leftarrow \boldsymbol{W}-\eta \frac{1}{\sqrt{\boldsymbol{h}}} \frac{\partial L}{\partial \boldsymbol{W}} \end{aligned}
$$

AdaGrad采用**学习率衰减**（learning rate decay）的方法，随着学习的进行，AdaGrad会为参数的每个元素适当的调整学习率，同时AdaGrad会记录**历史梯度信息**。具体实现为：通过变量 $h$  保存以前的所有梯度值的平方和。然后通过乘以$\frac{1}{\sqrt h}$就可以调整学习的尺度，例如参数的元素变化比较大（被大幅更新）的元素的学习率将变小。因此 AdaGrad开始时变动较大，但后面会根据这个较大的变动，按比例进行调整，减小更新的步伐，减少“之”字形的变动程度。

> 随时学习的深入，学习率会不断变小，如果学习率很低时，更新幅度就会变得很小，为了改善这个问题，可以采用RMSProp方法，它并不是将过去所有的梯度一视同仁的相加，而是逐渐遗忘过去的梯度，在进行加法运算时将新梯度的信息更多地反应出来，这种操作称为**指数移动平均**，即呈指数函数式的减小过去的梯度的尺度。

#### 6.1.4 Adam

Momentum 参照小球在碗中滚动的物理规则进行移动，AdaGrad为参数的每个元素适当地调整更新步伐，Adam将这两个方法相融合，通过组合这两个方法的优点，同时加入了**超参数的“偏置校正”**，实现参数空间的高效搜索。综上，Adam的更新过程像Momentum类似的移动，但左右摇晃的程度有所减轻，这得益于学习的更新程度被适当地调整了。

  

### 6.2 权重初始化

在神经网络的学习中，权重初始值非常重要，很多时候权重初始值的设定关系到神经网络的学习是否能够成功。

####  6.2.1 神经权重初始值设为0（任意相同值）

 **导致问题**：
在神经网络中不能采用0（实际上不光是0初始化，将权值初始化为任意相同值，都很有可能使模型失效）。因为在误差反向传播的过程中，**所有的权重值都会进行相同的更新**。

 **解释**：
 例如在一个三层结构神经网络中，在正向传播的过程中，若输入层的权重为0，第二层隐藏层的神经元被传入相同的值（0），经过第二层激活函数计算后，会输出相同的值，从而输出层计算会得到固定的值；因此在反向传播时，权重会进行相同的更新，从而权重被更新为相同的值，并且拥有对称的值（重复的值）不管进行多少轮的正向传播和反向传播，得到的权重参数都一样，从而使得神经网络拥有不同权值的意义丧失。因此，它们都在计算同一特征，网络变得跟只有一个隐含层节点一样，这使得神经网络失去了学习不同特征的能力，神经网络就失去了其特征学习的能力。

 **分类讨论**

 模型所有w初始化为0，b随机初始化：刚开始时每轮只能从后往前更新一层的参数，但由于b的不同，在经过足够多的轮数后所有参数还是能得到更新的。但是这种方式更新较慢，且存在梯度消失、梯度爆炸等问题，通常不会这么干。
 
模型所有w随机初始化，b初始化为0：每一层的参数的更新与其后一层的b并没有关系，因此b的初始值不会影响BP算法的效果，所有权值和b都能得到更新。

解决方法：因此为了防止“权重均一化”（**瓦解权重对称结构**），权重必须随机生成初始值。

> 补充：线性回归和逻辑回归可以采用0初始[https://blog.csdn.net/qq\_32623363/article/details/115035765](https://blog.csdn.net/qq\_32623363/article/details/115035765)

  
#### 6.2.2 梯度消失和梯度爆炸与权重初始化

反向传播过程中，由于链式法则的求导法则，反向传播计算梯度时，梯度的大小与激活函数的导数以及权重相关，如果两者相乘在(0,1)之间，层数越多，导数相乘累积也就越来越小，从而发生**梯度消失**（gradient vanishing）的情况，相反，如果两者相乘大于1，经过层层传递，梯度越来越大，从而发生**梯度爆炸**（gradient explored）的情况。

因此在考虑初始化权重时要注意，如果初始权重较小，可能会发生梯度消失的情况，反之，可能会发生梯度爆炸。

> 参考：[https://www.zhihu.com/question/290392414/answer/2262481447](https://www.zhihu.com/question/290392414/answer/2262481447)



#### 6.2.3 Xavier 初始化

为了使各层的激活值呈现出更具广度的分布，推导了合适的权重尺度，如果前一层的节点数为n，则初始值使用标准差为$\sqrt \frac{1}{n}$的高斯分布进行初始化，使用Xavier初始值之后，前一层的节点数越多，要设定为目标节点的初始值的权重尺度就越小。

Xavier 初始值是以激活函数是**线性函数**为前提而推导出来的，因为**sigmoid**函数和**tanh**函数左右对称（**S型曲线函数**），且中央附近可以视作线性函数，所以这两个激活函数可以采用Xavier初始化。

#### 6.2.4 He初始化

对于**ReLu**激活函数，一般推荐使用Kaiming He等人推荐的初始值，如果前一层的节点数为n，则初始值使用标准差为$\sqrt \frac{2}{n}$的高斯分布进行初始化，相较于Xavier，因为ReLU的负值区域的值为0，为了使它更具有广度，所以需要2倍的系数。

### 6.3 Batch Normalization

以mini-batch为单位进行学习，**向神经网络中插入对数据分布进行正规化的层**，调整各层激活值的分布使其拥有适当的广度，
>其中一种过拟合之所以发生则是因为，经过训练后，输出分布集中在某一区间，这种通常是训练参数较少导致的，经过BN操作后，让数据分布变广。

#### 6.3.1 BN基本流程

BN 对 mini-batch的m个输入数据集合 B求均值 $u_B$和方差$\sigma_B$ ，然后对输入数据进行均值为0、方差为1（合适的分布）的正规化。通常将这个处理插到激活函数的前面（或者后面），可以减少数据分布的偏向。

$$

\begin{aligned} \mu_{B} & \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_{i} \\ \sigma_{B}^{2} & \leftarrow \frac{1}{m} \sum_{i=1}^{m}\left(x_{i}-\mu_{B}\right)^{2} \\ \hat{x}_{i} & \leftarrow \frac{x_{i}-\mu_{B}}{\sqrt{\sigma_{B}^{2}+\varepsilon}} \end{aligned}

$$

接着BN层会对正规化后的数据进行缩放和平移的变换，这样可以变换回原始的分布，开始时$\gamma$=1, $\beta$=0,然后通过学习调整到合适的值。

$$

y_{i} \leftarrow \gamma \hat{x}_{i}+\beta

$$
#### 6.3.2 BN优点

* 可以使学习快速进行，加速训练，提高模型精度（用上BN层之后可以让损失函数更平滑，可以使用更大的学习率，从而跳出不好的局部极值）
* 不会过度依赖权重初始值
* 抑制过拟合，一定程度上增加了泛化能力（降低Dropout等的必要性）
* 因此当神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以采用BN

  

### 6.4 正则化

通过权重衰减、几种方法可以来达到抑制过拟合的产生。

#### 6.4.1 过拟合

过拟合指的是只能拟合训练数据，但不能很好的拟合不包含在训练数据中的其他数据的状态。其中过拟合产生的原因包括：

* 模型拥有大量参数、表现力强
* 训练参数少


#### 6.4.2 权值衰减

因为过拟合原本是因为权重参数取值过大才发生的，因此权值衰减在学习过程中对大的权重进行惩罚，来抑制过拟合。

具体而言，权值衰减就是**在损失函数上加上权重的平方范数**（L2范数；各个元素的平方和）,用符号表示的话, 如果将权重记为 $\boldsymbol{W}$ ，L2范数的权值衰减就是 $\frac{1}{2} \lambda \boldsymbol{W}^{2}$, 然后将这个 $\frac{1}{2} \lambda \boldsymbol{W}^{2}$ 加到损失函数上。这里：

* $\lambda$ 是控制正则化强度的超参数。 $\lambda$设置得越大, 对大的权重施加的惩罚就越重。
* $\frac{1}{2} \lambda \boldsymbol{W}^{2}$ 开头的 $\frac{1}{2}$是用于将 $\frac{1}{2} \lambda \boldsymbol{W}^{2}$的求导结果变成 $\lambda \boldsymbol{W}$的调整用常量。

对于所有权重, 权值衰减方法都会为损失函数加上 $\frac{1}{2} \lambda \boldsymbol{W}^{2}$。因此, 在求权重梯度的计算中, 要为之前的误差反向传播法的结果加上正则化项的导数 $\lambda \boldsymbol{W}$ 。


#### 6.4.3 Dropout

权重衰减可以适用于较为简单的网络模型，如果网络模型变得复杂，只用权值衰减就难处理。**Dropout是一种在学习的过程中随机删除神经元的方法**。

具体而言，训练时，每传递一次数据，就会随机选择隐藏层的神经元将其删除，被删除的神经元不再进行信号传递；测试时，虽然会传递所有神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后在输出。

**补充：集成学习**

集成学习就是让多个模型单独进行学习，推理时在取多个模型的输出的平均值。在神经网络中，就是分别学习多个神经网络，测试时，以所有神经网络的平均值作为输出。**Dropout将集成学习的效果（模拟地）通过一个网络实现了**，它在学习过程中随机删除神经元，从而达到每一次都让不同的模型进行学习，测试时，通过对神经元乘以删除比例，可以取得模型的平均值。

### 6.5 超参数的验证

通过使用验证数据（validation data），去评判超参数（hyper- parameters）取值的好坏。常见的超参数包括：神经元个数、batch大小、学习率、权值衰减等。

#### 6.5.1 超参数的最优化

在进行超参数最优化时，一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估（回合数不宜过大），重复该操作，观察模型的结果，缩小超参数的范围。这是偏实践性的方法，如果需要更精炼的方法，可以采用**贝叶斯最优化**（Bayesian optimization）。

#### 总结

 **抑制过拟合的方法:**

* Batch Normalization（在激活函数前加入对数据分布进行正规化的层）
* 权值衰减（损失函数加上权重的平方范数）
* Dropout（随机删除神经元）




## 七、 卷积神经网络

卷积神经网络（Convolution Neural Network，CNN）通常由卷积层（Convolution）、池化层（Pooling）、全连接层（fully-connected）组成。

### 7.1 卷积层

主要作用：**提取特征**

#### 7.1.1 全连接层存在的问题

全连接层通常处理一维的信息，图像处理中，图像通常是三维的，空间中像素之间可能存在一定的关联，因此在向全连接层输入时，需要将三维数据拉平为一维数据，因此全连接层会**忽视了数据的形状**；

而卷积层则会保持形状不变；卷积层会以三维数据的形式接收输入数据，并同样以三维数据的形式输出至下一层。卷积层的输入输出数据称为**特征图**（feature map）

#### 7.1.2 填充 & 步幅

在进行卷积层的处理之前，有时需要向输入数据的周围填入固定的数据（0），这称为**填充（padding）**。因为每次卷积运算都会缩小空间，使用填充主要是为了**调整输出的大小**，通过填充可以保持空间大小不变的情况下将数据传给下一层。增大填充之后，输出大小会变大。

应用滤波器的位置间隔称为**步幅（stride）**，增大步幅后，输出大小会变小，

#### 7.1.3 卷积运算

**卷积运算对输入数据应用滤波器**（核/权重）。对于输入数据（高、长方向的二维形状数据 ）， 卷积运算以一定**间隔**（步幅）滑动滤波器的**窗口**（输入数据中与滤波器大小相同的区域）与滤波器进行卷积计算。具体为：将各个位置上滤波器的元素和输入的对应元素相乘，然后在求和（**乘积累加运算**），其中**偏置**会被加到应用了滤波器的所有元素上，然后将这个结果保存到输出的对应位置。所有位置都进行一遍，即可得到卷积运算的输出。

输出高度和宽度的计算公式：假设输入大小为(H,W)，滤波器大小为 (FH,FW) , 输出大小为 (OH,OW)，填充为P，步幅为S。

$$  
\begin{aligned} O H &=\frac{H+2 P-F H}{S}+1 \\ O W &=\frac{W+2 P-F W}{S}+1 \end{aligned}  
$$

  ![![二维卷积运算，步幅为1，填充为0](img/conv.png)


#### 7.1.4 三维数据卷积运算

因为图像是三维的，除了高、长方向之外，还需要处理通道方向，通常表示为（chanel，height，width）、（C，H，W）。因此对于三维数据卷积运算时，对于通道方向有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。在三维数据的卷积运算中，输入数据和滤波器的**通道数**要设为相同的值，**滤波器的大小**可以设定为任意大小（不过每个通道滤波器大小要相同）。

![三维数据卷机运算](img/conv2.png)


上面的例子输出了一张特征图，如果想要输出 **FN 个特征图**，则需要 FN 滤波器（权重）。对于**偏置**的加法运算，需要按通道加上相同的偏置值。对于**批处理**，需要在各层之间传递4维数据(batch_num, channel, height, width) 。网络间传递的是 4 维数据，对这 N 个数据进行了卷积运算。也就是说，批处理将 N 次 的处理汇总成了 1 次进行。

![卷积运算的处理流（批处理+偏置）- 方块表示三维数据](img/conv3.png)

### 7.2 池化层

池化是缩小高、长方向上的空间的运算。主要作用：**压缩特征图，提取主要特征**

#### 7.2.1 池化方式

-   **MAX池化**：从目标区域中取出最大值。
-   **Average池化**：计算目标区域的平均值。
    

![Max池化的处理顺序](img/conv4.png)

步幅为2，进行 2x2 的Max 池化。其中 2x2 表示目标区域的大小，一般来说，池化的窗口大小会和步幅设定成相同的值。

#### 7.2.2 池化层的特征

-   **没有要学习的参数**：与卷积层不同，池化只是从目标区域中取最大值，所以不存在要学习的参数。
    
-   **通道数不发生变化**：经过池化操作后，输入数据和输出数据的通道数不会发生变化，池化计算是按通道**独立**进行计算的。而卷积层输出数据的通道数与滤波器的数量有关。
    
-   **对微小的位置变化具有鲁棒性**：输入数据发生微小偏差时，池化可能仍会返回相同的结果。
    

### 7.3 CNN实现

#### 7.3.1 卷积层实现

`im2col`(image to colum)函数会将输入数据展开以适合滤波器（权重），具体来说，`im2col`会考虑**滤波器大小、步幅、填充**，会把**应用滤波器**的区域（3维方块）横向展开，在所有应用滤波器的地方进行这个展开处理，从而**将三维数据展开为二维数据**。

im2col的实现可以归结到矩阵运算上，从而可以有效的利用已经被高度优化的线性代数库，不过im2col比普通的实现消耗更多的内存。

![将滤波器纵向展开为 1 列，并计算和 im2col 展开的数据的矩阵乘积，最后转换(reshape)为输出数据的大小](img/conv5.png)

#### 7.3.2 池化层实现

与卷积层相同，都使用`im2col`展开输入数据,不过池化操作在通道方向是独立的，池化的应用区域按通道单独展开。展开之后，只需对展开的矩阵求各行的最大值，并转换为合适的形状即可。

![池化层展开应用（步幅为2，池化窗口大小为2）](img/conv6.png)

#### 7.3.3 CNN实现

-   **滤波器的学习**：学习前的滤波器是随机进行初始化的，所以滤波器没有规律可循，通过学习后，滤波器被更新成了**有规律**的滤波器（从白到黑的渐变的滤波器、含有块状区域（blob）的滤波器等）。
    
-   **分层信息提取**：随着层次加深，神经元从简单的形状向”高级“信息变化。
    
    -   对于第一层的卷积层通常会边缘或斑状等原始信息。
    -   接下来的层对纹理有响应。
    -   在后面的层对更加复杂的物体部件有响应。
        

![CNN可视化：随着层次变深，提取的信息愈加高级](img/conv7.png)

#### 7.3.4 代表性CNN

-   LeNet（1998）：
    
    -   激活函数采用Sigmoid（目前主要采用ReLU）
    -   使用子采样缩小中间数据的大小（目前主要采用Max池化）
        
-   AlexNet
    
    -   激活函数采用ReLU
    -   使用进行局部正规化的LRN（Local response Normalization）层
    -   使用Dropout
        
-   VGG
    
    -   基于3x3的小型滤波器的卷积层
    -   激活函数为ReLU
    -   全连接层后使用Dropout层
    -   基于Adam的最优化
    -   使用He初始值作为权重初始值
        
-   GoogLeNet
    
    -   网络不仅在纵向上有深度，在横向上也有深度（广度），这称为“Inception结构”，Inception结构使用了多个大小不同的滤波器（和池化），最后再合并他们的结果。
    -   GoogLeNet使用了1x1的滤波器的卷积层，这个1x1的卷积运算通过在通道方向上减小大小，有助于减少参数和实现高速化处理。
        
-   ResNet
    
    -   为了避免过度加深层，导致无法很好进行学习，因此ResNet导入了“快捷结构(小路、捷径)”，可以随着层的加深（有限程度）而不断提高性能。
    -   快捷结构知识原封不动的传递输入数据，因此反向传播时将来自上游的梯度原封不动的传向下游。从而可以缓解因加深层带来的梯度变小的梯度消失问题。
        

## 八、深度学习

### 8.1 加深网络

#### 8.1.1 加深层的好处（叠加小型滤波器）

-   **减少网络的参数数量：** 加深了层的网络可以用更少的参数达到同等水平（或更强）的表现力。

> 例如一次5x5的卷积运算的区域可以由两次 3x3的卷积运算抵充，前者的参数数量25（5x5），后者为18 （2x3x3），通过叠加卷积层，参数数量减少了。

-   **提升网络表现力:** 通过叠加层，将ReLU等激活函数加在卷积层中间，向网络添加了基于激活函数的“非线性”表现力，可以表达更加复杂的东西；扩大感受野（receptive field）。

> 感受野：给神经元施加变化的某个局部空间区域；**CNN每一层输出的特征图(feature map)上的像素点在原始图像上映射的区域大小。**
    
-   **学习更加高效**；通过加深层，可以分层次地传递信息，将各层要学习的问题分解成容易解决的问题。

> 比如最开始的层只要专注于学习边缘就好，这样以来，只需要较少的学习数据就可以高效的进行学习。
    

#### 8.1.2 提升识别精度的技术

-   **集成学习**
-   **学习率衰减**
-   **Data Augmentation（数据扩充）**
    
    -   对于输入图像通过施加旋转、垂直或水平方向上的移动等微小变化，增加图像的数量。
    -   裁剪图像的“crop处理”，图像左右翻转的“flip 处理”
    -   施加亮度等外观上的变化、放大缩小等尺度上的
        

### 8.2 补充总结

#### 8.2.1 迁移学习

将学习完的权重（的一部分）复制到其他神经网络，进行再学习（fine tuning），比如，准备一个和VGG相同结构的网络，把学习完的权重作为初始值，以新数据集为对象，进行再学习。**迁移学习在数据集比较少时非常有效**。

#### 8.2.2 GPU

GPU可以高速地进行大量的**并行数值计算**，比如乘积累加运算（或者大型**矩阵的乘积运算**）。而CPU比较擅长**连续的、复杂的计算**。

#### 8.2.3 物体检测

物体检测是从图像中确定物体的位置，并进行分类的问题，例如使用CNN进行物体检测的方法：**R-CNN**方法

-   Extract region proposals（侯选区域的提取）：发现形似目标物体区域。
-   Comput CNN features （CNN特征计算）：对提取的区域应用CNN进行分类。
    

#### 8.2.4 图像分割

图像分割是指在**像素**水平上对图像进行分类。基于神经网络进行图像分割，需要以所有像素为对象，而使用卷积运算会发生重复计算很多区域的无意义的计算，针对该问题，提出了一个名为**FCN**（Fully Convolution Network）的方法，该方法通过一次forward处理，对所有像素进行分类。

-   相较于一般的CNN包含全连接层，**FCN将全连接层替换成发挥相同作用的卷积层**。
    
    > FCN的特征在于最后导入了扩大空间大小的处理（基于双线性插值法扩大），基于这个操作，变小的中间数据可以一下子扩大到和输入图像一样的大小。
    

#### 8.2.5 图像标题的生成

给出一个图像后，会自动生成介绍这个图像的文字（图像标题），这是一个融合计算机视觉和自然语言的研究（**多模态处理**）。一个基于深度学习生成图像标题的代表性方法为**NIC**（Neural Image Caption）的模型：CNN + RNN。

-   CNN：从图像中提取特征作为初始值
    
-   RNN：根据CNN提取的特征，递归的生成文本
    

#### 8.2.6 图像的生成

生成新的图像时不需要任何图像；事先需要使用大量的图像进行学习，但在生成新的图像时不需要任何图像。一个基于深度学习的方法为 **DCGAN**（Deep Convolution Generative Adversarial Network）：

-   Generator（生成者）：生成近似真品的图像
    
-   Discriminator（识别者）：判别它是不是真图像（是Generator 生成的图像还是实际拍摄的图像）
    

利用 **GAN**(Generative Adversarial Network）这个技术，可以让两者以竞争的方式学习，Generator 会学习到更加精妙的图像作假技术，Discriminator 则会成长为能以更高精度辨别真假的鉴定师。

#### 8.2.7 自动驾驶

自动驾驶需要结合各种技术的力量来实现，比如决定行驶路线的路线计划(path plan)技术、照相机或激光等传感技术等，在这些技术中，正确**识别周围环境**的技术据说尤其重要；基于 CNN 的神经网络 SegNet，对输入图像进行了分割(像素水平的判别)，在某种程度上正确地识别了道路、建筑物、人行道、树木、车