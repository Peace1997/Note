## 一、简述
模仿学习是学习器尝试模仿专家行为从而获取最佳性能的一系列任务。
>Imitation Learning is a sequential task where the learner tries to mimic an expert’s action in order to achieve the best perfomance.

从专家范例中进行学习，即对专家数据进行建模。

根据专家在训练过程中是否进行指导，可以将模仿学习分为两类，即**离线模仿学习**（offline imitation learning）和**在线模型学习**（online imitation learning）。

### 1. 离线模仿学习
通过一个固定的专家范例训练集来进行学习，使得我们学习的策略能够近似专家策略。这可以近似看作监督学习，通过对比模型的输出和专家范例的输出来进行学习。通常称这钟方式为**行为克隆**（behavioural cloning）

#### 专家范例数据
包含多条训练轨迹，每个轨迹包含了状态序列，以及专家执行的动作序列。
#### 目标 
最小化模型策略与专家行为的差异


Dataset： $D=\left\{\tau_i\right\}_{i=0}^N$
Trajectory： $T_i^{\prime}=\left\{s_1^i, a_1^i, s_2^i, a_2^i \quad s_{T}^i a_T^i\right\}$ 
policy $\prod: s \rightarrow a$
$$
\prod_\theta(a \mid s) \sim \operatorname{argmin} \sum_{i=1 j=1}^{j \leq N} L\left(\prod_\theta\left(s_i^j\right), a_i^j\right)
$$

#### 损失函数
量化我们设定的目标
如果动作是离散的，可以看作是一个分类问题，可以将交叉熵作为损失函数；如果动作是连续的，可以看作是一个回归问题，可以采用L2损失函数。
L2 损失函数等价于基于高斯分布的最大化期望 log 似然
$$
\begin{aligned}
a&=\pi_\theta(s)+\epsilon \\
p(a | s, \theta) &=\frac{1}{\sigma \sqrt{2 \pi }} \exp \left(-\frac{\left(a-\pi_\theta(s)\right)^2}{2 \sigma^2}\right) \\
\operatorname{argmax} E[\log ]] &=\operatorname{argmax} E\left[\log \exp \left(-\frac{\left.\left(a-\pi_\theta(s)\right)^2\right)^2}{2 \sigma}\right]\right.\\
& \approx \operatorname{argmin} \frac{1}{N} \sum\left(a-\pi_\theta(s)\right)^2: l_2 \text { error }
\end{aligned}
$$
[参考：L2损失与高斯分布推导 ](常用解释#L2损失和高斯分布)

#### 缺点
- **范化困难**；由于专家数据无法包含所有的状态动作，例如自动驾驶时，由于专家的驾驶经验很丰富，采集到的数据通常是正常驾驶的情况，如果智能体此时遇到危险的状态，由于没有对应的专家数据，因此智能体此时通常无法很好的处理这种“陌生”的状态其根本原因为 [协变量偏移](常用解释#协变量偏移 内部协变量偏移])。

#### 解决方法
- 丰富专家数据
- 在线模仿学习

## 2. 在线模仿学习
目前比较流行的在线模仿学习算法通常称为  **Data Aggregation Approach: DAGGER** ；这种方法可以有效的减少训练样本和测试样本之间分布的差异。简单来说，在训练过程中，*专家可以对来自学习者自身行为的示例提供正确的动作*，从而纠正过去学习者失败的行为。

#### 在线模仿学习与离线模仿学习
在线模仿学习与离线模仿学习（行为克隆）基本思想是一致的，即专家动作指导模型的学习，而离线模仿学习，专家数据集是固定的，而在线模仿学习会在学习过程中不断的收集专家轨迹。

#### 基本流程
1. 首先，该策略利用初始专家范例集合 $D$，通过行为克隆来生成策略 $\pi_1$
2. 智能体通过策略 $\pi_1$ 来与环境进行交互生成新的专家范例集 $D_1$
3. 把新生成的专家范例集 $D_1$ 加入总的专家范例集合 $D$
4. 在新的专家范例集合$D$中继续训练




### 应用
- 自动驾驶

## 二、模仿学习与强化学习
### 背景
使用模仿学习可以让智能体在比强化学习短得多的时间内得到与人类操作相近的结果，但是这种做法并不能超越人类。
而强化学习能够得到远超人类的智能体，但训练时间往往非常漫长。

为了

- [模仿学习与强化学习的结合](https://blog.csdn.net/tianjuewudi/article/details/122169309)



