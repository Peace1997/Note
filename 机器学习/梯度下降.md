# 梯度下降
#梯度下降

## 一、场景举例

一个人想要从山上下来，他具有能测量出最陡峭方向的能力，因此需要需要以自己所在位置为基准，寻找这个位置最陡峭的地方，朝着山的高度下降的方向走即可，同理目标是上山，应该是朝着最陡峭的方向向上走。

对应到函数中，就是找到给定点的**梯度**，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就函数在给定点**上升最快**的方向。重复利用这个方法，反复求取梯度，最后就能到达**局部最小值**。
![500][img/gd1.png]


## 二、微分和梯度

- 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率
  $$
  \begin{aligned}
  &\frac{d(5-\theta)^{2}}{d \theta}=-2(5-\theta)
  \end{aligned}
  $$

- 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向
  $$
  \begin{aligned}
  J(\Theta) &=0.55-\left(5 \theta_{1}+2 \theta_{2}-12 \theta_{3}\right) \\
  \nabla J(\Theta) &=\left\langle\frac{\partial J}{\partial \theta_{1}}, \frac{\partial J}{\partial \theta_{2}}, \frac{\partial J}{\partial \theta_{3}}\right\rangle \\
  &=\langle-5,-2,12\rangle
  \end{aligned}
  $$

## 三、 数学解释

$$
\Theta^{1}=\Theta^{0}-\alpha \nabla J(\Theta)
$$

$J$是关于 $\Theta$ 的一个函数，我们当前所处的位置为 $\Theta_0$ 点，我们需要从该点走到$J$的最小值点，首先我们要先确定前进的方向，也就是梯度的**反方向**，然后走一段距离的步长，也就是$\alpha$，走完这个步长，就到了$\Theta_1$点。

> 之所以是梯度要乘以一个负号，意味着我们需要朝着下降梯度相反的方向前进

  $\alpha$在梯度下降算法中被称为学习率或步长，意味着我们可以通过$\alpha$来控制每一步走的距离。$\alpha$不能太大或太小，太大的话会错过最低点，太小的话就会迟迟走不到最低点，消耗时间过大。

![500][img/gd2.png]

## 四、实例

###  1.单变量函数梯度下降

对于一个单变量函数，其函数、微分、起始点、学习率分别为：
$$
\begin{aligned}
J(\theta)=\theta^{2} \\
J^{\prime}(\theta)=2 \theta . \\
\theta^{0}=1 \\
\alpha = 0.4
\end{aligned}
$$
根据梯度下降的计算公式：
$$
\Theta^{1}=\Theta^{0}-\alpha \nabla J(\Theta)
$$
梯度下降迭代计算：
$$
\begin{aligned}
\theta^{0} &=1 \\
\theta^{1} &=\theta^{0}-\alpha * J^{\prime}\left(\theta^{0}\right) \\
&=1-0.4 * 2 \\
&=0.2 \\
\theta^{2} &=\theta^{1}-\alpha * J^{\prime}\left(\theta^{1}\right) \\
&=0.04 \\
\theta^{3} &=0.008 \\
\theta^{4} &=0.0016
\end{aligned}
$$
![500][img/gd3.png]

### 2. 多变量函数的梯度下降

多变量函数、初始位置、学习率、梯度依次为：
$$
J(\theta)=\theta^{2}_1 + \theta^2_2 \\
\Theta^0 = (1,3)\\
\alpha = 0.1 \\
\nabla J(\Theta)=\left\langle 2 \theta_{1}, 2 \theta_{2}\right\rangle
$$
进行多次迭代：

![500][img/gd4.png]

![500][img/gd5.png]



###  3. 线性回归

要求：使用梯度下降的方法来拟合出这条直线

![500][img/gd6.png]

- 首先，需要有个代价函数（损失函数），在这里使用均方误差代价函数
  $$
  J(\Theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\Theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
  $$

  > m是数据集中点的个数
  > $\frac {1}{2}$是一个常量，这样是为了在求梯度的时候，二次方乘下来就和这里的$\frac {1}{2}$抵消了，自然就没有多余的常数系数，方便后续的计算，同时对结果不会有影响
  > y 是数据集中每个点的真实y坐标的值
  > h 是我们的预测函数，根据每一个输入x，根据Θ 计算得到预测的y值，

- 预测函数h
  $$
  h_{\Theta}\left(x^{(i)}\right)=\Theta_{0}+\Theta_{1} x_{1}^{(i)}
  $$

- 代价函数中的变量有两个，所以是一个多变量的梯度下降问题，求解出代价函数的梯度，也就是分别对两个变量进行微分：
  $$
  \begin{gathered}
  \nabla J(\Theta)=\left\langle\frac{\delta J}{\delta \Theta_{0}}, \frac{\delta J}{\delta \Theta_{1}}\right\rangle \\
  \frac{\delta J}{\delta \Theta_{0}}=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\Theta}\left(x^{(i)}\right)-y^{(i)}\right) \\
  \frac{\delta J}{\delta \Theta_{1}}=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\Theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{1}^{(i)}
  \end{gathered}
  $$

- 若进行代码的编写，将公式可以转换为矩阵形式

- 结果
  $$
  \begin{gathered}
  \Theta=\langle 0.51583286,0.96992163\rangle \\
  J(\Theta)=405.984962493
  \end{gathered}
  $$
![500][img/gd7.png]

## 五、总结

- 下山的这个人实际上就代表反向传播算法

  > 梯度下降 是 找损失函数极小值的一种方法。
  > 反向传播 是 求解梯度的一种方法；反向传播只计算梯度，不更新参数。

  梯度下降法：是一种通用的优化算法，中心思想是**沿着目标函数梯度的方向更新参数值以希望达到目标函数最小（或最大）**。梯度下降法是深度学习网络最常用的优化算法。除了深度学习，很多其他场合也会用梯度下降法。

   反向传播：由于深度学习网络按层深入，层层嵌套的特点，对深度网络目标函数计算梯度的时候，需要用反向传播的方式由**深到浅倒着计算**以及更新参数。所以反向传播法是梯度下降法在深度网络上的具体实现方式。

  ==梯度下降法是通用的优化算法，反向传播法是梯度下降在神经网络上的具体实现方式==

- 下山的路径就代表着一直在寻找的参数$\Theta$

- 山上当前点的最陡峭的方向实际上就是代价函数在这一点的梯度方向

- 场景中观测最陡峭方向所用的工具就是微分



### REF

- https://www.jianshu.com/p/c7e642877b0e