# 一、简述


[[深度学习入门#七、 卷积神经网络]]

*与全连接神经网络的区别*
卷积神经网络和全连接神经网络（BP）的根本区别在于连接层之间的**连接模式**。卷积神经网络中相邻层之间的神经元不是完全连接的，而是**部分连接**的，也就是某个神经单元的感知区域来自于上层的部分神经元。

> 在全连接神经网络中，下层的任一神经元都与上层所有神经元相连接，

*为什么需要 CNN？*
BP 神经网络**无法处理多维数据的形状（空间）和数据之间的特征（联系）**。对于图像数据，由于 BP 神经网络下层任一神经元与上层所有神经元相连，虽说处理了整体图像的数据，但是数据之间缺乏联系，而图像数据空间上是有联系的，一个特征可能得需要图像中某个区域的数据共同表征，而 BP 神经网络则把多维数据展平成一维数据处理，自然忽视了多维数据空间上数据的联系。而 CNN 则可以处理这部分空间特征。

> BP神经网络可以看作是一种特殊的卷积神经网络，只是这个卷积核就是某层的所有权重，即感知区域是整个图像，

*CNN是怎么处理特征？*
既然 BP 神经网络直接连接各个神经元会把数据展平，从而忽视区域数据间关联，那怎样使区域间的数据有关联呢，在 CNN 中就通过一个权重窗口（**滤波器**）来实现，滤波器通常是多维的，例如图像是 2 维的，那么滤波器就是一个小的二维矩阵，图像是三维的，滤波器则是一个小的三维的矩阵，滤波器不断与图像中等大小区域的数据进行**点乘**计算，就可以提取图像的特征信息。

> 学习前的滤波器是随机进行初始化的，所以滤波器没有规律可循，通过学习后，滤波器被更新成了**有规律**的滤波器

*卷积神经网络的特点*

**局部区域感知：** 发现数据的局部特征，这些局部特征是描述整个图像的基本组成部分或关键要素。

**权重共享：** *卷积神经网络中每一层有多个Map组成，每个Map由多个神经元组成，同一Map的所有神经元共用同一卷积核，卷积核就是权重，我们并不需要单独计算一个卷积，而是一个固定大小的权重矩阵去匹配整个图像，卷积神经网络的权重共享策略减少了需要训练的参数，使训练出来的模型的泛化能力提高。*

> 处理同一通道的卷积核是相同的，而一个滤波器下的不同通道的卷积核是不同的。


**空间或时间上的采样：** 采样的目的主要是混淆特征的具体位置，当一个特征找出来之后，它的具体位置就不重要了，只需要这个特征与其对应的相对位置即可，这种混淆具体位置的策略，可以对变形核扭曲的图像进行识别。
> 识别数字8时，当我们得到上面一个 o 时，不需要知道它在图像具体位置，只需要判断它下方是否还有o，这样图片中的 8 发生偏移都不影响我们认识它。



补：
滤波器通常称为核、特征检测器
图像上某一区域与滤波器点乘得到的矩阵叫作卷积特征、激活图、特征图

# 二、卷积

卷积的主要目的是为了从输入图像中**提取特征**

***卷积核和滤波器***

- 卷积核就是由长和宽来指定的，是一个二维的概念。
- 过滤器是是由长、宽和深度指定的，是一个三维的概念。
- 过滤器可以看做是卷积核的集合。
- 过滤器比卷积核高一个维度——深度。

对于二维数据，卷积核和滤波器等价，若有 n 个通道，一个滤波器有 n 个卷积核，**一个滤波器就对应一个特征图**。

![500](cnn.gif)

## 1. 卷积核

一个卷积核处理一个通道的数据，一般包括：核大小（Kernel Size）、步长（Stride）以及填充（Padding）

![400](filter1.gif)
再这里核大小为 3* 3 的矩阵，步长是 1，填充为 0，再每个位置（最终输出），都再进行逐元素的乘法和加法。每个滑动位置以一个数字结尾，最终输出为 3x3 矩阵。

### 核大小

**核大小**（Kernel Size）：提取图像信息的大小；卷积核越大，感受野越大，看到的信息越多，从而可以获得更多的全局特征，但是大的卷积核也是会增加计算量，二维卷积核最常见的是 3* 3 的卷积核。对于三维卷积，深度与输入图像深度有关。
+

### 填充

**填充**（Padding）：解决卷积核于图像尺寸不匹配的问题；例如卷积后图片和卷积前的图片尺寸不一致（变小），因此需要对原始图片做边界填充处理。

*为什么需要填充*
- 由于边缘像素不会处于卷积核的中心，
*填充的作用*
- 防止图片的边界信息丢失
- 缩小图像
*填充的方法有哪些*
- **same padding**：对图片边缘信息进行填充，使输入输出图像保持一致
- **valid padding**：不进行任何处理，只使用原始图像，不允许卷积核超出原始图像边界

### 步长

**步长**（Stride）：提取图片信息的精度；每次卷积跨越的长度，步长越小，相邻感受野之间重复区域越多，图片信息提取的约精准，更有效的利用原图信息。



## 2. 滤波器

*图像输入输出通道数的影响因素？*

- 输入矩阵的通道数（C）决定一个滤波器的通道数（卷积核数）
- 滤波器（FN）决定输出矩阵的通道数（特征图数）

![卷积运算的处理流（批处理+偏置）- 方块表示三维数据](conv3.png)



*多通道图像如何处理*
最常见的多通道图像就是彩色照片（RGB 三通道）

![500](filters2.gif)
**分散计算**：在这里输入层是一个 3x5x5 矩阵，有 3 个通道，卷积核是一个 3x3x3 的矩阵，计算时按通道进行输入数据与滤波器的卷积运算（转换为单通道卷积），即执行三次卷积操作，产生 3 个尺寸为 3x3 的通道（3x3x3）。
**逐个相加**：将逐个三个通道逐个元素相加，最终形成一个单个通道（1x3x3）


*什么是 2D 卷积？*

2D 卷积就是，卷积运算是在二维空间上（H，W）滑动，因此2D卷积关注图像二维空间特征，一次计算输出的也是二维数据
使用 Dout 个 filters，将深度为 Din 的层映射为另一个深度为 Dout 的层；

> 虽然有输入数据和卷积核都有多个通道（C），但是进行卷积运算时，他们是**按照通道分开计算**的，因此本质是一次卷积运算还是在二维空间中进行的，因此计算后的数据也是一个二维数据。
> 同理多个滤波器（FN）也是独立计算在拼接的。

**计算公式**：

输入数据维度： $C*W*H$
卷积核个数： $FN$
滤波器维度: $C*FH * FW$
卷积核维度：$FH * FW$
步长 $S$, 填充 $P$，输出数据维度$FN*OH*OW$
参数量：(C* FH* FW +1) * FN    （卷积核维度+偏置维度）

输出高度和宽度的计算公式：
$$  
\begin{aligned} O H &=\frac{H+2 P-F H}{S}+1 \\ O W &=\frac{W+2 P-F W}{S}+1 \end{aligned}  
$$

*什么是 3 维卷积？*

3D卷积需要在三个维度上（C，H，W）滑动，关注的三维空间特征，一次计算输出三维空间特征。相较于2维卷积通道之间独立计算，3维卷积可以考虑不同通道之间的数据联系。


# 三、池化

池化层可以有效的缩小参数矩阵的尺寸，压缩特征图，提取主要特征， **所以加入池化层可以加快计算速度和防止过拟合的作用**。


## 池化方式

-   **MAX池化**：从目标区域中取出最大值。
-   **Average池化**：计算目标区域的平均值。
    

![Max池化的处理顺序](conv4.png)

步幅为 2，进行 2x2 的 Max 池化。其中 2x2 表示目标区域的大小，一般来说，池化的窗口大小会和步幅设定成相同的值。

## 池化特征

-   **没有要学习的参数**：与卷积层不同，池化只是从目标区域中取最大值，所以不存在要学习的参数。
-   **通道数不发生变化**：经过池化操作后，输入数据和输出数据的通道数不会发生变化，池化计算是按通道**独立**进行计算的。而卷积层输出数据的通道数与滤波器的数量有关。   
-   **对微小的位置变化具有鲁棒性**：输入数据发生微小偏差时，池化可能仍会返回相同的结果。
