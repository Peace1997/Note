# 优化器相关内容总结

作用：计算得出loss之后，通常会使用Optimizer对所构造的数学模型/网络模型（权重和偏置）进行参数优化调整。

目标：通常情况优化目标是使得loss趋向于最小（梯度下降）。有时可能需要目标函数越大越好（梯度上升）。

## 一、 常用优化器总结：

在动量和自适应学习率优化方法中都使用了历史梯度信息。

### 1. 梯度下降法（Gradient Descent）

#### 标准梯度下降法（GD）：

在有限视距内寻找最快路径下山。缺点：

* 训练速度慢：每走一步都要计算调整下一步的方向。即每输入一个样本都要更新一次参数，每次迭代都要遍历所有的样本。
* 容易陷入局部最优解：在有限的视距内，如果遇到“平坦洼地”会认为到达最低点，梯度逐渐为0，使得模型不在进行更新。

#### 批量梯度下降法（BGD）：

每次权值调整发生在批量样本输入之后，而不是每输入一个样本就更新一次模型。即在下山前掌握了附近的地势情况，选择总体平均梯度最小的方向下山。相较于GD训练时间短，且每次下降的方向都正确。

#### 随机梯度下降法（SGD）：

盲人下山（随机性+噪声），不用每走一步计算一个梯度，但他总能走到山下，不过过程可能会比较曲折。

$$
\boldsymbol{W} \leftarrow \boldsymbol{W}-\eta \frac{\partial L}{\partial \boldsymbol{W}}
$$

* 加快训练速度；对梯度的要求低，例如在百万数据中，只需要取几百个点，就可以计算SGD梯度，然后更新模型参数。对于引入噪声， 只要噪声不是很大，SGD可以很好的收敛。
* 无法克服局部最优解问题，因为引入噪声，权值更新的方向不一定正确。
* SGD如果函数的形状非均匀，比如呈延伸状，搜索的路径就会非常低效。

### 2. 动量优化法

动量优化方法是在梯度下降法的基础上进行的改变，具有加速梯度下降的作用。需要记录历史梯度。
#### Momentum
* 动量（Momentum）：使用动量的SGD，主要思想是引入一个积攒历史梯度信息的来加速SGD。当前权值的改变会受到上一次权值改变的影响，参照小球在碗中滚动的物理规则进行移动，加速梯度下降。

$$
\begin{gathered} \boldsymbol{v} \leftarrow \alpha \boldsymbol{v}-\eta \frac{\partial L}{\partial \boldsymbol{W}} \\ \boldsymbol{W} \leftarrow \boldsymbol{W}+\boldsymbol{v} \end{gathered}
$$
#### NAG
* 牛顿加速梯度（NAG, Nesterov accelerated gradient）：是Momentum动量算法的变种，添加了一个校正因子，在Momentun中小球会盲目地跟从下坡的梯度，走到坡底的时候速度慢下来而不是又冲上另一个坡。

### 3. 自适应学习率优化算法

####  AdaGrad：

为每个参数适当地调整学习率，具有代价函数（损失函数）最大梯度的参数相应地有个快速下降的学习率，而具有小梯度的参数在学习率上有相对较小的下降。

$$
\begin{aligned} &\boldsymbol{h} \leftarrow \boldsymbol{h}+\frac{\partial L}{\partial \boldsymbol{W}} \odot \frac{\partial L}{\partial \boldsymbol{W}} \\ &\boldsymbol{W} \leftarrow \boldsymbol{W}-\eta \frac{1}{\sqrt{\boldsymbol{h}}} \frac{\partial L}{\partial \boldsymbol{W}} \end{aligned}
$$

#### RMSProp算法：

RMSProp方法并表示将过去所有梯度一视同仁相加，而是逐渐遗忘过去的梯度，再做加法运算时将新梯度的信息更多表现出来，修改了AdaGrad的梯度积累为指数加权的移动平均，使得其在非凸设定下效果更好。

#### AdaDelta算法：

AdaGrad算法和RMSProp算法都需要指定全局学习率，AdaDelta算法不需要设置一个默认的全局学习率，结合两种算法每次参数的更新步长。

#### Adam算法：

结合Momentum和AdaGrad的方法，通过组合两种方法的优点，实现参数空间的高效搜索。同时也加入了超参数的“偏置校正”。

## 二、 学习率：

每次参数更新的幅度大小

* 学习率越低，可以确保我们不会错过任何局部极小值，不过损失函数的变化速度就越慢，收敛慢，容易发生过拟合行为。
* 学习率越高，容易发生梯度爆炸，loss振动幅度较大，模型难以收敛
![300](rate.png)

### 初始学习率选取：

初始选择学习率的时候，是从小到大，因为当学习率小的时候对loss影响不会很大，并且学习率比上一轮大，可以看做是在原始数据进行更新，如果一开始学习率很大对loss影响是很大的，说明神经已经不能很好学习该项任务，那么在不断降低学习率已经没有意义。因此通过打印学习率与loss的变化曲线，来观察选择什么样的学习率会比较好。对于RL来说一般选择0.001 - 0.0001。对于下图中一般选择0.1作为初始lr。

![300](rate2.png)

### 学习率衰减：

与初始学习率的不断递增不同，当我们通过初始率的选取，确定好一个大体学习率后，为了让模型收敛的更好，接下来需要让学习率不断递减。其中常用的方法包括：

* 步衰减（Step Decay）：学习率经过一定数量的训练 epochs 后下降一定的百分比。
* 指数衰减：以指数衰减方式进行学习率的更新，学习率的大小和训练次数指数相关

