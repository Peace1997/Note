#多头注意力机制 #残差结构

# 一、简述

## 什么是Transformer？

Transformer是一种深度学习模型，用于处理序列数据。它是一种**编码器-解码器**模型，其中编码器将输入序列编码为一个固定长度的向量，解码器再将向量解码为输出序列。


## Transformer 优势是什么？

利用注意力机制（多头注意力机制）来提高训练速度的模型，支持并行化计算，由于增加了模型的复杂度，使得它在精度和性能上都要优于 RNN 循环神经网络

> 加强了对注意力的使用，使用多头注意力机制
> 它不像RNN一样对每次都对整个句子加深理解，而是每次都注意到句中不同的部分，提炼对句子更深层的理解。


## Transformer 的结构

主要包含一个编码层（Encoders） 和解码层（Decoders） ，在 Encoders、Decoders 中存在多个小编码器 Encode、小解码器 Decode。

- 【输入层】：输入一个序列数据，并将每个元素映射为一个词嵌入向量
- 【编码器】：由多个自注意力层（self-attention layer）和前馈层（feedforward layer）组成。自注意力层使用 [[注意力机制]]  ，能够根据输入序列的不同部分分别关注不同的信息，前馈层则使用多层感知机来提取高级特征。它的输入是前一个编码器的输出。
- 【解码层】：由多个自注意力层、前馈层和编码器注意力层（encoder-attention layer）组成。解码器注意力层使用注意力机制，能够关注编码器输出的信息。
- 【输出层】：将解码器输出的向量转化为输出序列的每个元素。
对于每个解码器 decode，它的输入 包括两个部分：前一个解码器的输出+整个编码层 Encoders 的输出。

### 编码器
**Encode 模块**： 自注意力机制+前馈神经网络
- 自注意力机制（多头注意力机制）：每个自注意模块，经过矩阵计算（Query、Key、Value），得到一个权重矩阵，所有注意力模块拼接为一个多重权重矩阵。
- 前馈神经网络：由于 transformer 中使用了多个 Encoder、Decoder，为了解决**梯度消失**的问题，每个 Encoder、Decoder 都加入**残差**神经网络的结构，所以前馈神经网络的输入包括自注意力机制和原始数据输入。 

### 解码器
**Decode 模块**：自注意力机制 + 注意力 + 前馈神经网络
- 自注意力机制和前馈神经网络与 Encode 模型是相似的。
- Encoder-Decoder 注意力：利用 Encoder 的输出与 Decoder 的自注意力输出做一次注意力，这样 Decoder 就能参考 Encoder 自注意力的结果，这样自注意力才有意义。

预测模块：Softmax
- 将Decoder模块产生的向量映射到更高维度的向量（logits），用于单词的选择


简略版
![[transformer.png]]
完整版：
![[transformer2.png]]


>  注意力机制的核心：通过Query、Key、Value，快速准确地找到核心内容，换句话说：**用我的搜索（Query）找到关键内容（Key），在关键内容上花时间花功夫（Value）。**


**什么是多头注意力？**
Multi-head Attention
在同一层做注意力计算的时候，同时多做几次注意力（独立）。他扩展了模型关注不同位置的能力，提高自注意力机制层的性能。
>就是多个人帮我同时理解一句话，我在他们理解的基础上，给出自己的答案


