Recurrent Neural Network

# 一、 RNN 

## 简述
CNN 和普通的算法大都是输入和输出是一一对应的，不同输入之间是没有联系的，但是有些场合中，一个输入是不够的，例如处理序列数据时，就需要使用 RNN 来解决。

> RNN 最大的特点就是神经元在**某时刻的输出可以作为输入, 再次输入到神经元**，这种串联的网络结构非常 适合于时间序列数据，可以保持数据中的依赖关系
> 当时间序列数据存在长距离的依赖，并且该依赖的范围随时间变化或者未知，那么 RNN 可能是相对较好的解决方案。


![200](RNN_1.png)

-   **基本运行原理**：RNN跟传统神经网络最大的区别在于每次都会将前一次的输出结果，带到下一次的隐藏层中。

-   **缺点**：
	- RNN有**短期记忆问题**，无法处理很长的输入序列
	- 训练 RNN 需要投入极大的成本


# 二、 LSTM

Long-Short Term Memory

## 1. 简述
RNN是一种死板的逻辑，越晚的输入影响越大，越早的输入影响越小，且无法改变这个逻辑。长短期记忆(LSTM)是一种特殊的RNN， LSTM能够在更长的序列中有更好的表现。简单来说就是**抓重点**。
## 2. 结构

LSTM 的工作方式与 RNN 基本相同，区别在于 LSTM 实现了一个更加细化的**内部处理单元**，来实现上下文信息的有效存储和更新。

### 2.1 整体结构

相比 RNN 只有一个传递状态 $h^t$，LSTM 有**两个传输状态**，一个 $c^t$ （cell state），和一个 $h^t$ （hidden state）。
-  其中对于传递下去的 $c^t$ 改变得很慢，通常输出的 $c^t$ 是上一个状态传过来的 $C^{t-1}$ 加上一些数值。
-   而 $h^t$ 则在不同节点下往往会有很大的区别。  
![300](LSTM_1.png)

***优点***
  - 长期信息可以有效的保留
  - 挑选重要信息保留，不重要的信息会选择“遗忘”
  - 它能够有效克服 RNN 中存在的梯度消失问题

> 使用梯度下降方法来优化 RNN 的一个主要问题就是梯度在沿着序列反向传播的过程中可能快速消失

### 2.2 LSTM内部结构

![500](LSTM_3.png)

  黄色的矩阵表示学习到的神经网络层，粉色圆圈表示逐点运算，每一条黑线传输着一个向量，从一个节点的输出到另一个节点的输入，线合在一起表示向量的连接；分开的线表示内容被复制，然后分发到不同的位置。 

![200](LSTM_2.png)
  $z^f,z^i,z^0$是由拼接向量乘以权重矩阵之后，在通过一个$sigmoid$激活函数转换成0到1之间的数值，来作为一种门控状态。而 $z$ 则是通过一个 $tanh$ 激活函数将转换成-1到1之间的值（这里使用$tanh$ 是因为这里是将其做为输入数据，而不是门控信号)

![500](LSTM_4.png)

#### 门控信号

  LSTM 引入了三个门，记忆门（输入），遗忘门，输出门。输入门决定有多少信息可以流入细胞，遗忘门决定cell有多少信息会被遗忘，输出门决定细胞内多少信息被输出。

***忘记门 :*** $z^f$
对上一个节点传进来的输入进行**选择性忘记**，简单来说就是“忘记不重要的，记住重要的”。

>具体来说就是通过计算得到的 $z^f$ (f:forget) 来作为忘记门控，来控制上一个状态的 $c^{t-1}$ 哪些需要留哪些需要忘记。

***记忆门：***$z^i$ 
  将该阶段的输入有选择的进行“记忆”。主要是会对输入 $x^t$进行**选择记忆**。哪些重要则着重记下，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 $z$ 表示。而选择的门控信号则是由 $z^i$ (i:information) 来进行控制。

**忘记门 + 记忆门：**   $$
    \begin{aligned}
    c^{t} &=z^{f} \odot c^{t-1}+z^{i} \odot z \\
    \end{aligned}
    $$

> 将上面两步得到的结果相加，即可得到传输给下一个状态的 $c^t$。	


***输出门：*** $z^o$

这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 $z^o$  (o:output)来进行控制的。并且还对上一阶段得

到的 $c^o$ 进行放缩（通过一个$tanh$激活函数进行变化）。
$$
    \begin{aligned}
    h^{t} &=z^{o} \odot \tanh \left(c^{t}\right) \\
    \end{aligned}
$$

  
**最终输出**： 与普通RNN类似，输出$y^t$往往最终也是通过$h^t$变化得到。（$\sigma$表示sigmoid）

  $$
  \begin{aligned}
  y^{t} &=\sigma\left(W^{\prime} h^{t}\right)
  \end{aligned}
  $$

注：

LSTM因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。


# 三、GRU

Gate Recurrent Unit

## 1. 简述
- **问题解决**：GRU是RNN的一种，和LSTM一样，是用于解决解决长期记忆和反向传播中的梯度等问题而提出来的。
- **优点**：
  - 相比LSTM，GRU能够达到相当的效果
  - 并且相比 LSTM 更容易进行训练，很大程度上提高了训练效率。

## 2. 结构


### 2.1整体结构

![300](GRU.png)
  GRU的输入输出结构与普通的RNN一样。

  一个当前的输入 $x^t$ ，和上一个节点传递下来的隐藏状态（hidden state） $h^{t-1}$ ，这个隐藏状态包含了之前节点的相关信息。结合 $x^t$ 和 $h^{t-1}$，GRU 就会得到当前隐藏节点的输出 $y^t$ 和传递给下一个节点的隐状态 $h^t$。
  

### 2.2 内部结构

![300](GRU_3.png)

$\odot$： Hadamard Product 阿达玛乘，也就是操作矩阵中对应的元素相乘，因此要求两个相乘矩阵是同型的。
$\oplus$：则代表进行矩阵加法操作。

#### 门控信号

先通过上一个传输下来的状态 $h^{t-1}$ 和当前节点的输入 $x^t$ 来获取两个门控状态。其中 $r$ 控制**重置的门控**（reset gate），$z$ 为控制**更新的门控**（update gate）。

> $\sigma$ ：为 sigmoid 函数，通过这个函数可以将数据变换为 0-1 范围内的数值，从而来充当门控信号。

![300](GRU_1.png)

***重置门：*** $r$

得到门控信号后，首先使用重置门控来得到 “重置之后的” 数据 $h^{t-1^{\prime}}=h^{t-1} \odot r$ ，然后再将 $h^{t-1^{\prime}}$ 与输入 $x^t$ 进行拼接，然后在通过一个 $tanh$ 激活函数来将数据放缩到 -1 到 1 的范围之内，即可得到 $h'$。

![300](GRU_2.png)

这里的 $h'$ 主要是包含了当前输入的 $x^t$ 数据。有针对性的对 $h'$ 添加到当前的隐藏状态，相当于“记忆了当前时刻的状态”。类似于 LSTM 的**选择记忆**阶段。


***更新门***：$z$

在该阶段，同时进行了**忘记和记忆**两个步骤。

首先使用先前得到的更新门控 $z$（update gate）<z的范围为0-1>，然后通过 **更新表达式**：
$$
h^t = (1-z) \odot h^{t-1} + z \odot h'
$$
其中 $z$越接近1，代表记忆下来的数据越多；同理，越接近0则代表遗忘的越多。

> - 前半部分：表示对原本隐藏状态的选择性遗忘，类似遗忘门
> - 后半部分：对当前节点信息进行选择性记忆，类似记忆门

将遗忘门和记忆门进行拼接。


## 3. LSTM 与 GRU

**GRU 只使用了两个个门控 $r、z$ 就可以同时进行遗忘和选择记忆功能，而 LSTM 则需要三个门控

- GRU 的 $h'$ 可以堪称对应于 LSTM 的隐藏状态 (hidden state) $h^t$
- GRU 的上一个节点传下来的 $h^{t-1}$ 对应于 LSTM 的细胞状态 (cell state) $c^t$
- GRU 的 $1-z$ 对应于 LSTM 的遗忘门 $z^f$  (forget gate) ，对**过去**遗忘多少
- GRU的$z$ 对应于LSTM的记忆门$z^i$  (information gate)，对**现在**记忆多少
- GRU 内部少了一个"门控"，参数比 LSTM 少，但是却也能够达到与 LSTM 相当的功能。考虑到硬件的**计算能力**和**时间成本**，因而很多时候我们也就会选择更加”实用“的 GRU 。

