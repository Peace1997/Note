# 一、先导

## 贝叶斯定理

描述在已知条件下，某事件的发生概率。用来描述两个条件概率之间的关系。比如 P(A|B) 和 P(B|A)
$$
P\left(A \mid B\right)=\frac{P\left(A\right) P\left(B \mid A\right)}{P(B)}
$$

扩展：
$$
P\left(A_{i} \mid B\right)=\frac{P\left(A_{i}\right) P\left(B \mid A_{i}\right)}{\sum_{j=1}^{n} P\left(A_{j}\right) P\left(B \mid A_{j}\right)}=\frac{P\left(A_{i}\right) P\left(B \mid A_{i}\right)}{P\left(A_{1}\right) P\left(B \mid A_{1}\right)+\ldots+P\left(A_{n}\right) P\left(B \mid A_{n}\right)}
$$

## 朴素贝叶斯
朴素贝叶斯（naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类方法。

**总体过程**：
- 根据给定的训练数据集，基于特征条件独立假设 学习输入输出联合概率分布P(X,Y)
- 对给定输入x，利用贝叶斯定理求出后验概率分布P(Y | X)


**数学建模：**
- 给定训练数据集：
$$
T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
- 利用训练数据集学习先验概率分布$P(Y)$及条件概率分布$P(X | Y)$，然后得到联合概率分布
$$
P(X,Y) = P(Y)P(X|Y)
$$
- 然后选用极大似然估计或贝叶斯估计作为概率估计方法

条件独立性
