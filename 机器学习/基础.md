

# 最小二乘法

在最小二乘法中，我们通常使用它来拟合一条直线（**线性回归**），使得该直线能够尽可能准确地描述数据点的分布。设我们有$n$个数据点$(x_i, y_i)$，并希望找到一条线性模型：

$$
Y = \beta_0 + \beta_1 x
$$

其中，$\beta_0$是截距，$\beta_1$是斜率。
### 基本原理
最小二乘法的目标是找到$\beta_0$和$\beta_1$的值，使得预测值和实际值之间的误差平方和最小。换句话说，我们希望最小化以下损失函数：

$$
L (\beta_0, \beta_1) = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2
$$
### 求解斜率和截距
对损失函数$L (\beta_0, \beta_1)$分别对$\beta_0$和$\beta_1$求**偏导数**，并令**导数为零**，可以得到$\beta_0$和$\beta_1$的解析解。
1. **斜率$\beta_1$的公式：**
$$
   \beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}
$$
   其中：
   - $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$ 是$x$ 的均值。
   - $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$ 是 $y$ 的均值。
2. **截距$\beta_0$的公式：**

$$
   \beta_0 = \bar{y} - \beta_1 \bar{x}
$$
# 梯度下降法

梯度下降法是一种用于优化和机器学习中常用的迭代方法，目的是找到使目标函数（例如损失函数）最小化的参数值。它的核心思想是根据目标函数在参数空间的梯度信息，通过反复调整参数，使得每次迭代都朝着最小值的方向前进。

### 梯度的含义

梯度 $\nabla f(\theta)$ 是目标函数在参数空间的**偏导数向量**，表示在当前参数位置的变化趋势。梯度的方向是函数值增加最快的方向，而梯度的反方向则是函数值减少最快的方向。因此，梯度下降法的更新步骤将参数沿着负梯度方向前进，以减小目标函数的值。

### 基本概念

假设我们有一个待优化的目标函数 $f(\theta)$，其中 $\theta$ 表示我们希望找到的最优参数。梯度下降法试图通过不断调整参数 $\theta$ 的值来减小 $f(\theta)$ 的值，直到达到局部或全局最小值。

梯度下降的更新公式为：
$$
\theta := \theta - \alpha \nabla f(\theta)
$$
其中：

- $\alpha$ 是学习率（步长），控制每次更新的幅度大小；
- $\nabla f(\theta)$ 是目标函数$f(\theta)$ 对参数 $\theta$ 的梯度，表示函数在当前点的方向和变化率。
### 计算步骤

1. **初始化参数** $\theta$：通常使用随机数初始化。
2. **计算梯度** $\nabla f(\theta)$：计算当前参数 $\theta$下的目标函数梯度。
3. **更新参数**：按照公式 $\theta := \theta - \alpha \nabla$ 更新参数。
4. **检查收敛条件**：判断梯度下降是否收敛，例如，检测梯度的范数是否小于设定的阈值，或者检查目标函数的变化量是否小于一个很小的值。如果满足条件，停止迭代；否则，返回步骤 2 继续迭代。

### 举例：解线性回归

为了展示梯度下降法的计算过程，我们可以用一个简单的例子来说明。假设我们要拟合一个简单的线性回归模型，并使用梯度下降法来最小化误差，从而找到最佳的模型参数。

**一、 问题描述：**
假设我们有以下数据点：$(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$。目标是找到参数 $\beta_0$ 和 $\beta_1$ 使得线性模型 $y = \beta_0 + \beta_1 x$ 能够尽可能准确地拟合这些数据点。

**二、 计算步骤：**
1. **初始化**：随机初始化参数 $\beta_0$ 和 $\beta_1$（例如，可以初始化为 0），设定学习率 $\alpha$ 和迭代次数，并设置损失函数（均方误差）：

$$
J (\beta_0, \beta_1) = \frac{1}{2 n} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2
$$
在损失函数前加上了系数 $\frac{1}{2}$ 以方便求导数时的计算。

2. **计算梯度**：利用下述偏导数公式，计算当前参数下的梯度 $\frac{\partial J}{\partial \beta_0}$ 和 $\frac{\partial J}{\partial \beta_1}$。

$$
\frac{\partial J}{\partial \beta_0} = -\frac{1}{n} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))
$$
$$
\frac{\partial J}{\partial \beta_1} = -\frac{1}{n} \sum_{i=1}^{n} x_i (y_i - (\beta_0 + \beta_1 x_i))
$$

3. **更新参数**：
   - 更新 $\beta_0$：$\beta_0 := \beta_0 - \alpha \cdot \frac{\partial J}{\partial \beta_0}$
   - 更新 $\beta_1$：$\beta_1 := \beta_1 - \alpha \cdot \frac{\partial J}{\partial \beta_1}$
4. **迭代**：重复步骤 2 和 3，直到达到设定的迭代次数或满足收敛条件。

**三、代码实现**
可以使用 Python 编写代码来实现梯度下降：
```python
import numpy as np

# 数据
x = np.array([1, 2, 3, 4])
y = np.array([2, 3, 5, 7])

# 初始化参数
beta_0 = 0
beta_1 = 0
alpha = 0.01  # 学习率
iterations = 1000  # 迭代次数
n = len(x)

# 梯度下降
for _ in range(iterations):
    # 计算预测
    y_pred = beta_0 + beta_1 * x
    
    # 计算损失函数的梯度
    d_beta_0 = (-1 / n) * np.sum(y - y_pred)
    d_beta_1 = (-1 / n) * np.sum(x * (y - y_pred))
    
    # 更新参数
    beta_0 -= alpha * d_beta_0
    beta_1 -= alpha * d_beta_1

print(f"最优参数：beta_0 = {beta_0}, beta_1 = {beta_1}")
```

运行代码后，$\beta_0$ 和 $\beta_1$ 会逐渐收敛到最佳值，即最小化均方误差的参数值，从而得到拟合直线的方程。梯度下降法会在参数空间内搜索，逐步逼近最优解。


# 最小二乘法 vs 梯度下降法

|**特性**|**最小二乘法**|**梯度下降法**|
|---|---|---|
|**解法类型**|解析解|数值解|
|**应用场景**|线性回归及小规模、简单数据|各类优化问题，尤其是非线性和大数据|
|**收敛方式**|直接求得最优解（一次性）|通过迭代逐步逼近最优解|
|**计算复杂度**|较低（适用于小规模问题）|随数据量增大，计算量增大|
|**是否需要学习率**|不需要|需要，学习率对收敛影响较大|
|**是否全局最优解**|对线性模型可求全局最优解|可能收敛到局部最优解|
