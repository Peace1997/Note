
## 任务完成
1. 根据昨天预训练设置的超参数值，进行一次完整的训练，并对训练结果进行分析。
2. 通过 RLlib 训练及 Tune 超参数优化，确定 PPO 算法的相关超参数取值。

------------
由于电脑是轻薄本，训练速度较慢，训练 900个回合（每轮最大步长 5000 ，共计 470w 步）的时间大概为 6小时。通过昨日超参数设置，目前 PPO 算法可以完成收敛的，但是收敛的回报值较低。
![[Pasted image 20240312092939.png]]
通过对 px、pz 以及对动作输出结果分析可得：
- 对于 px 则是由于强化学习的动作输出从 100 s 开始都趋近于 0，从而导致系统响应变慢，该原因可能是由于输出层采用 tanh 激活函数导致，由于 tanh 是输出范围为[-1,1]，我再设置动作空间输出值，限定在了[0,1]，从而使某些结果被截断，接下来将考虑输出层使用 relu 激活函数取代 tanh 
- 对于 pz 频繁震荡是由于在设置 PD 控制器误差项的量纲时，都统一除以了 1000，通过强化学习输出的控制参数取值为[0,1]，从而导致 pz 在某一范围内频繁震荡；

![[Pasted image 20240312092306.png|400]]
![[Pasted image 20240312092325.png|400]]

------------------------
在 PPO 算法中，有些重要的超参数需要调节：例如裁剪策略比率、小批量处理数量

裁剪策略比率的取值范围：0.1、0.2、0.3、0.4
![[Pasted image 20240312110125.png]]
小批量处理数量的取值范围：64、128、256
![[Pasted image 20240312124618.png]]
熵正则化的取值范围：0.0，0.01，0.1，0.2
![[Pasted image 20240312163036.png]]

| 超参数     | 取值范围            | 迭代轮数 | 最优取值 |
| ------- | --------------- | ---- | ---- |
| 裁剪策略比率  | 0.1、0.2、0.3、0.4 | 100  | 0.1  |
| 小批量处理数量 | 64、128、256      | 100  | 256  |
| 熵正则化    | 0、0.01、0.1、0.2  | 100  | 0    |


## 明日安排
- 对 PPO 算法的网络结构中的激活函数进行调整
- 在上一次完整训练过程中发现的问题，考虑对 PD 控制器的相关参数进行适当调整