
在单导弹制导任务中，测试了不同算法、不同状态空间、不同增益参数的对比实验。
在多导弹制导任务中，设计整体实现方案，优化多导弹交互函数的代码，并解决上次多体控制时出现的延迟问题。

# 一、单智能体环境

相同超参数下，在笔记本上简单测试了SAC、TD3 、 DQN算法的训练效果 ，都不如当前使用的PPO算法。
尝试在状态空间增加历史动作信息 ，加入 RNN 和 attention 机制目前并没有对整体训练效果有显著的提升。

受限于笔记本电脑的训练条件，目前选用 PPO 算法进行训练效果最好，目前常用超参数取值为：

| 超参数                           | 取值                                 |
| -------------------------------- | ------------------------------------ |
| lr                               | 0.0001                               |
| gamma                            | 0.99                                 |
| train_batch_size                       | 4000                                 |
| num_workers                      | 1                                    |
| hidden_size                      | [256, 256]                           |
| max_episode_steps                | 1000                                  |
| 任务完成奖励                     | 200                                  |
| 任务失败惩罚                     | -100、-100*(dis)                                 |
| 基于距离奖惩                     | 当前时刻与上一时刻与目标点距离的差值 |
| 能量消耗惩罚                     | -power*（0.3、0.003）                                     |


# 二、多智能体环境

# 模型设计

**任务目标**：在一定时间阈值内，所有智能体到达同一目标区域。

**方案设计**：多个同构的智能体以协作的方式实现同一目标，每个智能体有相同的观测、动作空间，所有智能体可以共用一个奖励函数最大化团队的总收益。因此将多智能体的目标决策建模为 Dec-POMDP，每个智能体基于对环境的局部感知以及相互之间的部分信息共享, 独立进行目标点决策。

**网络设计**：由于每个智能体可以进行独立决策，且拥有相同的输入输出空间，可以借鉴CTDE的框架，同时为简化网络结构，可以所有智能体共用同一 Actor-Critic 网络决策。因此对于强化学习算法，既可以选用基本的MADPPG、QMIX等多智能体算法，也可以选用PPO、A3C、APPO等单体智能体学习算法。
- Actor 网络：输入为单个智能体的局部观测信息，输出为该智能体的动作。
- Critic网络：输入为所有智能体的局部观测信息，输出为对该智能体的动作评价。


## Dec-POMDP 模型

每个智能体的MDP模型如下所示：

**观测空间：**
- 导弹的位置坐标
- 导弹角度
- 导弹的线速度
- 导弹的角速度
- 导弹是否发生接触碰撞
- 导弹与其他导弹的相对位置（距离 & 角度差值）

**动作空间：**
- 垂直推进器
- 侧向推进器

**奖励空间：**

所有智能体共用同一奖励函数：
*协同制导任务奖励：*
- 多体任务完成奖励：在一定时间阈值内，所有导弹的头部与目标区域的地面发生碰撞时会获得一个较大的正奖励。
- 单体任务奖惩：智能体本身与目标区域地面发生碰撞且与其他智能体的距离相对较小会获得一个较小的正奖励（设定一个距离阈值，超出这个阈值即使单体完成头部与目标区域的碰撞也会获得惩罚）
- 距离过大惩罚：任意两个智能体间的距离大于一定阈值时会获得一个较小负奖励

*单体制导任务奖励：*
- 距离差值的目标奖励：t时刻导弹距离目标点的距离与t-1时刻导弹距离目标点的距离差值
- 失败惩罚：
	- 导弹运行过程中，如果导弹与地面发生碰撞会获得一个较大的负奖励
	- 导弹运行过程中，如果导弹头部与地面碰撞，与目标点距离越远惩罚越大
- 能量消耗惩罚：期望消耗更少的能量去到达目标点



PID控制器控制多个导弹的测试结果：
![[Pasted image 20230702170602.png|400]]