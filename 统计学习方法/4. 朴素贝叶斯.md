# 一、 先导

## 条件概率 

$$
P(Y \mid X) = \frac{P(X,Y)}{P(X)}
$$
X、Y 不相互独立

## 贝叶斯定理

P (X, Y) 是 X, Y 同时发生的概率
若 X 和 Y 相互独立，则 P (X, Y) = P (X) * P (Y)
若 X 和 Y 不相互独立，则 P (X, Y) = P (Y|X) * P (X)=P (X|Y) * P (Y)


描述在已知条件下，某事件的发生概率。用来描述两个条件概率之间的关系（非独立）。
$$
P\left(Y \mid X\right)=\frac{P(X \mid Y)P(Y) }{P(X)}
$$

扩展：


# 二、 概述

朴素贝叶斯（naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类方法。

是典型的生成学习方法  —— ([[1. 概述#生成模型与判别模型]]) 


**总体过程**：
- 根据给定的训练数据集，基于特征条件独立假设 学习输入输出联合概率分布P(X,Y)
- 对给定输入x，利用贝叶斯定理求出后验概率分布P(Y | X)


# 三、条件独立性

朴素贝叶斯法对条件概率作了条件独立性假设 
$$
\begin{align*}
P(X=x | Y=c_{k})
&= P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\
&=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) 
\end{align*}
$$
原条件概率分布有指数级数量的参数 $K \prod_{j=1}^{n} S_j$ ；其中 j 表示X的特征种类，$S_j$表示$x^{(j)}$可取值有$S_j$个。

条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的，牺牲部分准确率，降低计算难度。