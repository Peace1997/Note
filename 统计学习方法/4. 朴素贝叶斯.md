#条件概率 #条件概率 #贝叶斯定理 #条件独立性 #朴素贝叶斯分类器 

# 一、 概述

朴素贝叶斯（naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类方法。

是典型的生成学习方法  —— ([[1. 概述#生成模型与判别模型]]) 


**总体过程**：
- 根据给定的训练数据集，基于特征条件独立假设 学习输入输出联合概率分布P(X,Y)
- 对给定输入 x，利用贝叶斯定理求出后验概率分布 P (Y | X)，
- 求出后验概率最大的类标记 y

**数据描述**：

- 训练数据 $T={(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$，共有 N 组数组数据，由 P (X, Y) 独立同分布产生。
	- X 为输入空间
	- Y 为类标记集合

- 特性向量 $x_{i}=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$ 
	- $x_i^{(j)}$ 是第 i 个样本的第 j 个特征，共有 n 个特征；j=1, 2, ... , n
	- $x_i^{(j)}\in \{{a_{j1},a_{j2},...,a_{j{S_j}}}\}$, $a_{jl}$ 是第 j 个特征的可能取的第 $l$ 个值，$l$ =1, 2,... $S_j$

- 类标记 $y_{i}\in \{c_1,c_2,...,c_K\}$ 
	- $c_k$表示$y_i$可能的取值，共有K种不同的取值； k = 1,2, ... , K 

>因为一个实例可能由多个特征，不同的特征有不同的取值集合，例如有 $X^{(1)},X^{(2)}$ 两个特征 $A_1 =\{1,2,3\} ,A_2=\{S,M,L\}$
> 而类标记特征通常仅有一个，例如 $Y\in C=\{1,-1\}$ 


# 二、条件独立性

朴素贝叶斯法对条件概率作了特征条件独立性假设 
$$
\begin{align*}
P(X=x | Y=c_{k})
&= P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right) \\
&=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right) 
\end{align*}
$$

这是一个较强的假设，条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的，模型的包含的条件概率的数量大量减少，学习与预测过程进行简化，降低计算难度，因此朴素贝叶斯方法高效且易于实现，缺点是分类性能不一定很高。


# 三、朴素贝叶斯分类器

**贝叶斯定理**：
$$
P(Y \mid X)= \frac{P(X,Y)}{P(X)}=\frac{P(X \mid Y) P(Y)}{P(X)}
$$

我们将条件独立性公式带入[[贝叶斯定理]]公式可得到后验概率： 
$$y=f(x)=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X_{j}=x^{(j)} | Y=c_{k}\right)$$
从 K 个可能的后验概率中选取最大的。

**后验概率最大化**：
将实例分到后验概率最大的类中，这等价于期望风险最小化。

课本p61推到了，利用0-1损失函数，根据期望风险最小化准则得到了后验概率最大化准则，即朴素贝叶斯所采用的原理。


# 四、参数估计

通过极大似然估计法、贝叶斯估计法 去估计相应的概率（先验概率、条件概率），根据估计的概率去计算后验概率

## 1. 极大似然估计

利用现有样本，反推概率模型

**先验概率**的极大似然估计：
$$
P(Y=c_{k}) =\frac{\sum_\limits{i=1}^{N} I(y_i=c_k)}{N} 
$$
**条件概率**的极大似然估计：
$$
P(X^{(j)} = a_{jl} \mid Y= c_{k})= \frac{\sum\limits_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_\limits{i=1}^{N} I(y_i=c_k)}
$$
[[常用解释#极大似然估计]]

## 2. 贝叶斯估计

问题解决：极大似然估计时出现所要估计的概率值为 0 的情况，这会影响到后验概率的计算结果，从而使计算出现偏差

**先验概率**的贝叶斯估计：
$$
P(Y=c_{k}) =\frac{\sum_\limits{i=1}^{N} I(y_i=c_k)+\lambda}{N+K\lambda} 
$$
**条件概率**的贝叶斯估计：
$$
P(X^{(j)} = a_{jl} \mid Y= c_{k})= \frac{\sum\limits_{i=1}^{N}I(x_i^{(j)}=a_{jl},y_i=c_k)+\lambda}{\sum_\limits{i=1}^{N} I(y_i=c_{k)}+S_j\lambda}
$$
其中 $\lambda \geq 0$, 满足：
$$
P(X^{(j)} = a_{jl} \mid Y= c_{k}) > 0
$$
$$
\sum\limits_{l=1}^{S_j} P(X^{(j)} = a_{jl} \mid Y= c_{k})=1
$$
$\lambda=0$ 为极大似然估计，$\lambda=1$ 为拉普拉斯平滑，


# 五、朴素贝叶斯算法

**问题解决**：通过朴素贝叶斯算法完成对实例 x 的分类

**步骤**：
- 利用极大似然估计法/贝叶斯估计去计算先验概率 $P(Y=c_k)$ 和条件概率 $P(X^{(j)} = a_{jl} \mid Y= c_{k})$
- 利用贝叶斯定理与学到的联合概率模型（先验概率、条件概率）计算后验概率：
	- 对于给定的实例（特征向量）$x_{i}=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T$  —— 例如 ：$x=(2,S)^T$ 去计算：
$$
P\left(Y=c_{k}\right) \prod_{j=1}^{n} P\left(X_{j}=x^{(j)} | Y=c_{k}\right)
$$
- 通过朴素贝叶斯分类器，将实例分到后验概率最大的类中。



总结：
- 反向推导：朴素贝叶斯法将实例分到后验概率最大的类中，后验概率公式需要利用贝叶斯定理结合特征条件独立性得出，得到的后验概率公式的计算需要我们对输入输出的联合概率（先验概率、条件概率）分布进行学习（估计），估计先验概率、条件概率的方法有极大似然估计和贝叶斯估计。