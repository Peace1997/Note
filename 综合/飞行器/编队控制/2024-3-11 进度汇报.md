
### 任务完成
1. 通过较少轮数的预训练设置初始超参数。利用 Ray Tune 对强化学习算法的关键超参数进行调节，其中包括：学习率、折扣因子，批处理数量。
2. 对训练数据进行记录，其中包括强化学习状态信息和动作信息。

--------
对于学习率、折扣因子和批处理数量分别进行 100 次迭代训练，随机设置每轮训练的编队构型，采取平均回报最高的超参数，作为接下来强化学习训练的初始超参数。
- 学习因子取值：0.001、0.0005、0.0001、0.00005
![[Pasted image 20240311142941.png]]
- 折扣因子取值：0.9、0.99、0.999
![[Pasted image 20240311130203.png]]
- 批处理数量：2000、3000、4000、5000
![[Pasted image 20240311172920.png]]

| 超参数   | 取值范围                        | 迭代轮数 | 最优取值   |
| ----- | --------------------------- | ---- | ------ |
| 学习因子  | 0.001、0.0005、0.0001、0.00005 | 100  | 0.0005 |
| 折扣因子  | 0.9、0.99、0.999              | 100  | 0.9    |
| 批处理数量 | 2000、4000、8000、10000        | 100  | 5000   |

-----------------
对训练状态信息进行记录：
![[Pasted image 20240311161617.png]]

## 明日安排
- 明确 RLlib 中 PPO 算法的网络模型结构，并对相关参数进行调节，确定训练的初始超参数，例如：SGD 批量大小、GAE 参数、熵系数等
- 根据RLlib训练结果进行 Reward Scaling，对原环境奖励缩放到一个合适的数值范围