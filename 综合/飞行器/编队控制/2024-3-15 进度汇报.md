## 任务完成
- 增加编队控制训练难度，对编队控制场景进行适当调整。
- 降低学习率，解决训练过程中回报震荡和训练结果不稳定性的问题。
--------
为使编队控制任务更加稳定，增加任务完成难度，由持续 200 步低于 100m 的指标增加至 500 步。
为解决编队控制训练过程中奖励回报震荡和训练不稳定性的问题，降低 PPO 算法的学习率，进行训练，通过对几个不同的学习率进行测试，当前最优的回报曲线如下图所示：
![[Pasted image 20240315145708.png]]
相较于此前，收敛后稳定性得到了提升，但是训练速度和收敛后的回报较之前有所下降, 一方面是由于增加训练难度，导致交互步长增加，另一方面可能是由于降低了学习率，采用梯度裁剪的方法对梯度进行限制，对网络性能产生影响，同时其他超参数（如折扣因子，批处理数量等）也需要适当的进行调整。

综合这周训练情况，目前在某些简单场景中，已经初步具备智能状态，接下来将会在此基础上，对相关超参数进行微调，并加入相关技巧对网络模型进行优化，进一步提升网络模型效能，增加适应性。

## 下周安排

- 完成算法验证端的开发，实现算法模型的导出、转换和导入
- 自定义 PPO 策略网络和价值网络，加入正则化层 LayerNormalization
- 设计更加复杂的训练环境（修改主弹的运动性能，例如增加侧向过载等；考虑在状态空间中加入噪声），并设计相应的测试环境进行算法验证
- 学习 Ray 的并行运算机制
- 根据训练结果继续进行调参