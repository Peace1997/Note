
## 基础

***方法分类：***
- 基于控制的方法：MSR；从数学角度证明可达到一致性；针对整体建模，邻接权重均分，当存在问题节点时，对好坏节点进行选择更新自身状态。
- 基于强化学习的方法：Q-consensus；利用强化学习进行权重分配，通过一定的条件判断一致性；重点关注信任评估/权重分配，正常节点邻接权重被分配较大的值。

***难点***
- 通信量大小：期望以较少的通信量实现收敛
- 动态拓扑：拓扑结构不是静态固定的；可以是与某一节点随机通信，也可以是与不同节点随机通信
- 通信时延：通信过程中，不能及时获得当前时刻邻居节点的状态
- 容错：存在错误节点信息作为干扰
- 收敛时间：收敛时间长短可以作为判断算法效用的评价指标

***关键点***
- 拓扑结构的设定；不同的拓扑结构，需要针对设定不同的算法； 生成树、强连通性等要求
- 错误节点：错误节点的个数要满足一定的约束
- 建模过程：强化学习重点关注权重分配，通过修改邻接权重来达到一致性，传统控制方法从整体角度去理论证明。


**RL解决一致性问题本质**：优化问题，通过优化每个节点相邻权重的值，识别出错误节点的同时（错误节点权重降低），以最短的时间使正常的节点的状态达到一致。

邻接权重 $x_{i,j}$ ： j 节点指向 i 节点的权重值大小
> 通常存在一个指向自身的权重，该权重大小固定


# 论文

## 一、 Group Partition and Group Information based Multi-agent Consensus Systems
基于组划分和组信息的多智能体一致性系统
### 简述

研究背景：在一致性问题中，每个智能体通常利用智能体间的相对状态去更新自身的状态，但是在某些场景中，由于隐私保护、系统测量误差，智能体间的相对状态通常无法获得，在此情况下需要提出一种新的策略实现一致性策略。

问题解决：本文通过组划分和组间信息替代智能体间相对状态来实现一致性算法。

整体过程：首先，在每一时刻通过一定的概率对所有智能体划分为2个或2个以上的子组，每个子组内的智能体在通过与之对应的子组信息更新自身的状态。


### Gossip 算法

Gossip 是一个最终一致性算法。虽然无法保证在某个时刻所有节点状态一致，但可以保证在”最终“所有节点一致"。

因为Gossip不要求节点知道所有其他节点，因此又具有去中心化的特点，节点之间完全对等，不需要任何的中心节点
>   Gossip is a final consistency algorithm. Although the state of all nodes cannot be guaranteed to be consistent at a certain time, it can be guaranteed that all nodes are consistent at "final".
> Gossip does not require nodes to know all other nodes, so it has the characteristics of decentralization. Nodes are completely equivalent, and no central node is required


## 二、Self-Learning based Distributed Resilient Consensus for Multi-Agent Systems









通过强化学习方法解决**多智能体弹性一致性（Multi-agent Resilient Consensus (MARC)）问题**，在故障节点存在的情况下，以分布式方式实现状态一致性。提出了两种解决方法:
- D-OPDG（Distributed On-Policy Deterministic Policy Gradient）：对 MARC 问题进行建模为 MDP，通过梯度上升策略训练和学习每个节点的最优策略。
- 为解决神经网络及其耗时训练的问题，提出了一种基于 Q-learning 的自适应率 Q- consensus 方法。通过对每对智能体建立合理的奖励函数和可信度函数，使相邻权值自适应更新。


第一种方法符合 MDP 模型，根据策略网络输出动作，然后依据回报来调整策略。
第二种方法没有动作，符合MRP模型。动作的输出是一种启发式的基于历史信息的长期评估Q来确定的，Q的确定是基于历史的奖励来确定的。这个过程中没有涉及神经网络。并解决了固定拓扑和可变拓扑的


多智能体容错问题中，状态动作对和奖励之间的关系是已知的，且对于每个状态-动作对，在下一瞬间存在唯一的状态，状态转移概率为 1


### 基础

***三种节点***
- 正常节点 Normal Node
- 随机故障节点 Persistent Faulty Node (PFN)：以一定概率 $p_p$% 输入随机值，1 - $p_p$%概率为正常节点。
- 恒定故障节点 Constant Faulty Node (CFN)：以一定概率 $p_c$% 输入为零，1 - $p_c$%概率为正常节点。


### D-OPDPG
***建模***

State：相邻节点的权重、相邻节点与自身状态的差值：$s_{i, k}:=\left\{x_{j, k}-\right.$ $\left.x_{i, k}, \alpha_{i j, k}, \forall j \in N_i\right\}$
Action：所有相关的相邻节点的权重： $a_{i, k}:=\left\{\alpha_{i j, k}, \forall j \in N_i\right\}$.
Reward：相邻节点的状态的差值越小，回报越大： $r_{i, k}:=$ $f\left(\sum_{j \in N_i} \alpha_{i j, k}\left|x_{j, k}-x_{i, k}\right|\right)$ 
> f 为反比例函数

利用自己生成和采集的信息进行更新。
![500](D-OPDPG.png)
直接使用回报来计算梯度来更新策略网络，

评价：
-  耗时、计算量大，当动作空间较大时，缺点更为明显。


###  Q-consensus

基于强化学习中的 Q-learning 算法， 提出了一种基于 Q 一致性（Q-consensus）的算法来识别和隔离 MARC 问题中的问题节点



#### FQ-consensus
在固定通信拓扑下基于Q-consensus的算法
![500](FQ-consensus.png)

#### SQ-consensus

在随机通信拓扑下基于Q-consensus的算法
![500](SQ-consensus.png)

### 实验设计

***固定有向拓扑下的弹性一致性验证***

- D-OPDPG ：
	- 不同节点相邻权重变化曲线(噪声大小未知？)
	- 节点状态变化曲线（固定两种问题节点，及其失败概率）
- FQ-consensus：
	- 不同问题节点组合，以及不同失败率下，平均收敛时间
- 两个超参数（噪声、收敛条件）对成功率的影响：
	- 固定问题节点、噪声阈值$w$ ，收敛成功率与收敛条件的关系
	- 固定问题节点、收敛条件$\epsilon$，收敛成功率与噪声阈值的关系
- D-OPDPG 算法与 FQ-consensus 算法对比实验
	- 较大噪声下，FQ-consensus算法与D-OPDPG算法对比

***交互拓扑下弹性一致性验证***

> 交换拓扑： 所有节点在每个时刻都以一定的概率相互连接
- SQ-consensus
	- 不同问题节点的组合下不同连接概率下的各节点的状态
	- 不同问题节点组合下不同链接概率下的平均收敛时间




背景：

邻居的 ROR 信息是错误的，随机值（自行设定），


针对单个任务Task2
Sensor 五做错误节点
使用 Q-consensus、dpog


8个sensor 2个错误
网络拓扑问题（双层网络）：主要针对Sensor之间传递错误的情况。

Task对Sensor的奖励分配可以限定一下条件





问题总结：
- 画图工具
- 

## 三、Fairness based Multi-task Reward Allocation in Mobile Crowdsourcing System

### 基础

**问题解决**： 实现公平性奖励分配一致性问题；优化的目标为RoR（回报比率）

***场景：***
Mobile Crowdsourcing System（移动众包系统），一个典型的移动众包系统通常由众包平台、任务发布者和任务完成者 (移动用户) 组成。它利用智能手机用户的众包感知数据，而不会给数据感知和收集的额外成本带来任何负担。然而，用户参与众包会产生电池、带宽等资源成本，因此设计激励机制来**推动用户参与**就显得至关重要。

本文重点研究了一个多任务系统，其中每个用户可以同时加入多个任务。

`“众包”（crowdsourcing）是混合群众（crowd）和外包（outsourcing）的混合词，个人或组织可以利用大量的网络用户来获取需要的服务和想法。`


***目标***

在给定的报酬预算下，**公平地**分配每个任务的报酬给用户，使得每个用户都能根据自己的花费得到相应的报酬。即**奖励分配问题**。

***研究现状***

- 大多数采用集中式方法，要收集所有用户信息，成本增加，可能会存在单节点故障问题；
- 分布式的方法通常适用于大规模场景，需要重点关注的**公平性**问题，即分配给每个参与者的奖励
要与参与者的成本相匹配，若当前未能吸引用户注意力，则是激励用户参与群体感知任务的关键。


***关键：***
设计合理激励机制（incentive mechanisms），用户奖励分配不但要根据用户的贡献还要考虑公平性因素（用户成本）。引入**收益率**（Rate of Return，RoR）概念，定义每个用户所获的**回报**与所付出的**成本**之比，以此来证明公平性的性质。

***实现：***
为保证多任务系统中每个用户奖励分配的公平性，针对不同场景的需求设计了三种算法，以集中式或分布式的方法来优化公平函数（fairness function）来同步 RoR 值。

**- 一致性的奖励分配算法（Consensus based Reward Allocation ，CRA) —— 集中式系统公平性**
- 基于一致性的平衡拓扑奖励分配算法（Consensus based Balanced topology Reward Allocation ，CBRA）—— 分布式系统公平性
- 基于流言的奖励分配 （Gossip based Reward Allocation ，GRA)  —— 分布式系统公平性


***创新点***
- 提出了一种新的衡量系统公平性的指标 RoR。
- 针对不同的场景提出了三种奖励分配算法：一般集中式网络、平衡拓扑分布式网络和稀疏分布式网络。

### CRA

引导用户集合根据邻居信息来调整自身的奖励，使得 RoR 值趋于一致性，通过局部交互达到全局目标。

流程：
- 每个用户$i$可以获得它自身和邻居的 RoR 信息，根据这些信息更新自己的奖励$r^l_i$
- 每个用户向服务端汇报自己每个任务 $l$ 的奖励 $r^l_i$，并得到一定的反馈信息 $h^l$，每个用户通过乘以 $h^l$ 来调整自身奖励，从而保证总奖励有界。
- 每个用户将自己的成本分配给其参与的所有任务，使所有参与任务的RoR值相等。

![500](CRA.png)


### CBRA
基于 CRA 的思想，以分布式的方式运行，取消了服务器与用户之间的通信，但这需要满足拓扑平衡

![500](CBRA.png)


### GRA

基于 Gossip 的奖励分配算法，以分布式的方式运行。

在该算法中，
- 每次迭代中，随机选择具有相同任务的两个用户 $i,j$ 进行交互，两者共享彼此的 RoR 值以及成本值，
- 然后更新他们的奖励，使 RoR 的值逐渐达到一致。
- 用户 $i,j$ 将自己的成本 $C_i,C_j$ 分配给他参与的所有任务，使得它们参与的每个任务的 RoR 值相同

![500](GRA.png)



- 对于 D-OPDPG 和 FQ-consensus 的收敛图是画成怎样的，横纵坐标是怎么表示的（有包括图 3、4、5 等不同的类型，以及是否考虑错误节点的类型等问题）








因为固定任务 $l$，对每个 Sensor $i$ 的 $c_i$ 值也是固定的
对于奖励计算公式：
$$r_i^l(k+1)=r_i^l(k)+c_i^l(k)[\sum_{j \in N_i^l(k)} a_{i j}^l(k).\left.\left(z_j^l(k)-z_i^l(k)\right)\right]$$
对于 RoR 计算公式：
$$z_i^l(k)=r_i^l(k)/c_i^l(k)$$

**整体方案**：利用OPDPG和FQ-consensus不断迭代调整 $a_{i j}^l(k)$ 的值，更新$r_i^l(k+1)$ ，进而使每个 Sensor 的 RoR 值（$z_i^l(k)$）达到收敛。 收敛之后确定每个task分配给每个Sensor的奖励$r_i^l$

![500](CRA.png)
**问题总结：**
1. Algorithm 1中 $x_i^k(k)$ 的含义是什么？是否这里应该是$z_i^k(k)$？在固定任务，权重 $a_{i j}^l(k)$迭代更新过程中$h^l(k)$是不是就不用进行考虑。
2. 最后的收敛图应该是怎么样画。横纵坐标分别代表什么，应该以原文图3、4、5中是哪些为参考。以及对噪声阈值$w$ 和收敛条件$\epsilon$的限定有要求。