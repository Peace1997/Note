
下面给你一套**专门针对候选人常琳楠（强化学习 + 全身运动控制方向）**的**完整面试提纲（60 分钟）**，适用于你们部门：  
**“强化学习全身运动控制”**。

这套题目覆盖：

- 项目深度验证
    
- RL + 全身控制核心能力
    
- 多模态感知与融合（他简历重点）
    
- 分层 RL / 协同控制（他主线）
    
- 动力学基础
    
- Sim2Real
    
- 工程落地能力
    

每个问题我都帮你“暗中”对应到候选人的项目点，能精准判断他到底是“做过”还是“看过”。

---

# ⭐ **一、项目深度验证（15 分钟）**

（确认是否真正做过六足 / 四足 / Manipulation 协同 + RL）

---

## **Q1. 你在 Go2 + 机械臂的协同 RL 项目中，高层策略输出什么？低层策略控制什么？它们之间如何解耦？**

**为什么问？**  
他写了“扭矩控制 + 分层 RL”，但不一定真正理解分层结构。

**期待回答：**

- 高层：机体姿态调整、机械臂 EE target、前进方向
    
- 低层：足端位置/接触模式/扭矩控制
    
- 解耦方式：
    
    - action space 分割
        
    - latent coordination
        
    - 或者 SATA 架构里的 torque-level adaptation
        

**危险信号：**  
“高层管规划，低层管执行”这种空话。

---

## **Q2. 你们如何处理机械臂运动导致四足稳定性被破坏？你们引入了哪些奖励（或模块）来保证 stability？**

**期待回答：**

- base 角速度/姿态稳定奖励
    
- CoM shift compensation
    
- contact consistency
    
- torque smoothness
    
- foot clearance
    
- 机械臂动作的 penalty / phase constraints
    

---

## **Q3. 你在六足的 HeLoM 中提到“基于动捕数据”，动捕数据在模型中扮演什么角色？**

**期待点：**

- imitation learning（BC 或 adversarial）
    
- reference motion normalization
    
- gait phase learning
    
- behavior cloning warm start
    

---

## **Q4. KiVi 框架中，你们是怎么将视觉 + 本体感知进行融合的？什么时候视觉失效？如何 fallback？**

**期待回答：**

- proprioception (IMU, joint) as primary
    
- visual feature embedding
    
- gating network 或 modal selection
    
- occlusion → fallback to proprioceptive policy
    

如果回答“直接拼接特征”那就是浅显理解。

---

# ⭐ **二、强化学习 + 全身运动控制核心（20 分钟）**

---

## **Q5. 为什么在全身控制中常用 PPO？相比 SAC / TD3 有什么优势？**

**优秀回答：**

- PPO 更稳定
    
- 不需要太多超参
    
- 适用于大 action space
    
- 轨迹优化/clip 保证训练不会崩
    
- SAC 在 torque control 可能过于 aggressive
    

---

## **Q6. 你能解释 PPO 里 “Proximal（近端）” 的含义吗？为什么要限制更新幅度？**

**期待回答：**

- trust region approximation
    
- 防止策略 collapse
    
- clip 限制更新范围
    
- 保证控制任务稳定性
    

他在你之前的问题回答过，应该能讲对。

---

## **Q7. 你们的低层 locomotion（SATA）实现了零样本 sim2real，它最关键的设计点是什么？**

**期望回答：**

- torque-level control（减小 sim2Real gap）
    
- strong domain randomization
    
- implicit actuator model
    
- compliant contact reward
    
- smooth PD 或 torque mapping
    

**重点：**  
能否讲出 SATA 的本质（safety + torque adaptation），否则说明只“照着代码跑”。

---

## **Q8. 你们如何解决 RL 中的“接触不一致（contact inconsistency）”？**

**优秀回答：**

- contact velocity penalty
    
- phase-based swing/stance
    
- don’t reward foot slip
    
- contact scheduling
    
- or using foot height / contact classifier
    

---

## **Q9. 六足 / 四足 RL，请举例一个 reward shaping “让机器人学会某个特定动作”的场景。**

例如：

- 上台阶 → foot clearance 高频激励
    
- 搬运重物 → base stabilization term
    
- 机械臂 reach → EE position reward
    

看是否能给出具体数学形式。

---

## **Q10. 在全身 RL 中 action space 很大，你如何避免高维控制导致 training collapse？**

**期待：**

- 分层
    
- 动作解耦（base vs legs vs arm）
    
- latent action
    
- policy modularization
    
- regularization / action penalty
    

他说过“分层控制”，必须能讲清楚。

---

# ⭐ **三、动力学 / 控制基础（15 分钟）**

---

## **Q11. 请解释：为什么扭矩控制（torque control）比位置控制更适合 sim2real？**

**期待回答：**

- actuator model mismatch smaller
    
- position → 多环控制（位置/速度/电流）误差大
    
- torque → 直接驱动
    
- 更自然、更 compliant
    
- 适合 RL 输出
    

---

## **Q12. Newton–Euler / RNEA 是怎么计算力矩的？对 RL 有什么意义？**

**期待点：**

- forward pass 计算速度、加速度
    
- backward pass 求关节力矩
    
- RL 需要近似这些动力学（implicit learning）
    
- domain randomization 需要基于动力学参数
    

他不是做动力学的人，通过这题可查基础深度。

---

## **Q13. 奇异性（singularity）在全身控制中有什么影响？RL 会学到奇异区吗？**

**期待：**

- Jacobian rank deficiency
    
- torque amplification
    
- foot/arm pose 不可达
    
- RL 可能自动学到“腿伸直”这种危险姿态
    
- 所以需要 joint limit、orientation regularization
    

---

## **Q14. 为什么 HeLoM / KiVi 等算法常结合 MPC/WBC？你认为 RL 能完全替代传统控制吗？**

你希望听到：

- RL 擅长隐式动力学适应
    
- 传统控制有可解释性 + 安全性
    
- 最佳是 hybrid RL + MPC/WBC
    

如果他说 RL 取代一切 → 危险。

---

# ⭐ **四、工程能力 / Sim2Real（10 分钟）**

---

## **Q15. 你提到 Go2 locomotion 完成过 sim2real，你们遇到过哪些 sim2real gap？是怎么解决的？**

应该讲到：

- latency
    
- friction mismatch
    
- IMU drift
    
- contact modeling
    
- joint backlash
    
- torque smoothing
    
- domain randomization
    

---

## **Q16. 如果我们让你做“全身模仿学习 + RL”，你会设计怎样的 pipeline？**

**期待：**

- 动捕（AMASS/SMPL）
    
- retarget
    
- normalize + phase
    
- imitation pretrain（AMP）
    
- RL finetune
    
- WBC executed on real robot
    

---

## **Q17. 假设机器人在现实里总是步态抖动，你怎么定位问题？**

期待：

- 传感器噪声
    
- 控制频率 mismatch
    
- action blowup → clip
    
- latency compensation
    
- torque saturation
    
- contact mis-detection
    
- reward shaping 问题
    

---

# 🎯 **最终通过这套题能看出：**

- 这个候选人是“真的动手做 RL”，还是只跑过代码
    
- 是否知道 RL 全身控制的核心痛点
    
- 是否懂得动力学 / 控制理论
    
- 是否具备 sim2real 的实战意识
    
- 是否能胜任你们部门的“全身 RL 控制”工作
    

---

如果你需要，我还能继续帮你：  
🔥 **每题评分标准（优秀 / 合格 / 不合格）**  
🔥 **对该候选人的风险点预测（基于他简历）**  
🔥 **输出一套“高难度 RL + 全身控制”的面试题**

你想要哪个？