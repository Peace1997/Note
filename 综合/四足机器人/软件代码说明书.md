# 软件代码说明书

## 内容
- 如何进行训练
- 如何进行测试
- 核心函数解释
- 参数说明
- 修改文件

注：
代码整体框架见：代码整体框架.pdf
其中在源代码基础上新建文件的在代码整体框架. pdf 中标红显示
在源代码基础上修改的文件见：修改文件

## 如何进行训练

1. 首先在  *legged_gym/utils/helpers. py* 文件的 `get_args()` 函数中设置 `--task` 标签的值，`quadruped_arm_flat` 表示在平坦环境下运行，`quadruped_arm_rough` 表示在复杂环境下运行；
2. 跟据所选的环境，在 *legged_gym/envs/quadruped_arm* 文件下选择合适的强化学习算法-环境派生类文件 *quadruped_arm_flat_config.py* 或 *quadruped_arm_rough_config.py* ，对其中智能体数量、初始姿态、环境信息、奖励函数、命令空间进行修改设置；
3. 当配置好算法、环境等信息后，运行 *legged_gym/scripts/train. py* 文件开始进行训练。

## 如何进行测试

1. 与进行训练步骤的前两步相同，选择合适的环境，配置算法和环境超参数的信息
2. 若是对平坦环境进行测试，则运行 *legged_gym/scripts/play_flat. py* 文件，若是对复杂环境进行测试，则运行 *legged_gym/scripts/play_rough. py* 。测试的主要内容包括每个模块的运行时间以及失败次数，对复杂环境测试时，需要额外设置环境的参数。若是简单对平坦环境、复杂环境测试，可以直接运行  *legged_gym/scripts/play. py* 文件。


## 核心函数解释


 `make_env(name,env_cfg) -> env,env_cfg`
*legged_gym/utils/task_registry. py*

- 输入：环境名称，环境配置参数
- 输出：实例化环境，环境参数

1. 获取我们设定的矢量化环境 `task_class` 及其对应的环境配置参数 `env_cfg` 
2. 获取仿真平台 Isaac Gym 的参数 `sim_params` 
3. 根据环境配置参数和仿真平台参数实例化环境`env`


 `make_alg_runner(env,name,args) -> runner,train_cfg`
*legged_gym/utils/task_registry. py*

- 输入：实例化环境，环境名称、环境参数
- 输出：实例化算法，训练参数

1. 获取算法训练参数 `train_cfg`
2. 根据算法训练参数实例化算法 `runner`
3. 判断是否需要加载之前的模型


 `learn(num_learning_iterations)`
*rsl/runners/on_policy_runner.py*

主函数，输入最大迭代后，智能体不断与环境交互，更新网络，定期保存网络模型。

1. 获取智能体的状态信息 `obs = get_observations()`
2. 将状态信息输入到 actor 网络中得到接下来要执行的动作 `actions`
3. 执行动作得到新的状态和奖励`obs,rewards,_ = env.step(actions)`
4. 将状态、动作、奖励存储到回放池 `storage`
5. 不断重复1-4步，直至达到设定的回合步`num_steps_per_env`
6. 从回放池中多次取出 `mini-batch` 数据用于神经网络的更新，并定期保存网络模型

 
 `step(actions) -> observation 、reward 、 reset `
*legged_gym/envs/ base/legged_robot.py*

- 输入：智能体的动作信息
- 输出：智能体的状态、奖励、是否达到终止条件

1. 首先根据设定的动作最大动作范围（`clip_actions`）进行裁剪，
2. 因为输出的动作是目标关节位置，需要通过 `_compute_torques` 函数转换为力矩信息后，在输入 `set_dof_actuation_force_tensor`、`simulate`  函数完成仿真执行命令
3. 然后根据设定的 `decimation` 的大小，重复执行第 2 步操作
4. 动作执行完成后，执行 `post_physics_step` 函数来更新下一时刻的状态 `compute_observations` 、计算奖励 `comput_reward` 、检查重置 `check_termination` 



 `make_terrain(self,choice,diffcult) -> terrain`
*legged_gym/envs/utils/terrain.py*

- 输入：环境类型、环境难度
- 输出：创建好的复杂地形

1. 根据设定的环境类型 `choice` 和环境难度 `diffcult` 创建相应的子环境
2. 在 Isaac Gym 中提供了 `terrain_utils` 的功能包，目前对这个功能包进行了开源，因此我们可以自定义上述的地形，例如设定斜坡、障碍物高度，楼梯的高度、层数。而且也可以进行拼接，例如粗糙的地形可以和斜坡地形拼接在一起。
	-  `pyramid_sloped_terrain` : 用于生成金字塔式斜坡地形，可以自己设定斜坡的角度和高度
	- `random_uniform_terrain` ：用于生成粗糙地形，可以自己设定粗糙地形最大、最小高度和密集度
	- `pyramid_stairs_terrain` ：用于生成金字塔式楼梯地形，可以自己设定每层楼梯高度，以及楼梯层数
	- `discrete_obstacles_terrain` ：用于生成离散障碍物地形，可以自己设定每个子环境中长方体障碍物的大小、数量


`_get_env_origins() -> self.env_origins`
*legged_gym/envs/base/legged_robot.py*

设置每个智能体初始位置

- 对于平坦环境，可以根据每个智能体间隔的距离 `sapcing` 将智能体平均分配在平坦环境中
- 对于复杂环境，每个智能体被平均分配在各列环境中，设定一个最大初始化等级 `max_init_level`，每列智能体被随机分布在各个等级的子环境中，具体操作为：
	1. 初始化一个记录智能体在世界环境中位置的变量 `self.env_origins` (dim: (env_nums，3))
	2. 对于每个智能体平均分配到各列（类型）子环境 `terrain_types` 中，对于平均分配到各列的智能体进行，在 `max_init_level` 约束下，随机分配到各个等级子环境 `terrain_levels`
	3. 获取世界环境中各个子环境的坐标位置 `terrain.env_origins` (dim: (diffcult, type, 3)；diffcult、type 分别表示训练环境的地形难度、类型。
	4. 每个智能体根据被分配到子环境类型 `terrain_types` 和子环境等级 `terrain_levels` 与各个子环境坐标位置进行匹配 `terrain.env_origins[terrain_types,terrain_levels]` 从而得到每个智能体在世界中的位置，将位置存储到 `self.env_origins` 便于日后修改和调用


`check_termination()`

用于判断智能体是否达到终止状态

- 摔倒（环境与基体主躯干发生碰撞）- 通过接触力传感 `self.contact_forces`判断
- 超时（到达设定的最大运行步长）



`_reset_dofs & _reset_robot_states`

智能体位置和关节重置

平坦环境下：
每个智能体位置计算 `robot_states[:3]` = 基体位置初始位置 `base_init_state` + 智能体在环境中被分配的位置`self.env_origins` ;
每个关节的状态会随机化，即默认关节状态 * rand (0.5, 1.5);
基体的线速度、角速度也会随机初始化：即 rand (-0.5, 0.5)。

复杂环境下：
首先计算：`robot_states[:3]` =  智能体位置初始位置 `base_init_state` + 智能体在环境中分配的位置`self.env_origins` ，然后智能体的位置（x，y）会进行一个初始化,即`robot_states[:2] += rand(-1,1)`。
每个关节的状态、基体角速度和线速度的初始化与平坦环境相同。


## 参数说明

### 算法相关参数

`num_envs = 2048` ：并行运行的智能体数量
`num_observation = 48` ：状态空间维度为 48dim
`num_actions = 12`：动作空间维度12dim
`num_steps_per_env = 24` ：一个回合运行的步数，即每 24 步就会更新一次神经网络。
`current_learning_iteration` ：当前运行步数累积
`max_episode_length = 1001`：超时步长；神经网络的参数一个回合（24步）会更新一次，而机器人的状态不会随之更新，机器人只有达到终止条件时，才会重置更新
`clip_actions=100` ：由于神经网络的输出是高斯分布的均值，然后在高斯分布中采样，得到最后的动作，为了避免输出的动作过大影响，需要对动作进行一个裁剪。

其他参数说明：

| 超参数              | 数值/类型         | 解释                                                              |
|:------------------- | ----------------- | ----------------------------------------------------------------- |
| num robots          | 2048              | 并行运行的智能体数量                                              |
| num_steps_per_env   | 24                | 单个智能体的每回合运行次数                                        |
| batch size          | 49152 (2048 x 24) | 批处理数量， 一个回合（一次迭代）采集的数据量                     |
| mini-batch size     | 12288（2048 x 6） | 从batch size大小的数据集提取出一部分（mini-batch size）来进行更新 |
| num_learning_epochs | 5                 | 一批数据更新的次数                                                |
| num_steps_per_env   | 24                | 单个智能体的每回合运行次数                                        |
| max_episode_length  | 1000              | 单个智能体超时限定次数，超过该值，重置状态                                                                  |
| Clip range          | 0.2               | PPO算法用于限制更新步幅的参数                                     |
| $\gamma$            | 0.99              | 奖励折扣因子；未来奖励对当前的影响                                |
| $\lambda$           | 0.95              | GAE折扣因子，平衡优势函数的方差与偏差                             |
| clip range          | 0.2               | PPO 算法用于限制更新步幅的参数                                    |
| learning rate       | adaptive          | 学习率                                                            |
| activaruin          | elu               | 激励函数                                                          |
| optimizer           | adam              | 优化器                                                            |

### 平坦环境相关超参数
`sapcing =5` ：平坦环境下，每个智能体的间隔为 5m

状态空间相关超参数：

| 状态         | 维度 | 缩放       |
| ------------ | ---- | ---------- |
| 基体的线速度 | 3    | 2          |
| 基体的角速度 | 3    | 0.25       |
| 基体重力向量 | 3    | 1          |
| 基体指令速度 | 3    | 2、2、0.25 |
| 关节位置差值 | 12   | 1          |
| 关节速度     | 12   | 0.05       |
| 上一时刻动作 | 12   | 1           |

奖励空间相关超参数：

| 超参数                    | 数值         | 解释                           |
|:------------------------- | ------------ | ------------------------------ |
| Linear velocity tracking  | 1            | xy 轴线速度跟踪奖励            |
| Angular velocity tracking | 0.5          | z 轴角速度跟踪奖励             |
| Linear velocity penalty   | -2           | z 轴线速度惩罚                 |
| Angular velocity penalty  | -0.05        | xy 轴角速度惩罚                |
| Joint torques             | -0.000025    | 关节力矩惩罚                   |
| Feet air time             | 2            | 脚腾空时间奖惩                 |
| Collisions                | -1           | 碰撞小腿、大腿、臀部惩罚       |
| Clip range                | 0.2          | PPO 算法用于限制更新步幅的参数 |
| Action rate               | -0.01        | 动作幅度惩罚                   |
| Orientation               | -5           | 方向偏移惩罚                   |
| Action scale              | 0.3          | 动作幅度缩放                   |
| Linear command            | \[-1, 1]     | 线速度命令范围                 |
| Angle  command            | \[-1.5, 1.5] | 角速度命令范围                 |
| Resampling Command        | 200          | 每200步控制命令进行一次重采样   |

### 复杂环境相关超参数

`mesh_type = 'trimesh'` ： 地形类型
`measure_posits_x` : 设置高度检测点 x 的取值
`measure_points_y` ：设置高度检测点 y 的取值
`terrain_proportions`：设置每种地形所占比
`num_rows = 10`：设置地形的难度，难度与比值成正比
`num_cols = 20` ：设置地形类型，根据每种地形所占比分配地形数量，总量为 20


状态空间相关超参数

| 状态         | 维度 | 缩放       |
| ------------ | ---- | ---------- |
| 基体的线速度 | 3    | 2          |
| 基体的角速度 | 3    | 0.25       |
| 基体重力向量 | 3    | 1          |
| 基体指令速度 | 3    | 2、2、0.25 |
| 关节位置差值 | 12   | 1          |
| 关节速度     | 12   | 0.05       |
| 上一时刻动作 | 12   | 1          |
| 高度测量     | 150   | 5           |

奖励空间相关超参数

| 超参数                    | 数值          | 解释                           |
|:------------------------- | ------------- | ------------------------------ |
| Linear velocity tracking  | 1             | xy 轴线速度跟踪奖励            |
| Angular velocity tracking | 1             | z 轴角速度跟踪奖励             |
| Linear velocity penalty   | -2            | z 轴线速度惩罚                 |
| Angular velocity penalty  | -0.05         | xy 轴角速度惩罚                |
| Joint torques             | -0.000025     | 关节力矩惩罚                   |
| Feet air time             | 2             | 脚腾空时间奖惩                 |
| Collisions                | -1            | 碰撞小腿、大腿、臀部惩罚       |
| Clip range                | 0.2           | PPO 算法用于限制更新步幅的参数 |
| Action rate               | -0.02         | 动作幅度惩罚                   |
| Orientation               | -5            | 方向偏移惩罚                   |
| Action scale              | 0.2          | 动作幅度缩放                   |
| Linear command            | \[-1,1]      | 线速度命令范围                 |
| Angle  command            | \[-1,1]      | 角速度命令范围                 |
| Heading command           | \[-3.14,3.14] | 航向误差；控制角速度缓慢增加                              |
| Resampling Command        | 1000          | 每1000步控制命令进行一次重采样 |



## 修改文件

新建的文件在代码整体框架中标红表示，下面列举了修改的文件名，及其修改的主要内容。


*legged_gym/envs/ base/legged_robot. py*
增加 `test_post_physics_step` 函数，在测试时不需要奖励函数计算和检查重置等操作，只需要更新智能体的状态信息即可。

*legged_gym/envs/base/config. py*
修改 `runner` 类中， 最大训练迭代次数 `max_iterations=1300/6000` ，平坦环境下迭代 1300 次，复杂环境下迭代 1600 次。修改模型保存间隔 `save_interval=100/1000`，平坦环境下每迭代 100 次保存一次模型，复杂环境下每 1000 步保存一次模型。

*legged_gym/envs/utils/terrain. py*
增加 `make_test_terrain` 函数，该函数类似于 `make_terrain` 函数用于创建测试环境。

*legged_gym/envs/_init_. py*
根据*legged_gym/envs/quadruped_arm*文件夹下设置的派生类，在初始化函数中对我们的任务进行注册。


