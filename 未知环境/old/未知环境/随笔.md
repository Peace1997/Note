

## 目标搜索算法对比

| 算法名称 | 问题解决 | 创新点 | 缺点 | 适应度函数|
| :--: | :--: | :--: | ---- | ---- |
| A-RPSO |   |      |||
|     RBA     |          |  |||
| ARBA |   |      |||
|     改进PSO     |  |  |||



问题提出：

- 未知环境（POMDP）
- 单目标/多目标搜索
- 静态环境 + 动态环境
- 多智能体搜索



- MARL + 优化算法 + 课程学习/分布式/内在动机 + SLAM + 迁移学习



未知环境下基于深度强化学习的智能体目标搜索

> 未知环境：
>
> - 目标搜索：更偏向于路径规划（算法）
> - 自主探索：更偏向于图像（定位，建图）
>
> 障碍物：静态 + 动态 ；先从静态开始。
>
> 智能体：单智能/多智能体；先由单智能体做起。
>
> 深度强化学习：POMDP；动作是连续动作；观测值由传感器获得（距离传感器//摄像头。。）
>
> 辅助技术：
>
> - 迁移学习；课程学习
> - 各种常用神经网络（卷积、循环）
> - 优化算法
> - 图像 ；SLAM
> - 辅助任务
> - 内在奖励



SLAM：用于环境重建 （基于滤波器的SLAM算法；基于图像的SLAM）

路径规划：用于生成机器人的动作（A*）



