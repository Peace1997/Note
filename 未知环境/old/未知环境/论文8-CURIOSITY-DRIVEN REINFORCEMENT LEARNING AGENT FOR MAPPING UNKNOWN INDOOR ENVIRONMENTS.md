## CURIOSITY-DRIVEN REINFORCEMENT LEARNING AGENT FOR MAPPING UNKNOWN INDOOR ENVIRONMENTS

### 基于好奇心驱动强化学习的未知室内环境建图



#### 1.简述

- 整体流程：通过强化学习方法控制移动机器人的移动，通过SLAM估计机器人的姿态以及建图。
- 实质：将探索未知环境的问题转化为访问地图中新位置的问题。如果机器人所处的位置，与存放历史新位置的缓冲区的位置相距较远，获得奖励越大，若与缓冲区中和其最近的一个位置距离为k，则将其所在位置加入缓冲区。

- 创新点
  - 基于好奇心的奖励函数

- 优点

  - 相较于在动作空间添加噪声的方法来增加探索，使用好奇心驱动的奖励函数设置，增加了探索效率，快速学习，提高鲁邦性。
  
- 缺点：

  - 相较于强化学习输出目标点，每次移动都要经由神经网络得出（若隔一定时间周期更新一次，有可能会影响准确性）

  - 文章中没有体现出建图相关的内容
  - 训练环境偏简单



### 2. 总体设计

- **DDPG**
  - actor、critic：三层隐藏层、ReLU、512神经元
  - actor：为了使得线速度和角速度可行，最后接sigmoid输出线速度，接tanh输出角速度。
  - critic：state输入到第一个隐藏层，action会输入到第二个隐藏层

- **state space**
  $$
  s_{t}=\left[z_{t}, \chi_{t}^{(x, y, \theta)}, a_{t-1}, \zeta_{t}^{\neg}, \tau_{t}^{\neg}\right]
  $$

  - 360度激光雷达数据； 80
  - 机器人自身的位置：$x，y，\theta$
  - 上一步动作
  - 建图百分比
  - 每回合累计步长

- **action space**

  - 线速度
  - 角速度

  > 机器人只会前向移动，同时可以左右转弯

- **reward space**
  $$
  \mathrm{R}\left(s_{t}\right)= \begin{cases}r_{\text {map completed }}, & \text { if } \zeta_{t} \geq C \\ r_{\text {crashed }}, & \text { if } z_{t} \leq z_{\min } \\ r_{t}^{c}, & \text { otherwise }\end{cases}
  $$

  $$
  r_{t}^{c}=\frac{\alpha}{M} \sum_{i=1}^{M} d\left(\chi_{t}^{(x, y)}, \chi_{i}^{(x, y)}\right)
  $$

  - 当建图百分比达到要求时，会给一个正奖励

  - 当机器人离障碍物较近时（小于设定的阈值），会给你一个惩罚

  - 基于好奇心的奖励，促进探索：创建一个M大小存放机器人位置的一个缓冲区，如果一个位置与这个空间中所有的位置都有一段距离k，那么认定该位置为可记录的新位置，并将其添加到缓冲区中。如果缓冲区满了，就从较早进入的历史数据中随机删除。**贴近墙壁的位置不会被视为新位置**。

    具体奖励值为：机器人当前位置与缓存区中的所有新位置的距离总和的平均

    ![image-20211127152822692](/Users/mapeixin/Documents/Typora/未知环境/img/Paper8_1.png)

    > 其中$\alpha$就是用于衡量好奇心奖励所占比例，以及控制达到新位置的迫切性，d是欧几里得距离

  - **优点：**
    - 鼓励探索；
    - 只使用姿态信息，独立于SLAM算法所构建的地图信息，从而提高了它的场景适用性
  - **重点：**k值的选择，过小会降低探索速度，过大好奇心的奖励就会消失，因为新的位置太难达到，就没法解决奖励稀疏的问题。

### 3. 实验环境

- ROS；Gazebo；SLAM

- 评价标准：建图完成率；完成探索动作数量；轨迹长度

- 训练与测试：

  - 在地图1（65$m^2$）进行训练；在地图2、3（75$m^2$）进行测试。
  - 训练策略： 相同初始位置 ； 四个不同初始位置
  - 实验环境：

  ![image-20211127171308206](/Users/mapeixin/Documents/Typora/未知环境/img/Paper8_2.png)

- 实验对比：与三个不同的奖励函数进行比较，其中前两个奖励条件相同，基于好奇心的奖励函数与其他三个实验不同。最后与基于边界的探索进行对比

  - 第一个奖励为0
  - 第二个是基于地图完整性的奖励函数：$\zeta_t-\zeta_{t-1}$

  - 第三个是基于地图熵的奖励函数：$H_t - H_{t-1}$



- 训练结果对比

  ![image-20211127172831426](/Users/mapeixin/Documents/Typora/未知环境/img/Paper8_3.png)

- 测试结果对比

  ![image-20211127215535783](/Users/mapeixin/Documents/Typora/未知环境/img/Paper8_4.png)

  - Sparse和Oracle虽然移动轨迹较短，但是很难完成任务要求，有时经常会在一个点循环，所以有时会导致移动轨迹较短。
  - 与基于边界的方法相比，更具效率，不用分别计算边界点、躲避障碍物、导航到目标点。





active SLAM 