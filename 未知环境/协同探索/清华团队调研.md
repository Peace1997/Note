

## 一、ANS（Active Neural SLAM）



### Neural SLAM
利用RGB观测信息和传感器信息预测地图和智能体的姿态

### Global Policy

使用预测的地图和智能体的姿态来制定长期目标，并使用analytic path planner（Dijkstra 或 A*）将长期目标转换为短期目标

### Local Policy

根据当前观察采取导航行动，以实现短期目标




## 二、可扩展强化学习协同探索框架
《 Learning Efficient Multi-Agent Cooperative Visual Exploration 》

提出了第一个多智能体合作探索框架： MAANS（Multi-Agent Active Neural SLAM），采用模块化设计，其核心为集中式全局规划器 MSP（Multi-agent Spatial Planner），对Transformer结构进行扩展，通过Spatial-TeamFormer来适应不同的团队规模，

假设智能体之间有完美的通信。

对当前SOTA单智能体RL策略（ANS）扩展到多智能体，提出了MAANS策略


本文提出一种更具表现力的网络架构，**Spatial-TeamFormer**：采用层次化自注意力架构，来获取智能体内部和空间的关系。

**ANS**（Active Neural SLAM）包含四个部分：
- Neural SLAM：通过监督学习训练
	- 输入：获取 RGB 图像、姿态传感数据、历史输出
	- 输出：新的 2D 重建地图和当前姿态估计（agent-centric local map）
- 基于强化学习的全局规划器 —— Multi-agent Spatial Planner（MSP）
	- 输入：已探索区域、未探索区域和障碍物的通道；历史轨迹（augmented agent-centric local map）
	- 输出：从两个高斯分布中采样两个数进行，作为长期目标点坐标（long-term goal）
	- 实现：参数化为CNN策略，并由PPO算法训练
- 基于规划的局部规器
	- 实现：快速推进法（Fast Marching Method (FMM)）；在以智能体为中心的局部地图上向给定的长期目标移动，并且输出短期子目标轨迹。
- 局部策略
	- 实现：在给定RGB图像和子目标的情况下产生动作，并通过模仿学习进行训练。
![[MAANS.png]]

1. 【感知】：首先，通过 **Neural SLAM** 模块，接收每个智能体的 RGB 图像和姿态感知信息，获得以智能体为中心的局部地图和姿态估计，通过 **Map Refiner** 对每个以智能体为中心的局部地图进行校准，使其在同一全局坐标系中，从而形成以智能体为中心的全局地图。
2. 【决策】：首先，加入轨迹信息，对以智能体为中心的全局地图进行扩充，通过**CNN-based Feature Extractor** 对空间特征进行提取；对提取的空间特征，结合该智能体的特定信息，输入到 Spatial-TeamFormer，利用基于 Transformer 的分层网络结构融合团队信息；最后，通过**Action Generator**，将来自 Spatial-TeamFormer 的特征生成空间全局长期目标。
3. 【规划】：首先，通过**Map Merge** 对N个以智能体为中心的全局地图进行合并，形成一个完整的全局地图；然后，利用**Local Planner** 模块对将全局目标转换为短期子目标；最后通过 **Local Policy** 输出到子目标点的导航动作。



1. 每个智能体接受来自环境的视觉和姿态传感信号时，Neural SLAM模块校正传感器误差并执行SLAM，从而构建包括探索区域和障碍物的自上而下的2D占用地图（occupancy map）。
2. Map Refiner：旋转每个智能体局部地图到一个全局坐标系。
3. 用每个智能体的轨迹信息来扩充这些精确的地图，并将这些空间输入连同其他智能体的特定信息输入到 MSP（Multi-agent Spatial Planner）中，以生成全局目标作为每个个体智能体的长期导航目标
4. 为了有效达到全局目标，智能体首先使用 FMM 在手动合并的全局地图中规划到达该长期目标的路径，并生成短期子目标序列。
5. 最后，给定短期子目标，局部策略基于视觉输入和到子目标的相对空间距离以及相对角度输出最终导航动作。


> 在地图融合过程中，仅利用估计的几何信息
> 

ANS包括四个模块，因为Neural SLAM和局部策略不涉及多智能体交互，因此直接对ANS的两个模块
进行修改。使用MAPPO算法来训练MSP。实际动作空间为全局目标的空间位置。



知识点：
自注意力机制（self- attention）：适应不同输入大小的策略框架。
参数共享（parameter sharing）：团队规模泛化技术，即也可以适应不同输入大小。同时也被证明可以有效减少环境非平稳性，加速训练。
FMM：
多模态问题（multi-modal）


待改进：
- 集中式规划，假设智能体间有完美的通信。


### Neural SLAM

利用RGB观测信息和传感器信息得到 以智能体为中心的局部地图和智能体的姿态估计。

### Map Refiner & Map Merge

***Map Refiner***

对局部地图进行规范化

确保来自 Neural SLAM 模块的所有地图都在同一坐标系内。

- 首先将所有以智能体中心的局部地图（agent- centric local map）组合成智能体全局地图（agent-centric global map）
- 然后我们基于姿态估计转换坐标系，归一化所有智能体的全局地图
- 当智能体探索边界区域时，以智能体为中心的局部地图通畅覆盖不可见区域的大部分，为了确保MSP中特征提取器只集中在可行的部分以及集中动作空间，对归一化地图的不可探测边界进行裁剪，并放大了房间区域作为作为的细化地图

***Map Merge***

利用其他智能体的信息来规划子目标，可以产生更好的短期局部目标。具体来说，通过Map Refiner获得N张放大的全局地图后，利用max-pooling来完成地图合并。


### Local Planning & Local Policy

***Local Planning***
为了有效达到全局目标，智能体首先使用 FMM 在手动合并的全局地图中规划到达该长期目标的路径，并生成短期子目标序列。

***Local Policy***

对给定短期子目标，局部策略基于视觉输入和到子目标的相对空间距离以及相对角度输出最终导航动作。


### MSP
- **CNN-based Feature Extractor**
- **Spatial-TeamFormer**
- **Action Generator**



CNN 特征提取器：从每个智能体的局部导航轨迹中提取空间特征映射


Spatial-TeamFormer：利用基于 Transformer 的分层网络结构融合团队信息，包含
- Individual Spatial Encoder：基于自注意力机制，仅关注自身空间信息
- Team Relation Encoder：仅关注智能体间团队交互信息，不利用任何空间信息

> 以分层的方式，学习一个团队规模不变的空间表示


动作生成器：将来自 Spatial-TeamFormer 的特征生成空间全局目标。全局目标由两部分组成；
- 离散区域空间用于从 GxG 的图中选择一个区域 g
- 连续区域空间用于从区域 g 中确定一个连续的目标点 (x, y)




### Policy Distillation

采用基于两阶段蒸馏的解决方案：
- 第一阶段：固定智能体规模，在不同的训练场景下分别训练策略
- 第二阶段：对不同训练场景下的预训练策略集合进行蒸馏，直接衡量该蒸馏策略对新场景和不同团队规模的泛化能力

通过策略蒸馏提高策略对不同场景不同智能体规模的泛化能力


### 实验

***任务设定***：Habitat 仿真平台

***任务目标***：在有限时间内最大限度的增加探索区域

***评价指标：***
- 探索率
- 达到 90%探索率所需时间步长
- 重叠探索区域

***对比实验：*** 只替换全局规划模块，
- 单智能体 Baseline
	- Nearest：选择最近的候选点
	- Utility：人工设计的效用函数
	- RRT：快速搜索随机数
- 多智能体 Baseline：基于规划的方法
	- Voronoi：通过Voronoi分区对地图进行划分，每个智能体只搜索自己分区中的未开发区域
	- APF：人工势场方法；计算集群边界的人工势场，规划具有最大信息增益的势降路径
	- WMA-RRT：RRT 的一种多智能体变体，遵循 locking-and-search 机制
- 随机策略


***消融实验：***
在两个选定场景中测试多个智能体的训练表现
- ANS 变体
	- ANS-blind：N个智能体在环境中不进行任何通信
	- AND-stack：将智能体为中心的局部地图作为输入叠加到全局规划器中。
- MSP 变体
	- MSP w. o. TeamFormer:使用平均池化层来替换Spatial-TeamFormer（从CNN提取特征空间）
	- MSP w. o. Act. Gen.：在动作生成器中去除 region head，而是通过两个高斯分布直接在整个精细的地图上生成全局目标。
	- MSP-merge：合并全局地图过程中，使用单个 CNN 提取特征，而不是每个智能体的信息
- Spatial-TeamFormer 变体
	- No Ind. Spatial Enc：在 Spatial- teamformer 中移除 Individual Spatial Encoder
	- No Team Rel. Enc：Team Relation Encoder
	- Unified：在所有空间特征中应用一个统一的 Transforer 而不是分层 Spatial-TeamFormer
	- Flattened：未保留特征图的空间结构，直接将CNN提取的特征转换为每个智能体的flattened vectors，然后输入到标准Transforer中进行特征学习。

***其余实验对比***
- 固定智能体规模下，基于规划的方法与 MAANS 进行对比
- 不同团队规模下，表现优异的规划方法与 MAANS 进行对比
- 运行过程中切换团队规模，表现优异的规划方法与 MAANS 进行对比


待看论文：
Nikolay Savinov, Anton Raichuk, Raphaël Marinier, Damien Vincent, Marc Pollefeys, Timothy
Lillicrap, and Sylvain Gelly. Episodic curiosity through reachability. In International Conference
on Learning Representations. ICLR, 2019. —— ANS

对ANS的扩展




## 三、多机器人协同主动建图算法
Multi-Robot Active Mapping via Neural Bipartite Graph Matching

主动建图（Active Mapping），即机器人在未知场景中主动地移动和采集数据从而自动化地构建场景地图的过程。该任务的难点在于如何高效地探索未知环境并构建完整的场景地图。传统算法大多基于贪心策略，因此效率有限；近几年基于机器学习的算法注重效率，然而无法保证地图的完整性。因此，我们将传统算法和机器学习相结合，同时吸取了传统算法地图完整度高和机器学习方法全局效率高的优点，提出了 NeuralCoMapping 算法。


NeuralCoMapping 算法由四个模块组成：建图模块（Mapping Module），全局规划器（Global Planner），局部规划器（Local Planner）和动作控制器（Action Controller）。

![[NeuralCoMapping.png]]

1. Mapping Module：每个智能体根据当前时间步的深度图、位姿信息更新障碍物地图。
2. Global Planner：每隔一定时间，根据当前地图和机器人位置，进行全局规划产生每个智能体的目标位置
3. Local Planner：根据每个机器人当前目标位置和障碍物地图，在地图上计算出移动路线。
4. Actor Controller：根据移动路线计算出当前步机器人应该采取采样的动作（前进、左转、右转）

### Mapping Module

### Global Planner

根据当前地图和机器人位置，决定每个机器人在下一次全局规划之前需要前往的目标位置，从而逐步探索场景并更新地图。

具体来说，从地图中提取两类节点，机器人位置（robots）和前沿点位置（frontiers）。前沿点也叫做边境点，指地图中位于已探索无障碍物区域和未探索区域的分界线上的点。基于这两类节点，将全局规划抽象成二分图匹配问题：为每一个机器人分配一个前沿点作为目标位置。

终止条件：当地图中不再有前沿点时，我们就得到了完整的场景地图。

为了实现全局规划之间的连续性，我们额外加入了两类辅助节点，历史机器人位置（history robots）和历史目标位置（history goals）。通过多路图神经网络（Multiplex Graph Neural Network），我们提取机器人和前沿点之间的距离代价，作为二分图匹配的关联矩阵（Affinity Matrix）。基于关联矩阵，我们使用线性分配层（Linear Assignment Layer）进行可微匈牙利算法（Differentiable Hungarian Algorithm），从而得到二分图匹配，即每个机器人分配一个前沿点作为目标位置。

强化学习在这里用于对mGNN网络进行优化，mGNN用于机器人当前位置和前沿点距离代价计算。


### Local Planner

### Action Controller