### 论文1
**Han R et al. 提出了一种在动态环境下通过强化学习方法解决多智能体导航问题的方案，将目标位置分配和避免碰撞结合到训练过程中，以学习协作导航策略**

>Han R, Chen S, Hao Q. Cooperative multi-robot navigation in dynamic environment with deep reinforcement learning[C]//2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020: 448-454.

#### 贡献：
- 利用所有机器人的累积经验进行目标位置分配的协作架构
- 通过深度强化学习来训练目标位置分配以及碰撞躲避
- 开发了从模拟到真实的迁移机制，将训练好的策略更好地应用在真实环境中。

训练策略由所有智能体同时共享和更新
在仿真训练过程中开发了一组随机动力学模型，可以减少仿真和实体之间的不匹配。

#### 目标：
多个智能体在动态环境以最小代价（时间）无碰撞的完成多目标点的导航。即在动态环境中，所有智能体通过分享和优化信息，最小化完成任务的时间，来得到最优的导航策略。

#### 总体策略：
- 目标分配策略（组合优化）；首先将目标位置分配给每个机器人（实时变化）； 组合优化问题；计算机器人与目标点之间的分配方案的所有排列，找到总距离最短的排列
- 目标导航策略（RL）；每个智能体通过策略（actor网络）根据获取的状态信息得到相应的动作，智能体执行动作，得到下一时刻状态，根据环境反馈的奖励以及Critic网络预测的价值来评判当前动作的好坏，以此指导actor网络的更新。

**状态**：
- 当前智能体状态（笛卡尔坐标系的位置、方向、速度）；
- 其他智能体状态
- 障碍物状态（从所有智能体的观测聚合而成）
- 目标位置
**动作**：
- 平移速度（transitional）和旋转速度（rotational）
**奖励：**
- 接近或到达目标点奖励


### 论文2
**Jin Y et al. 提出了一种交错深度强化学习方法(interlaced DRL)，同时学习动态目标选择策略和避碰策略来解决多机器人协作导航问题( multi-agent cooperative navigation problem MCNP)，多智能体在未知环境中不发生碰撞的情况下协同合作到达不同的目标点。**

>Jin Y, Zhang Y, Yuan J, et al. Efficient multi-agent cooperative navigation in unknown environments with interlaced deep reinforcement learning[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 2897-2901.


交错 RL 同时学习两个任务、
奖励函数不是启发方式，而是直接根据优化目标设定。