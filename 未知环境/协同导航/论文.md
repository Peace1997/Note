未知环境下多智能体深度强化学习在协同导航中的应用


# 1. 介绍

近来，随着人工智能和机器人技术的发展，智能移动机器人的应用场景不断扩展，例如消毒机器人、室外巡检机器人、救援机器人。随着机器人数量增多，我们期望多个机器人协作完成复杂的任务。多机器人协作导航问题（Multi-Robot Navigation Problem，MRNP）是移动机器人领域的热点问题之一，MRNP 在实际场景中有很多应用，如协作搜救，编队控制，智能仓储和物流。与单机器人导航相比，多机器人协作导航可以更好的利用机器人共享的部分信息完成多目标导航任务，提升对环境的适应性和容错性。





在本文中，MRNP的目标是多个机器人需要以最少的时间且没有冲突的到达所有未被提前分配的目标点。

MRNP

在本文中吧，我们解决了多机器人导航到未分配


环境非稳态问题


本文的主要贡献如下：
- 基于 ROS 中间件，搭建了多机器人协同导航实验平台，设计了不同难度的导航场景；
- 设计了分层控制体系结构，上层为动态目标选择策略，下层为目标路径规划策略；
- 为解决强化学习稀疏奖励问题，设计了基于课程学习和优先经验回放池的多智能体深度强化学习算法。


## 研究现状

传统基于组合优化的目标分配策略，计算机器人与目标点之间分配的所有排列，找到总距离最短的分配排列作为最后的结果，该策略适用于简单的环境，当环境变复杂时，仅依靠距离作为衡量标准，是不合理的。

基于预先分配的方案，通常此时环境是已知的
预先分配目标点、 编队协同到达同一目标点、动态目标分配


【1】Cooperative multi-robot navigation in dynamic environment with deep reinforcement learning
将目标分配策略与基于强化的目标导航策略相结合，解决动态环境中多机器人导航问题，开发了从模拟环境到真实环境的迁移机制，更好的将训练好的策略应用在真实环境中

> 实时目标分配、动态环境

【2】Efficient multi-agent cooperative navigation in unknown environments with interlaced deep reinforcement learning

提出了一种交错深度强化学习方法，同时学习目标选择策略好避障策略来解决多机器人协作导航问题。

> 动态目标分配（可重复）

【3】Hierarchical and Stable Multiagent Reinforcement Learning for Cooperative Navigation Control 

提出了一种新的分层和稳定的框架去解决未分配目标的多智能体导航策略，通过分层强化学习学习整体任务，利用扩展Q函数去解决多智能体的非平稳问题，

【4】Optimal Target Assignment and Path Finding for Teams of Agents
研究了已知地形中的智能体团队的目标分配和路径查找（combined target-assignment and path-
ﬁnding；TAPF）问题，提出了 CBM（Conﬂict-Based Min-Cost Flow）分层算法，为所有智能体分配目标点，然后规划一条无碰撞的路径。
> 环境已知、预先分配目标点

【5】Connectivity guaranteed multi-robot navigation via deep reinforcement learning
提出了一种融合约束满足函数 (constraint satisfying parametric function CSPF) 的深度强化学习的策略，多个机器人协同移动无碰撞的到达同一目标点（一个目标点）的同时保持导航过程中的连通性。
> 环境未知、单一目标点、协同移动

【6】Hybrid IWD-DE: A Novel Approach to Model Cooperative Navigation Planning for Multi-robot in Unknown Dynamic Environment
将智能水滴（Intelligent Water Drop ；IWD） 算法和差分进化（Differential Evolution ；DE）算法相结合，在未知动态环境下，通过迭代评估适应度函数优化每个机器人的导航路线（优化每个机器人的导航路线）。
> 环境动态未知、预先分配目标点

【7】Multi-agent navigation in human-shared environments: A safe and socially-aware approach

采用分层体系结构，通过将全局路径规划，预测性路径规划和反应性方法相结合，获得安全且具有社会意识的多智能体导航策略。

> 环境已知，动态环境、预先分配目标点

【8】A modular functional framework for the design and evaluation of multi-robot navigation

提出了一种模块化功能框架，该框架不仅可以提高代码的可重用性，而且方便比较不同功能模块的特定组合会影响最终的导航性能。

>集体移动（编队控制）


【9】Low-Cost Multi-Agent Navigation via Reinforcement Learning With Multi-Fidelity Simulator

提出了一种多保真模拟器框架来训练多智能体强化学习，提高多智能体导航任务数据效率。为了实现联合训练和加速收敛，引入了一种基于回溯的本地MDP识别方法，用于分解任务空间，并引入了具有优先级动作队列的深度优先搜索作为专家策略搜索本地可行策略。



【10】 Distributed Non-Communicating Multi-Robot Collision Avoidance via Map-Based Deep Reinforcement Learning

提出了一种基于地图的DPPO方法，在分布式和无通信环境中无碰撞的到达各自预先分配的目标点，并利用课程学习的方法提升整体训练的效果。

> 分布式、无通信、预先分配目标点


【11】Collective navigation ofa multi-robot system in an unknown environment

先利用单个智能体仅利用距离传感器在未知静态环境中导航，然后将其扩展为多机器人导航策略，通过邻居-邻居通信和信息交换，多机器人无碰撞集中运动到同一目标点。

> 集体路径规划，未知环境
> 仅利用感知信息和智能体脂减的共享数据

【12】 Decentralized Goal Assignment and Safe Trajectory Generation in Multi-Robot Networks via Multiple Lyapunov Functions
应用启发式的方法，将动态多智能体系统的分配和安全问题转换为交换系统的稳定性问题，在通信范围内两个智能体才可以交换信息决定是否更换目标点，该方法可以确保互换目标位置的安全性和全局稳定性，为了避免冲突，每个智能体采用本地规划的方法，缺乏全局目标分配和路径规划。

> 分布式本地分配


弊端：例如根据距离分配公式，机器人 1、2 应该分别选择目标 1、2 作为接下来前往的目标，这显然增加了移动代价，因为此时智能体没有考虑到有障碍物的情况。

# 2. 背景

## 2.1 多智能体深度强化学习
强化学习 (Reinforcement Learning, RL) 【】是一种常用的机器学习方法，智能体在与环境交互的过程中利用奖励信号去改进策略，目标是最大化累计奖励。
多智能体强化学习【】将强化学习、博弈论等应用到多智能体系统，多个智能体通过交互和决策完成更加复杂的任务。


## 2.2 优先经验回放池





# 3. 协同导航策略


## 3.1分层控制结构

本文提出了一种分层控制结构，高层为目标决策层，用于智能体动态选择目标点；低层为目标导航层，用于引导智能体无碰撞的到达所选目标点。整个分层控制结构如下图【】所示，在每个时间步，智能体根据自身观测信息，选择一个接下来要前往的目标点，然后通过全局路径规划算法和局部路径规划算法，规划控制机器人向目标点进行移动。




## 3.2 目标决策算法


### 3.2.1 问题建模
一个完全合作的协同导航问题可以建模为分布式部分可观测马尔可夫决策过程（Dec-POMDP）。Dec-POMDP 可由表示，其中n表示

（1）状态空间

每个智能体的观测信息 $o$ 主要由四部分组成：智能体对周围环境的感知，智能体到所有目标点的相对位置，智能体到其他智能体的相对位置，上一时刻各个智能体所选目标点。

（2）动作空间
智能体的动作是一个 \[0, 1] 范围内的连续值，通过建立与目标点的映射，可以得出智能体所选的目标点。

（3）奖励空间
奖励函数由三部分组成：

任务完成奖励：当智能体无重复的到达所有目标点，将会获得一个较大正奖励。
时间步奖励：每个时间步都会给予一个较小负奖励，鼓励智能体尽快完成任务要求
重复选点奖励：如果智能体同时选择同一目标点，会给予一个负奖励，


### 3.2.2 课程学习
课程学习【】是一种训练策略，它模仿人类课程中的学习顺序，先在简单的样本中训练智能体学习模型，然后逐步增加样本的难度继续进行训练，从而提高模型的泛化能力和收敛率。课程学习策略通常分为两步完成：首先学习简单的示例集，然后学习目标训练集。
在本文中，我们将目标任务分为两阶段完成。具体来说，智能体先在简单环境和目标任务中学习无重复的到达所有目标点，然后增加环境复杂度，设置更具挑战性的目标点，智能体在第一阶段的基础上学会更复杂的任务。由于缺乏密集型奖励的引导，智能体会很难完成最终的任务目标，从而导致智能体无法很好利用已有的经验进行学习，为此，我们首先设计了简单的任务场景【】，先让智能体学会如何完成最终目标任务，在此基础上，增加任务难度【】，让智能体学会更加复杂的策略，使智能体更有效的到达所有目标点。



在本文中，我们允许智能体可以选择同一目标点，不同智能体可以在运动前期朝同一目标点前进，但最终到达不同的目标点。如下图所示【】，如果我们强制设置智能体

智能体在运动初期为了减少路径消耗，会选择同一目标点前进，

### 3.2.3 PER-MATD3


> 智能体开始时可能会选择同一目标点，最终会到达不同的



优先经验回放池：


多智能体强化学习：



智能体是可以选择同一个目标点，如果强制为智能体分配不同的目标点前往是不合理的。


因为每个智能体可以选择同一个目标点，智能体首先在简单环境下，学习目标分配策略，尽可能的避免重复到达相同的目标点。

## 3.3 目标导航算法



# 4. 实验验证

## 4.1 实验设置

## 4.2 模型训练结果




# 5. 结束语