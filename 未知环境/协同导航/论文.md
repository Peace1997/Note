
#课程学习 #优先经验回放池


## 研究现状

传统基于组合优化的目标分配策略，计算机器人与目标点之间分配的所有排列，找到总距离最短的分配排列作为最后的结果，该策略适用于简单的环境，当环境变复杂时，仅依靠距离作为衡量标准，是不合理的。

基于预先分配的方案，通常此时环境是已知的



【1】Cooperative multi-robot navigation in dynamic environment with deep reinforcement learning
将目标分配策略与基于强化的目标导航策略相结合，解决动态环境中多机器人导航问题，开发了从模拟环境到真实环境的迁移机制，更好的将训练好的策略应用在真实环境中

> 实时目标分配、动态环境

【2】Efficient multi-agent cooperative navigation in unknown environments with interlaced deep reinforcement learning

提出了一种交错深度强化学习方法，同时学习目标选择策略好避障策略来解决多机器人协作导航问题。

> 动态目标分配（可重复）

【3】Hierarchical and Stable Multiagent Reinforcement Learning for Cooperative Navigation Control 

提出了一种新的分层和稳定的框架去解决未分配目标的多智能体导航策略，通过分层强化学习学习整体任务，利用扩展Q函数去解决多智能体的非平稳问题，

【4】Optimal Target Assignment and Path Finding for Teams of Agents
研究了已知地形中的智能体团队的目标分配和路径查找（combined target-assignment and path-
ﬁnding；TAPF）问题，提出了 CBM（Conﬂict-Based Min-Cost Flow）分层算法，为所有智能体分配目标点，然后规划一条无碰撞的路径。
> 环境已知、预先分配目标点

【5】Connectivity guaranteed multi-robot navigation via deep reinforcement learning
提出了一种融合约束满足函数 (constraint satisfying parametric function CSPF) 的深度强化学习的策略，多个机器人协同移动无碰撞的到达同一目标点（一个目标点）的同时保持导航过程中的连通性。
> 环境未知、单一目标点、协同移动

【6】Hybrid IWD-DE: A Novel Approach to Model Cooperative Navigation Planning for Multi-robot in Unknown Dynamic Environment
将智能水滴（Intelligent Water Drop ；IWD） 算法和差分进化（Differential Evolution ；DE）算法相结合，在未知动态环境下，通过迭代评估适应度函数优化每个机器人的导航路线（优化每个机器人的导航路线）。
> 环境动态未知、预先分配目标点

【7】Multi-agent navigation in human-shared environments: A safe and socially-aware approach

采用分层体系结构，通过将全局路径规划，预测性路径规划和反应性方法相结合，获得安全且具有社会意识的多智能体导航策略。

> 环境已知，动态环境、预先分配目标点

【8】A modular functional framework for the design and evaluation of multi-robot navigation

提出了一种模块化功能框架，该框架不仅可以提高代码的可重用性，而且方便比较不同功能模块的特定组合会影响最终的导航性能。

>集体移动（编队控制）


【9】Low-Cost Multi-Agent Navigation via Reinforcement Learning With Multi-Fidelity Simulator




【10】 Distributed Non-Communicating Multi-Robot Collision Avoidance via Map-Based Deep Reinforcement Learning

提出了一种基于地图的DPPO方法，在分布式和无通信环境中无碰撞的到达各自预先分配的目标点，并利用课程学习的方法提升整体训练的效果。

> 分布式、无通信、预先分配目标点



弊端：例如根据距离分配公式，机器人 1、2 应该分别选择目标 1、2 作为接下来前往的目标，这显然增加了移动代价，因为此时智能体没有考虑到有障碍物的情况。


## 研究内容
课程学习：

在简单的场景下，学习不重复到达不同的目标点
在复杂的场景下，学习更有效的目标点分配策略，以最小的代价到达目标点。



优先经验回放池：


多智能体强化学习：






智能体是可以选择同一个目标点，如果强制为智能体分配不同的目标点前往是不合理的。


因为每个智能体可以选择同一个目标点，智能体首先在简单环境下，学习目标分配策略，尽可能的避免重复到达相同的目标点。