


实验拓扑图：
![[node.png]]
对于奖励（reward）计算公式：
$$r_i(k+1)=r_i(k)+c_i(k)[\sum_{j \in N_i(k)} a_{i j}(k).\left.\left(z_j(k)-z_i(k)\right)\right]$$
对于 RoR 计算公式：
$$z_i(k)=r_i(k)/c_i(k)$$



## 对于FQ-Consensus 算法 ：
**初始值 Cost、RoR 对实验收敛的结果具有较大的影响：**

初始cost：[12, 10, 9, 11, 8, 7, 9, 6, 5, 13]  --原论文值
初始 ROR：[30, 40]随机 --- 原论文值
> 原来的实验中初始 state 值为[0, 1]

FQ-Consensus 5000步都无法收敛
![[consensus4.png]]


初始cost：[12, 10, 9, 11, 8, 7, 9, 6, 5, 13] 
初始 ROR：[5，10] 随机
好的情况下的收敛图：

![[consensus3.png]]

初始 ROR：[35, 40]随机
![[consensus2.png]]

当我尝试修改 cost 的值时，最后 ROR 的收敛曲线也不是太好。
**对目前来说，RoR 的值、可变范围越小，收敛结果越好**。

初始 ROR：[0 ，1]随机

![[consensus1.png]]

## 对于OPDPG方法

以初始 cost：[12, 10, 9, 11, 8, 7, 9, 6, 5, 13]  、初始 ROR：[30, 40]随机值进行训练
训练后的 rl 方法，经测试可以很快达到收敛
![[consensus5.png]]

在ROR：[10, 20]中也具备一定的适应性
![[consensus6.png]]