# Learning Quadrupedal Locomotion over Challenging Terrain







## 一、传统方法

设计**复杂的状态机**来协调运动基元（motion primitives）和反射控制器（ reflex controllers）的执行；许多系统明确估计了状态（地面接触和滑移），但是在复杂场景（泥沙、植被、雪）中容易变得不稳定。

为了触发状态之间的转换或反射的执行，许多系统都明确地预估状态，例如地面接触和滑行移动。这种预估通常是基于经验设置的，并且在存在诸如泥土、雪地或植被等未建模因素的情况下可能会变得不稳定。还有一些在脚部使用接触式传感器的系统，在野外条件下也会变得不可靠。

> 传统方法的弊端就是随着对复杂场景的考虑，系统的维护开发将会越来越复杂，而且还会受到极端情况的影响。即传统方法很难将所有的情况考虑在内。



## RL方法

目前RL的方法通常用于平坦的环境，

优点：训练好模型后不需要为不同的环境而进行调整。

## 二、 本文方法

传统RL方法通常仅用于平坦的环境中，无法处理复杂环境的任务。

- 感知信息：关节编码器和惯性测量单元（IMU）的感知信息。

  > 使用LiDAR或相机（外部传感器）的地形估计管道？？在某些复杂环境中会失效，而我们使用历史本体感知信息，可以更好帮助控制器进行全向运动，在未经训练的环境中，也可以更具鲁棒性。

- 测试：该控制器通过两代 ANYmal 四足机器人，在各种复杂环境下进行测试。

  > 不同的机器人在运动学（kinematics）, 惯性（inertia）和 执行器（actuators）

- 训练环境：刚性地形和一套生成地形剖面（山丘和台阶）的程序

  



### 创新点：

- 通过使用**时序卷积网络**（ Temporal Convolutional Network ；TCN）替代传统的MLP；通过序列模型，利用历史本体感知信息，产生驱动力决策
- **特权学习**；通过强化学习直接对粗糙地形开始训练，是无法成功的。因此将训练过程分成了两个阶段：
  - 首先训练一个能获取“特权信息”的教师网络，即机器人与地形接触的真实地面信息。（可以是模拟环境也可以扩展到真实环境）
  - 然后通过训练好的这个特权教师去指导仅使用自身感知传感器的机器人。
- 使用**自动课程学习**（automated curriculum）；根据控制器在训练过程中不同阶段的表现，自适应的合成地形。腿部控制器可以穿越他们，同时变得更加稳健。通过评估参数化地形的可穿越性，并使用粒子过滤来维持中等难度的地形参数分布，其中参数会随着神经网络的训练而变化，训练也越具有挑战性了。









### 当前存在弊端：

- 相比于自然界中四足动作，本次试验仅表现出快步走这一特征。
- 因为仅使用本体感知信息，所以在面临一些极端环境时，可能动作会比较偏保守一些，因此将外部感知与本体感知相结合，受损下在面临一些致命条件（悬崖），也可以很好的穿过，在保证安全的条件下，提高行走速度和能量效率。
- 





优势：

### 自然环境

在自然环境下（平坦）， 本文提出的方法平均速度更高，但代价更少。



### DARPA 地下挑战

第一个通过无模型强化学习训练腿部运动控制器的方法，比赛中通过控制两个两个ANYmal-B机器人完成相应的任务，控制器的故障率为0。





### 室内环境

**面对需要较高障碍物**：

四足机器人在面对障碍物时，仅依靠本体的感知观测信息可以识别出脚的陷入困境，并将脚抬起越过障碍物，对于传统控制器设计方法，需要明确建立这种反射，并通过更高级别的状态机来协调其执行。本文提出的方法无需建立这样特殊的关联反射，控制器学习到的策略更具有通用性，与特定的触发事件无关。

**面对陷阱时**：

面对各种复杂陷阱环境时（障碍、干扰），对于传统方法，脚接触没有发出陷阱事件的信号，所以使用脚接触事件作为触发器的脚本控制器无法正确处理这种情况，本文提出的方法，将感知信息流作为整体进行分析，并在没有提前假设接触位置的情况下进行训练， 学习各种干扰环境下的反应。即使在训练环境中即使没有负重的情况，本文提出的方法也可以达到较好的效果。

**包含负载的平面**：

对于平面上包含负载的机器来说，通过分析各个方向速度剖面，相比于传统的方法，速度剖面偏离中心较小，航向误差（机器人指令速度和基本速度的角度）也较低。

**光滑地面**：

传统方法无法在光滑地面中运动，而本文提出的方法可以按照指令方向成功运动



# 补：

相较于轮式和履带式机器人，可以穿越更复杂的环境。

通常目标命令由人工操作员或更高级的导航控制器给出





> 



# 二、 整体流程

## Command vector

命令向量被定义为：$\left\langle\left({ }_{I B}^{B} \hat{v}_{T}\right)_{x y},\left(\hat{\omega}_{T}\right)_{z}\right\rangle$，有点类似线速度和角速度，不过这里大小仅代表方向不代表速度的大小。

- 第一部分表示相对于基坐标的目标水平方向：$\left({ }_{I B}^{B} \hat{v}_{T}\right)_{x y}:= \left\langle\cos \left(\psi_{T}\right), \sin \left(\psi_{T}\right)\right\rangle$。其中$\psi_{T}$表示相对于基坐标系的偏航叫
- 第二部分表示绕z轴旋转方向：$\left(\hat{\omega}_{T}\right)_{z} \in\{-1,0,1\} $，其中1表示绕z轴进行逆时针旋转，-1表示绕z轴进行顺时针旋转。

其中停止命令为： $\langle 0.0,0.0\rangle$

> 本文控制器只控制方向，不控制整体速度。 因为在具有挑战挑战的地形上，比如上下坡时，目标的速度的可行范围往往不同。

## 方法概述

![image-20220628120508171](img/Paper1-1.png)

### Teacher Policy

首先对于教师网络通过强化学习的方法，利用特权信息在仿真环境进行训练，学习输入状态与动作之间的隐藏关系$\bar{l}_{t}$ 。

> 教师网络仅用于仿真环境中，学生网络可以用于真实机器上，

特权信息主要包含：

对于教师网络，是完全可观测环境，智能体能够观察到环境的所有状态，所以将运动控制描述为MDP。

**神经网络输入信息**：$s_t : = \langle o_t,x_t \rangle $

- $o_t$：智能体的自身状态信息（measurement vector）

- $x_t$：特权信息；智能体与地形的接触信息（感知接触的状态信息、接触力）。地形属性 （剖面、摩擦系数、干扰因素 ）。

  > 对于特权信息在现实中是无法得到的，所以教师网络是在一种理想的情况下进行训练的。通过给定越多有用的信息，训练出来的策略才有可能会更好，

### Student Policy

对教师网络进行模仿，教师网络的$\bar{l}_{t}$和$\bar{a}_{t}$用于指导学生网络$$l_t$$和$$a_t$$的学习。学生控制器学习仅依赖本身可用的传感器信息（本体感知）。

类似于Actor-Critic网络的思想，但是学习方式不一样，对于student policy更像是监督学习，将自己的得出的值与teacher policy 的值进行误差计算。

输入除 

### Automatic terrain curriculum

利用了自动课程学习（automatic curriculum learning）的思想

地形是自适应生成的，控制器会经历各种合成地形的考验，根据控制器在训练阶段的表现，会匹配合适难度的地形。

定义了一种**可遍历**的策略：设计了一种**基于采样**的方法来选择训练过程中具有适当难度的地形、使用粒子滤波的方法来保持（中等难度）地形的分布。

其中地形课程被用于教师网络和学生网络的学习

###  control architecture

**根据本体感知信息，生成运动轨迹，然后跟踪轨迹**



通过使用策略调制轨迹生成（Policies Modulating Trajectory Generators (PMTG)）的架构去提供运动生成的优先级。

根据合成剩余位置命令（？），利用已经学习好的本体感知策略神经网络（学生策略）去调节腿部相位和运动基元（原语），

仿真使用了机器人关节 PD 控制器的动力学模型（？）

主要包含两个部分：**运动生成和跟踪**，

控制器输入：仅包括本体感知的一系列信息：基底速度、方向和关节状态。

> 不包含外部传感器：触觉传感器、摄像头、深度传感器。同时也不包含脚接触状态或估计的地形几何体

控制器输出：关节目标位置

#### 运动生成

运动生成策略是基于周期性腿部相位。

> 以往工作通常利用预定义的脚接触状态。

每个腿部周期相位变量：$$\phi_i \in [0,2\pi)$$

运动生成的输出：目标脚部位置





通过PMTG框架（四个相同的脚轨迹生成器；foot trajectory generators (FTGs) ）结合神经网络来调节 控制器



### 跟踪控制

采用解析逆运动学（inverse kinematics  (IK) ）和关节位置控制完成。通过解析你运动学计算关节目标位置，然后由关节位置PD控制器跟踪关节目标位置。

> 使用解析逆运动学的原因是：最大限度地提高计算效率并重用现有位置控制执行器模型

