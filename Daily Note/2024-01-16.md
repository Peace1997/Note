

# 什么是大模型？一文读懂大模型的基本概念
大模型（Large Model,也称基础模型，即 Foundation Model
大语言模型（Large Language Model，LLM）
GPT（Generative Pre-trained Transformer）


## 定义
大模型是指具有**大规模参数**和**复杂计算结构**的机器学习模型。大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了**智能的涌现**，展现出类似人类的智能。具备**涌现能力**的机器学习模型就被认为是独立意义上的大模型，这也是其和小模型最大意义上的区别

>涌现能力：
>当模型的训练数据和参数不断扩大，直到达到一定的临界规模后，其表现出了一些未能预测的、更复杂的能力和特性，模型能够从原始训练数据中**自动学习并发现新的**、**更高层次**的特征和模式，这种能力被称为“涌现能力”。

## 特点

ChatGPT 的巨大成功,就是在微软Azur强大的算力以及 wiki 等海量数据支持下，在 Transformer 架构基础上，坚持 GPT 模型及人类反馈的强化学习（RLHF）进行精调的策略下取得的。

- 涌现能力指的是当模型的训练数据突破一定规模，模型突然涌现出之前小模型所没有的、意料之外的、能够综合分析和解决更深层次问题的复杂能力和特性，展现出类似人类的思维和智能。**涌现能力也是大模型最显著的特点之一。**
- **迁移学习和预训练**： 大模型可以通过在大规模数据上进行预训练，然后在特定任务上进行微调，从而提高模型在新任务上的性能。
- **多任务学习:** 大模型通常会一起学习多种不同的 NLP 任务,如机器翻译、文本摘要、问答系统等。
- **迁移学习和预训练**： 大模型可以通过在大规模数据上进行预训练，然后在特定任务上进行微调，从而提高模型在新任务上的性能
- **自监督学习**： 大模型可以通过自监督学习在大规模未标记数据上进行训练，从而减少对标记数据的依赖，提高模型的效能。


# 模型微调

常见的模型微调方法：
- Fine-tuning：这是最常用的微调方法。通过在预训练模型的最后一层添加一个新的分类层，然后根据新的数据集进行微调。
- Feature augmentation：这种方法通过向数据中添加一些人工特征来增强模型的性能。这些特征可以是手工设计的，也可以是通过自动特征生成技术生成的。
- Transfer learning：这种方法是使用在一个任务上训练过的模型作为新任务的起点，然后对模型的参数进行微调，以适应新的任务。



# 中国人工智能系列白皮书大模型技术（2023 版）



*“无标注数据预训练标注数据微调”的预训练模型*
对于基于有标注数据微调的预训练模型，其训练过程分为两个阶段：首先，在大规模的无标注数据上进行预训练，然后使用一小部分有标注数据进行微调。这种方法可以有效地利用无标注数据中的信息来提高模型的性能，同时又能够更好地适应具体的任务需求。


*大规模无标注数据预训练+指令微调+人类对齐 的大模型*