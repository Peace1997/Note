# Introduction

**持续强化学习**（Continual Reinforcement Learning，CRL）是强化学习的一个分支，专注于在动态和不断变化的环境中学习和适应。CRL 的核心目标是构建能够**持续学习的新知识并保留旧知识的智能体**，即解决**稳定性-可塑性难题**。


***稳定性-可塑性***
- **稳定性**：系统需要保留已有知识，避免因学习新任务或新数据而丧失已掌握的技能或信息。这种现象称为**灾难性遗忘（catastrophic forgetting）**。
- **可塑性**：系统需要能够快速学习新知识或适应新的环境，这需要模型具有足够的灵活性。

***目标***
- 知识保留：防止学习过程成发生灾难性遗忘，提高稳定性并确保最佳可塑性。
- 学会学习：学习如何自我学习，类似于元学习。
- 共享结构：在智能体不断学习的情况下，利用过去解决过的问题的方案解决新的问题，通过掌握所学知识的共享结构



# CPPO

CPPO 通过利用学习**权重策略**来平衡策略学习和知识保留（权重策略），以改进 PPO 算法在持续学习人类偏好方面的能力。

***样本划分***

CPPO 算法的核心是根据**样本的奖励和生成概率**，将输出样本划分为五种类型：
- **高性能样本**： 生成概率高，奖励也高，表明旧策略已经表现良好。CPPO 会增强新策略对这类样本的学习和知识保留。
- **过拟合样本**： 生成概率高，但奖励低，表明旧策略可能过拟合了有偏差的样本。CPPO 会增强对这类样本的策略学习，但减缓知识保留，以降低生成偏差样本的概率。
- **高方差样本**： 生成概率低，但奖励高，表明样本方差较高。CPPO 会增强对这类样本的策略学习，以提高生成此类样本的概率，从而获得更稳定的性能。
- **噪声样本**： 生成概率和奖励都低，被认为是噪声数据。CPPO 会减缓对这类样本的知识保留和策略学习，以避免过度优化。
- **普通样本**： 生成概率或奖励至少有一个落在判别区间内，CPPO 不会改变学习过程


***权重策略***
**权重策略**：根据样本的不同特性，为每个样本分配不同的权重，以平衡策略学习和知识保留之间的关系

CPPO 提供了两种确定平衡权重的方法：
- **启发式权重法**： 根据样本类型，线性增加或减少权重。
- **可学习权重法**： 将权重策略转化为不等式约束，并通过优化拉格朗日函数来学习最佳权重。

