# 简述

***1. 策略***
在策略梯度方法中，策略通常表示为一个带参数的概率分布，即$\pi_θ(a∣s)$ 。

***2. 策略目标***
策略梯度方法的目标是最大化累积奖励的期望值，即： $J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} R(\tau)$

***3. 策略梯度***
为了优化策略，我们需要计算目标函数 $(\theta)$ 对策略参数 $\theta$ 的梯度： $\nabla_\theta J(\theta)$
根据**策略梯度定理**，这个梯度可以表示为：
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) R(\tau) \right]$$
可以通过以下步骤更新策略参数：
- 采样：根据当前策略$\pi_{\theta}$, 采样一条轨迹$\tau$
- 计算回报：对于每个轨迹计算其累计奖励$R(\tau)$
- 梯度更新：根据策略梯度，使用回报$R(\tau)$作为权重，更新策略参数

> 回报作为权重的关键原因在于：每个状态下采取的动作的好坏需要通过其对**最终累积奖励**的影响来评估。

***4. 策略梯度优缺点***

优点：
- **策略的直接优化：** 相较于值函数的方法，策略梯度方法可以直接优化策略，不需要依赖于间接推导的动作价值函数。
- **处理连续运动空间：** 基于值函数的 DQN 算法仅适用于离散动作空间，策略梯度算法可以应用于连续动作空间。
- 

缺点：
- **贡献分配问题**：对于每个状态-动作对使用的是"全局奖励"作为它的权重，即以一条轨迹作为更新单位。这样的累积奖励分配方法，无法很好的体现出当前状态 -动作对对整个轨迹的贡献。
- **奖励总为正值的问题**：按照正奖励增加动作概率，负奖励减少动作概率的原则，如果所有动作奖励都为正，则按理所有的动作概率都该增加，这显然是无法完成。
-

## 策略优化

### 最大化轨迹的加权对数似然

在基于策略的强化学习中，我们希望通过优化策略 $\pi_\theta(a \mid s)$ ，使得策略生成的轨迹能够最大化预期累积奖励 $R(\tau)$ ：
$$
J(\theta)=\mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
$$
－这里的 $R(\tau)=\sum_{t=0}^T r_t$ 是轨迹的累积奖励。


#### 对数似然
将该目标与轨迹的概率关联，可以将问题转化为最大化轨迹的加权对数似然：
$$
J(\theta)=\mathbb{E}_{\tau \sim \pi_\theta}\left[\log p_\theta(\tau) R(\tau)\right]
$$
具体分析：
1．轨迹的对数似然为：
$$
\log p_\theta(\tau)=\log p\left(s_0\right)+\sum_{t=0}^T\left[\log \pi_\theta\left(a_t \mid s_t\right)+\log P\left(s_{t+1} \mid s_t, a_t\right)\right]
$$
2．策略的优化参数只影响 $\log \pi_\theta\left(a_t \mid s_t\right)$ ，因此优化目标可以简化为：
$$
J(\theta)=\mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T \log \pi_\theta\left(a_t \mid s_t\right) R(\tau)\right]
$$
$$
\nabla_\theta J(\theta)=\mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) R(\tau)\right]
$$
这就将策略优化问题看作最大化对数似然问题，其中对数似然的权重是轨迹的奖励 $R(\tau)$

由于轨迹 $\tau$ 的概率 $p_\theta(\tau)$ 直接依赖于策略 $\pi_\theta(a|s)$ ，**最大化轨迹的似然等价于提高策略生成高奖励轨迹的概率。** 
对数似然梯度指向的是当前策略下，增加高奖励轨迹概率的方向。当 $\log \pi_\theta(a_t|s_t)$ 的值越大（策略选择当前动作的概率越高），梯度的放大作用也越明显。


#### 权重的不同来源及其影响

对数似然的权重会显著影响策略的优化过程。这个权重通常决定了策略如何分配更多的学习资源给哪些轨迹或动作。

***1. 权重的类型***

 **（1）累计奖励作为权重**：直接使用轨迹的总回报作为权重：
$$w_t = R(\tau) = \sum_{k=t}^T r_k$$
​
- 高奖励轨迹会被赋予更大的权重，这会倾向于优化策略，使其更频繁地产生高奖励的轨迹。
- 缺点：如果奖励尺度过大，可能导致梯度爆炸；如果轨迹奖励的方差过大，会导致优化不稳定。

 **（2）优势函数作为权重：**
优势函数评估某一动作相对于平均动作的优势：
$$A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$$
- 优势函数权重能够更细粒度地指导策略优化，只关注比**平均水平**表现好的动作，从而提升样本效率。
- 使用优势函数权重的算法（如 Actor-Critic 方法）更稳定，因为它通过基线$V(s_t)$ 减少了方差。

 **（3）熵正则化项作为权重：**
在最大熵 RL 中，引入熵作为权重的一部分，目标函数变为：
$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \log \pi_\theta(a_t|s_t) \cdot (R(\tau) + \alpha \mathcal{H}(\pi_\theta(\cdot|s_t))) \right]
$$

- 熵项增加了探索的多样性，避免策略过早收敛到次优解。
- 权重中的熵正则化系数$\alpha$的大小直接影响探索与开发的权衡。


***2. 权重的影响：***

（1） 正面影响
- **引导优化方向**：权重能够==**指引优化过程向更有价值的轨迹和动作集中**==，提升学习效率。
-  **减少不必要探索：** 通过优势函数或 TD 误差，权重可以剔除低价值动作的影响。
- **鼓励多样性：** 在最大熵方法中，权重中的熵项鼓励策略探索多种可能的轨迹。

（2）负面影响
- **方差问题：** 权重来源（如奖励）如果波动较大，会导致策略梯度的高方差，影响收敛稳定性。
- **过度偏向高奖励轨迹：** 如果权重过于强调高奖励轨迹，可能忽略潜在的其他次优解，导致过早收敛。
- **权重的不合理设定：** 权重尺度或形式选择不当（例如奖励过大或过小）可能导致梯度爆炸或消失。