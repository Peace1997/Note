

# 1. 文本数据处理

- **序列数据**：一串相互依赖 (相互联系，前后依赖) 的数据流。
- **文本数据**：文本是最常用的序列数据之一，可以理解为字符序列或单词序列
- **文本向量化**：文本转换为数值张量的过程
  - 原因：深度学习模型不会接收原始文本作为输入, 它只能处理数值张量
  - 常用方法：
	- 将文本分割为单词、字符，并将每个单词、字符转换为一个向量
	- 提取单词或字符的 n-gram，并将每个 n-gram 转换为一个向量

>n-gram 是多个连续单词或字符的集合(n-gram 之间可重叠) 。

# 2. 文本向量化

- **标记**：文本分解而成的单元 (单词、字符、n-gram)
- **分词**：将文本分解成标记的过程
- 向量与标记相关联方法：**one-hot编码**和**标记嵌入（词嵌入）**

所有文本向量化过程，都是应用某种**分词提案**，然后将生成的**标记**与**数值向量**相关联。这些向量组合成**序列张量**,被输入到深度神经网络中

![400](nlp_1.png)

## 2.1 one-hot编码

***原理***

它将每个单词与一个唯一的整数索引相关联, 然后将这个整数索引 i 转换为**长度为 N 的二进制向量** (N 是词表大小) ,这个向量只有第 i 个元素是 1,其余元素都为 0。

> 每个单词都对应一个唯一的索引，将这些所有索引转换成相应个数的二进制向量，这个向量只有第i个元素是1，其他的都是0。

***特点***

- 让不同的分类处在**平等**的地位，不会因为数值的大小而对分类造成影响。
- 该编码只能反映某个词是否在句中出现，无法衡量不同词的**重要程度**
- 使用One-Hot 对文本进行编码后得到的是**高维稀疏矩阵**，会浪费计算和存储资源；

举例：
![300](nlp_2.png)


## 2.2 词嵌入
word embedding

***原理***
是一种词的类型表示，具有相似意义的词具有相似的表示，是将词汇映射到实数向量的方法总称。

> 词嵌入模型本身并不重要，重要的是生成出来的结果——**词向量**。

***特点**
- 他可以将文本通过一个低维向量来表达，不像one-hot那么长。
- 语意相似的词在向量空间上也会比较相近。

***常用方法***
- **Embedding Layer**： 一开始是随机的词向量,然后对这些词向量进行学习,其学习方式与学习神经网络的权重相同
- **Word2vec**：这是一种基于统计方法来获得词向量的方法，有2种训练模式：通过上下文来预测当前词；通过当前词来预测上下文
- **GloVe**：是对 Word2vec 方法的扩展，它将全局统计和 Word2vec 的基于上下文的学习结合了起来。

### 2.2.1 Word2vec

**Word2Vec**：就是把单词转换成向量。它本质上是一种**单词聚类**的方法，是实现单词语义推测、句子情感分析等目的一种手段。Word2vec有2种训练模式：CBOW、Skip-gram

***特点***：
- 维度更少，速度更快，通用性很强，可以用在各种 NLP 任务中
- 词和向量是一对一的关系，所以多义词的问题无法解决

#### CBOW

通过上下文来预测当前值。相当于一句话中扣掉一个词，让你猜这个词是什么。

> CBOW 我 _ 吃早饭

#### Skip-gram

通过当前词来预测上下文。相当于给你一个词，让你猜前面和后面可能出现什么词。

> Skip-gram : _ 喜欢 _ 


