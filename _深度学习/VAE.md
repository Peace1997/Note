
# Abstract

**变分自编码器（VAE, Variational Autoencoder）** 是一种生成模型，它通过学习数据的概率分布来生成新的样本。相对于传统的自编码器（Autoencoder），VAE 对编码的潜在空间进行约束，使得该空间能够生成**更平滑、连续的分布**，从而提高生成样本的质量和多样性。VAE 的提出主要解决了传统 Autoencoder 在生成新数据时难以控制生成质量的问题。

![[Pasted image 20241111203040.png]]

作用：获取数据并将其压缩成低维表示，然后将其重建为原始形式。


# VAE 的实现过程

VAE 的实现与普通 Autoencoder 有一些相似之处，但在细节上有很大的不同，具体实现步骤如下：
## 编码器（Encoder）
   - 与传统 Autoencoder 不同，VAE 的编码器不仅仅是将输入数据压缩为一个固定的潜在表示（例如一个向量），而是生成一个**概率分布**的参数，即潜在空间的均值和方差。
   - 给定输入$x$，编码器生成潜在变量$z$ 的分布参数$\mu$（均值）和$\sigma$（标准差）。
   - 这些参数通常使用神经网络的全连接层来输出，因此，编码器输出的是$\mu (x)$ 和$\sigma (x)$ 两个向量。

## 重参数化（Reparameterization）
   - 为了从生成的分布中采样潜在变量$z$，VAE 引入了重参数化技巧。
   - 具体来说，从正态分布$N (\mu, \sigma^2)$ 采样$z$ ，即：
     $$
     Z = \mu (x) + \sigma (x) \cdot \epsilon
     $$
     其中$\epsilon \sim N (0, 1)$ 是标准正态分布的噪声。
   - 这个技巧的关键在于，使得采样$z$ 的过程变为可微，从而可以进行梯度传播，进而用反向传播法来优化模型。

## 解码器（Decoder）
   - 解码器接受潜在变量$z$ 作为输入，生成数据的重构$\hat{x}$，即解码器输出的是对输入$x$ 的重构。
   - 通过学习解码器的参数，VAE 试图使得重构数据$\hat{x}$ 与原始数据$x$ 尽可能相似。

## 损失函数（Loss Function）
   VAE 的损失函数由两部分组成：
   - **重构损失**：衡量重构数据$\hat{x}$ 和输入数据$x$ 的差异，通常使用均方误差（MSE）或交叉熵损失。重构损失保证解码器能够准确地重构输入数据。
   - **KL 散度损失**：将编码器生成的潜在分布$q (z|x)$ 与先验分布$p (z)$（通常为标准正态分布）进行比较。通过最小化 KL 散度，使得潜在表示$z$ 接近标准正态分布。
     $$
     \text{KL}(q (z|x) \| p (z)) = \int q (z|x) \log \frac{q (z|x)}{p (z)} \, dz
     $$
   VAE 的总损失函数为：
   $$
   \text{Loss} = \text{重构损失} + \text{KL 散度损失}
   $$

训练过程：
   - 通过反向传播最小化总损失函数，优化编码器和解码器的参数。
   - 训练完成后，可以通过解码器从标准正态分布中采样潜在变量$z$，生成新的数据样本。

## VAE 相对于 Autoencoder 的改进

1. **潜在空间的结构化分布**：
   - VAE 通过 KL 散度将潜在空间约束为标准正态分布。这样编码的潜在空间是连续的，样本间的过渡平滑，使得采样新的$z$ 值生成的样本更有意义和一致性。
   - 传统 Autoencoder 的潜在空间可能没有明确的结构，导致采样生成新样本时，可能会生成无法识别的数据。

2. **生成数据的能力**：
   - VAE 是生成模型，能够生成符合训练数据分布的新样本。只需从标准正态分布中采样潜在变量$z$，然后通过解码器生成数据。
   - 传统 Autoencoder 没有约束潜在空间的分布，因此生成数据的效果较差，难以保证生成数据的质量。

3. **概率模型**：
   - VAE 是概率模型，它为输入数据生成了一个潜在分布（均值和方差）。这种分布化的表示不仅提高了生成数据的多样性，还允许在潜在空间中探索不同样本的潜在变量变化。
   - Autoencoder 是确定性的映射，没有使用概率模型，不具备 VAE 的生成优势。

# Basics

## Autoencoder

Autoencoder（自编码器）是一种**无监督学习模型**，通过编码-解码结构学习数据的压缩和重构。
![[Pasted image 20241111201548.png|425]]

***编码器（Encoder）***：将输入数据压缩成低维的潜在空间（latent space，$z$）表示。
- **输入层**：接收原始数据（如图像的像素值或特征向量）。
- **隐藏层**：通过一系列非线性变换，将数据从高维空间映射到低维潜在空间，得到“编码”后的表示 $z$。
- **作用**：是提取数据的主要特征，因此潜在空间的维度通常比输入数据维度低得多。

***解码器（Decoder）***：将潜在表示还原为原始数据的近似。
- **隐藏层**：从编码器生成的潜在表示开始，通过非线性变换将其还原成输入数据的高维表示。
- **输出层**：解码器的输出尽可能接近输入数据，用于训练时的重构损失计算。
- **损失函数**：计算解码器的输出与输入之间的差异，通常使用均方误差（MSE）或交叉熵损失。
- **作用**：是尽量还原编码的输入数据。

***作用***：
- 数据降噪
- 数据生成
- 降维与特征提取
- 异常检测
- 图像超分辨率

***优点***：
- 有效的无监督学习，可以用于无标签数据的特征提取。
- 能够自适应地学习数据的低维表示，实现数据降维、去噪、生成等任务。

***缺点***：
- 通常没有标签监督，难以确保潜在表示具有明确的意义。
- 可能会出现过拟合，尤其是对小数据集，导致无法泛化到新数据。
- 重构效果不一定总是能达到理想效果，特别是在处理复杂数据时

***Bottleneck &&  Latent Space*** 
瓶颈层（Bottleneck）是自编码器模型结构中的具体层，是一个实现信息压缩的具体位置。
潜在空间（Latent Space）是数据在瓶颈层压缩后的表示，是一个数学概念和数据空间


## PCA

后验分布 （posterior distribution） p (x| z) ：对于给定图像 x ，生成潜在向量 z 的概率。-- 对应Decoder
似然分布（likelihood distribution） q (z|x) ：对于给定潜在向量 z ，重建图像 x 的概率。--- 对应Encoder

重参化（reparameterization）


# 总结


