

两篇论文：

《Self-Learning based Distributed Resilient Consensus for Multi-Agent Systems》
通过强化学习方法解决多智能体弹性一致性（Multi-agent Resilient Consensus (MARC)）问题，在故障节点存在的情况下，以分布式方式实现状态一致性。提出了两种解决方法


《Fairness based Multi-task Reward Allocation in Mobile Crowdsourcing System》
为解决奖励分配的公平性，提出了三种奖励 分配算法，更好的实现公平性和收敛性。


**论文十问

Q1论文试图解决什么问题？

Q2这是否是一个新的问题？

Q3这篇文章要验证一个什么科学假设？

Q4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？

Q5论文中提到的解决方案之关键是什么？

Q6论文中的实验是如何设计的？

Q7用于定量评估的数据集是什么？代码有没有开源？

Q8论文中的实验及结果有没有很好地支持需要验证的科学假设？

Q9这篇论文到底有什么贡献？

Q10 下一步呢？有什么工作可以继续深入？


1. 复现上次清华大学团队提出的MAANS代码
	1. 【环境配置】；Habitat 环境安装：安装了一个habitat环境，需要和数据集进行适配，我现在的话先在云服务器上多试错几次，为日后在我们自己服务器上安装做准备。
	2. 【源码学习】
		- 作者提供的源码中，少了创建环境的模块，作者针对该问题没有进行回复。
		- MAANS 框架有三大模块，每一块展开后很复杂，我重点关注其中 MSP 决策模块，这个模块是基于 MARL、CNN、Transformer 进行的，CNN 之前有过一定的实践经验，但是对于 Transformer 不是很熟悉。这个 Transformer 也是我们这个框架的基础结构之一。重点对这一块进行学习。
	3. 【Transformer学习】看了不少Transformer的课程，我认为台大李宏毅老师的Transformer相关课程讲的最清楚，然后使用Pytorch对基础Transformer进行复现。




协作探索任务：基于一致性邻接权重分配的思想，控制每个智能体之间的通信信息的权重，也就是神经网络输入前，加入一个通信信息分配的过程。










每个时间步中，在目标决策层，每个智能体根据环境的观测信息，通过MARL算法决策自身要前往的目标点；然后将目标点输入目标导航层，智能体根据全局路径规划算法和局部路径规划算法，向目标点移动一段距离（运行时间为9s），更新每个智能体的状态，若未完成整体任务要求，则继续迭代，由目标决策层重新分配目标点，目标导航层进行路径规划，直至所有智能体无冲突的到达所有目标点。




>每个时间步中每个智能体根据自身环境的观测信息，首先通过MARL算法决策要前往的目标点，再根据全局路径规划算法和局部路径规划算法，引导智能体向目标点移动一段距离，更新每个智能体的观测信息并重新分配目标点，直至所有智能体无冲突的到达所有目标点。


  

  

ROS作为机器人软件开发和控制的平台，广泛应用于真实机器人和仿真环境中。Gazebo与ROS紧密集成，Gazebo内置多种物理引擎，支持多种机器人模型和传感器（激光雷达、相机、惯性测量单元）的导入，可以更好的模拟真实机器人的行为，包括机器人的动力学、运动控制和传感器数据等。相较于其他论文[8-11]使用的小型格点环境，Gazebo环境因其可定制性和真实性，可以灵活的搭建所需的各种场景，降低算法在真实环境的迁移难度。因此在本文中选用Gazebo物理仿真平台创建了训练和测试的仿真环境。

  

> Gazebo内置多种物理引擎，支持多种机器人模型和传感器（激光雷达、相机、惯性测量单元）的导入，与ROS紧密集成，便于将仿真环境的训练结果导入真实机器人中进行测试。


经典多智能体强化学习算法 MADDPG，


对于评审意见：
1. 在本文2.1 节分层控制结构中，对分层控制结构的目标决策层和目标导航层之间的结合、目标点移动距离、目标点重新分配做了更详细的介绍。
2. 在本文 3.1 节实验设置中，增加了对 Gazebo 环境的介绍，以及 Gazebo 相较于现有工作强化学习环境的优势。
3. 在本文3.2节训练结果分析与3.3节测试结果分析中，增加了经典多智能体强化学习算法MADDPG的训练、测试的对比实验。



策略梯度算法的核心就是计算损失函数关于策略函数参数的梯度，这个梯度指导我们更新策略函数的参数。具体来说，对于离散动作空间的情况，策略梯度的计算公式为：

$$\nabla_\theta J(\theta) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta (a_t|s_t) G_t$$

其中，$J(\theta)$ 是策略函数 $\pi_\theta$ 的目标函数，$G_t$ 是从时间步 $t$ 开始的累计奖励（可能是回报或价值函数），$s_t$ 是状态，$a_t$ 是在状态 $s_t$ 下选择的行动，$\theta$ 是策略函数的参数。

对于连续动作空间的情况，可以使用基于高斯分布的策略函数，并使用梯度下降或其变种算法来最大化目标函数。具体来说，如果策略函数是基于高斯分布的，即：

$$\pi_\theta(a|s) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(a-\mu_\theta(s))^2}{2\sigma^2}\right)$$

其中，$\mu_\theta(s)$ 和 $\sigma$ 是策略函数的参数，则策略梯度的计算公式为：

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)]$$

其中，$Q^{\pi_\theta}(s,a)$ 是在状态 $s$ 下选择行动 $a$ 的价值函数。这个期望可以通过采样得到。



确定性策略梯度（Deterministic Policy Gradient，DPG）算法是一种策略梯度算法，它适用于连续动作空间的问题，并且比基于高斯分布的策略函数更稳定。

DPG算法主要包括两个部分：一个是确定性策略函数，另一个是 Q 函数（状态-行动价值函数）。与基于高斯分布的策略函数不同，确定性策略函数直接输出确定性行动，而不是采样。

确定性策略函数可以表示为：

$$a = \mu(s;\theta_\mu)$$

其中，$s$ 是状态，$\mu$ 是确定性策略函数，$\theta_\mu$ 是策略函数的参数。

Q 函数用于估计给定状态和行动的价值，即 $Q(s,a)$。DPG算法使用 Q 函数来更新策略函数的参数。具体来说，可以通过最小化 Q 函数与目标 Q 函数之间的差异来更新策略函数的参数，即：

$$\theta_\mu \leftarrow \theta_\mu + \alpha \nabla_{\theta_\mu} Q(s,a|\theta_Q) \nabla_a \mu(s;\theta_\mu)|_{a=\mu(s)}$$

其中，$\alpha$ 是学习率，$\nabla_{\theta_\mu} Q$ 是 Q 函数相对于 $\theta_\mu$ 的梯度，$\nabla_a \mu(s;\theta_\mu)|_{a=\mu(s)}$ 是确定性策略函数相对于行动的梯度，$\theta_Q$ 是 Q 函数的参数。

DPG 算法在更新策略函数的过程中，使用了 Q 函数的信息来指导策略更新，从而提高了算法的效率和稳定性。





MADDPG（Multi-Agent Deep Deterministic Policy Gradient）算法是一种基于 actor-critic 框架的多智能体强化学习算法。以下是MADDPG算法的具体实现步骤：

1.  环境建模：将多智能体系统的环境建模成一个 Markov 博弈模型。环境的状态由所有智能体的状态组成，动作也由所有智能体的动作组成。奖励函数应该考虑到智能体之间的协作关系，促进整个团队的性能提升。
    
2.  神经网络架构：为每个智能体建立两个神经网络：一个是行动者（Actor）神经网络，用于决策智能体的动作；另一个是评论家（Critic）神经网络，用于评估智能体策略的好坏。这两个网络的参数分别记为 $\theta^{i}_{\text{A}}$ 和 $\theta^{i}_{\text{C}}$，其中 $i$ 表示第 $i$ 个智能体。
    
3.  经验回放：维护一个经验回放缓存区，用于存储智能体的经验。每个经验包括状态 $s$、动作 $a$、奖励 $r$、下一个状态 $s'$ 和结束标志 $done$。
    
4.  训练过程：对于每个时间步 $t$ 和每个智能体 $i$，执行以下步骤：
    
    a. 从行动者神经网络中获取动作 $a_{t}^{i} = \pi(s_{t}^{i};\theta_{\text{A}}^{i}) + \epsilon_{t}^{i}$，其中 $\pi$ 表示行动者策略函数，$\theta_{\text{A}}^{i}$ 表示行动者神经网络的参数，$\epsilon_{t}^{i}$ 表示动作噪声。
    
    b. 执行动作 $a_{t}^{i}$，观察环境的奖励 $r_{t}^{i}$ 和下一个状态 $s_{t+1}^{i}$，并将经验 $(s_{t}^{i}, a_{t}^{i}, r_{t}^{i}, s_{t+1}^{i}, done_{t}^{i})$ 存储到经验回放缓存区中。
    
    c. 从经验回放缓存区中随机抽取一批经验 $(s_{j}^{i}, a_{j}^{i}, r_{j}^{i}, s_{j+1}^{i}, done_{j}^{i})$，并计算目标动作 $a_{j+1}^{i} = \pi(s_{j+1}^{i};\theta_{\text{A}}^{i'})$，其中 $\theta_{\text{A}}^{i'}$ 是目标行动者网络的参数。



论文将

- 设置了 6 种不同的输入来测试对智能体的影响。每个智能体具有不同的观察空间来研究不同输入的影响
- III. C 设计了设计了密集（dense）奖励来加速 Agent 的训练，建议按照相似的意义分类规整
- 在公式（1）存在公式与解释不对应的问题，公式中并未存在 r_s^t、s_{ss}^t 这一奖励项。对于 r_{tc}这项奖励并未详细解释。
- IV. A 的实验，测试了不同的输入对实验结果的影响，但并未进一步阐述为什么不同的输入参数会对训练结果产生影响。 



This article proposes a reinforcement learning method that combines global path planning, waypoint generator, and local path planning for autonomous navigation problems. The feasibility of the algorithm is verified through multiple experiments. Overall, the algorithm is described clearly and has certain practical significance. Some other concerns are as follows.

1. In Section III. C (Reward Functions) , a dense reward is designed to accelerate the training of the agent, and it is recommended to organize it according to similar meanings. At the same time, there is an issue with formula (1) where the formula does not correspond to the explanation. The reward terms r_s^t and s_{ss}^t do not exist in the formula. The reward term r_{tc} is not explained in detail.
2. In Section 4.A (Training Performance), different inputs were tested to investigate their impact on the experimental results, but it was not further explained why different input parameters would have an effect on the training results.
3. There are two citation issues on page three.

## Transformer

Transformer 主要用于特征提取；重新组合各个输入向量，得到更完美的特征。
模型都是自回归的[10]，即在生成下一个符号时将先前生成的符号作为附加输入。

对于每一步解码，模型都是自回归的[10]，即在生成下一个符号时将先前生成的符号作为附加输入。

Token向量
q: 查询向量，用于查询与其他输入向量之间的关系
k：被查向量
v:特征代表
```
Tokens是自然语言处理中最基本的单位，它们是文本中的单个单词、标点符号、数字等元素。在机器学习和深度学习中，tokens被用于表示文本信息，并在各种应用中扮演着重要的角色。

在自然语言处理中，q、k、v是Transformer模型中用于实现自注意力机制的三个向量。

-   Q（Query）向量表示查询向量，是用来和K向量进行点积操作以得到注意力权重的向量。在编码器中，Query向量是来自上一层的输出向量。
-   K（Key）向量表示键向量，用于与Query向量计算注意力权重。在编码器中，Key向量是来自上一层的输出向量。
-   V（Value）向量表示值向量，用于与注意力权重相乘并求和得到上下文向量，最终用于生成下一层的输出。在编码器中，Value向量也是来自上一层的输出向量。

使用Q、K、V向量的自注意力机制可以对输入的每个元素进行加权聚合，并产生对应的输出表示。这种机制可以有效地捕获文本中的上下文关系，进而提高各种自然语言处理任务的性能，例如语言建模、翻译和文本分类等。
```


Embedding：词转向量


位置编码问题：？？

在Transformer中可以支持行维度不同，但是列维度要相同，在MARL中，也就是保证每个智能体获取周围的智能体信息的数量是可以不同的，但是要保证每个智能体自身获取的状态信息是相同，


