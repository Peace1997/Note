

两篇论文：

《Self-Learning based Distributed Resilient Consensus for Multi-Agent Systems》
通过强化学习方法解决多智能体弹性一致性（Multi-agent Resilient Consensus (MARC)）问题，在故障节点存在的情况下，以分布式方式实现状态一致性。提出了两种解决方法


《Fairness based Multi-task Reward Allocation in Mobile Crowdsourcing System》
为解决奖励分配的公平性，提出了三种奖励分配算法，更好的实现公平性和收敛性。


**论文十问

Q1论文试图解决什么问题？

Q2这是否是一个新的问题？

Q3这篇文章要验证一个什么科学假设？

Q4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？

Q5论文中提到的解决方案之关键是什么？

Q6论文中的实验是如何设计的？

Q7用于定量评估的数据集是什么？代码有没有开源？

Q8论文中的实验及结果有没有很好地支持需要验证的科学假设？

Q9这篇论文到底有什么贡献？

Q10 下一步呢？有什么工作可以继续深入？


1. 复现上次清华大学团队提出的MAANS代码
	1. 【环境配置】；Habitat 环境安装：安装了一个habitat环境，需要和数据集进行适配，我现在的话先在云服务器上多试错几次，为日后在我们自己服务器上安装做准备。
	2. 【源码学习】
		- 作者提供的源码中，少了创建环境的模块，作者针对该问题没有进行回复。
		- MAANS 框架有三大模块，每一块展开后很复杂，我重点关注其中 MSP 决策模块，这个模块是基于 MARL、CNN、Transformer 进行的，CNN 之前有过一定的实践经验，但是对于 Transformer 不是很熟悉。这个 Transformer 也是我们这个框架的基础结构之一。重点对这一块进行学习。
	3. 【Transformer学习】看了不少Transformer的课程，我认为台大李宏毅老师的Transformer相关课程讲的最清楚，然后使用Pytorch对基础Transformer进行复现。




协作探索任务：基于一致性邻接权重分配的思想，控制每个智能体之间的通信信息的权重，也就是神经网络输入前，加入一个通信信息分配的过程。










每个时间步中，在目标决策层，每个智能体根据环境的观测信息，通过MARL算法决策自身要前往的目标点；然后将目标点输入目标导航层，智能体根据全局路径规划算法和局部路径规划算法，向目标点移动一段距离（运行时间为9s），更新每个智能体的状态，若未完成整体任务要求，则继续迭代，由目标决策层重新分配目标点，目标导航层进行路径规划，直至所有智能体无冲突的到达所有目标点。




>每个时间步中每个智能体根据自身环境的观测信息，首先通过MARL算法决策要前往的目标点，再根据全局路径规划算法和局部路径规划算法，引导智能体向目标点移动一段距离，更新每个智能体的观测信息并重新分配目标点，直至所有智能体无冲突的到达所有目标点。


  

  

ROS作为机器人软件开发和控制的平台，广泛应用于真实机器人和仿真环境中。Gazebo与ROS紧密集成，Gazebo内置多种物理引擎，支持多种机器人模型和传感器（激光雷达、相机、惯性测量单元）的导入，可以更好的模拟真实机器人的行为，包括机器人的动力学、运动控制和传感器数据等。相较于其他论文[8-11]使用的小型格点环境，Gazebo环境因其可定制性和真实性，可以灵活的搭建所需的各种场景，降低算法在真实环境的迁移难度。因此在本文中选用Gazebo物理仿真平台创建了训练和测试的仿真环境。

  

> Gazebo内置多种物理引擎，支持多种机器人模型和传感器（激光雷达、相机、惯性测量单元）的导入，与ROS紧密集成，便于将仿真环境的训练结果导入真实机器人中进行测试。


经典多智能体强化学习算法 MADDPG，


对于评审意见：
1. 在本文2.1 节分层控制结构中，对分层控制结构的目标决策层和目标导航层之间的结合、目标点移动距离、目标点重新分配做了更详细的介绍。
2. 在本文 3.1 节实验设置中，增加了对 Gazebo 环境的介绍，以及 Gazebo 相较于现有工作强化学习环境的优势。
3. 在本文3.2节训练结果分析与3.3节测试结果分析中，增加了经典多智能体强化学习算法MADDPG的训练、测试的对比实验。



策略梯度算法的核心就是计算损失函数关于策略函数参数的梯度，这个梯度指导我们更新策略函数的参数。具体来说，对于离散动作空间的情况，策略梯度的计算公式为：

$$\nabla_\theta J(\theta) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta (a_t|s_t) G_t$$

其中，$J(\theta)$ 是策略函数 $\pi_\theta$ 的目标函数，$G_t$ 是从时间步 $t$ 开始的累计奖励（可能是回报或价值函数），$s_t$ 是状态，$a_t$ 是在状态 $s_t$ 下选择的行动，$\theta$ 是策略函数的参数。

对于连续动作空间的情况，可以使用基于高斯分布的策略函数，并使用梯度下降或其变种算法来最大化目标函数。具体来说，如果策略函数是基于高斯分布的，即：

$$\pi_\theta(a|s) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(a-\mu_\theta(s))^2}{2\sigma^2}\right)$$

其中，$\mu_\theta(s)$ 和 $\sigma$ 是策略函数的参数，则策略梯度的计算公式为：

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)]$$

其中，$Q^{\pi_\theta}(s,a)$ 是在状态 $s$ 下选择行动 $a$ 的价值函数。这个期望可以通过采样得到。



确定性策略梯度（Deterministic Policy Gradient，DPG）算法是一种策略梯度算法，它适用于连续动作空间的问题，并且比基于高斯分布的策略函数更稳定。

DPG算法主要包括两个部分：一个是确定性策略函数，另一个是 Q 函数（状态-行动价值函数）。与基于高斯分布的策略函数不同，确定性策略函数直接输出确定性行动，而不是采样。

确定性策略函数可以表示为：

$$a = \mu(s;\theta_\mu)$$

其中，$s$ 是状态，$\mu$ 是确定性策略函数，$\theta_\mu$ 是策略函数的参数。

Q 函数用于估计给定状态和行动的价值，即 $Q(s,a)$。DPG算法使用 Q 函数来更新策略函数的参数。具体来说，可以通过最小化 Q 函数与目标 Q 函数之间的差异来更新策略函数的参数，即：

$$\theta_\mu \leftarrow \theta_\mu + \alpha \nabla_{\theta_\mu} Q(s,a|\theta_Q) \nabla_a \mu(s;\theta_\mu)|_{a=\mu(s)}$$

其中，$\alpha$ 是学习率，$\nabla_{\theta_\mu} Q$ 是 Q 函数相对于 $\theta_\mu$ 的梯度，$\nabla_a \mu(s;\theta_\mu)|_{a=\mu(s)}$ 是确定性策略函数相对于行动的梯度，$\theta_Q$ 是 Q 函数的参数。

DPG 算法在更新策略函数的过程中，使用了 Q 函数的信息来指导策略更新，从而提高了算法的效率和稳定性。





MADDPG（Multi-Agent Deep Deterministic Policy Gradient）算法是一种基于 actor-critic 框架的多智能体强化学习算法。以下是MADDPG算法的具体实现步骤：

1.  环境建模：将多智能体系统的环境建模成一个 Markov 博弈模型。环境的状态由所有智能体的状态组成，动作也由所有智能体的动作组成。奖励函数应该考虑到智能体之间的协作关系，促进整个团队的性能提升。
    
2.  神经网络架构：为每个智能体建立两个神经网络：一个是行动者（Actor）神经网络，用于决策智能体的动作；另一个是评论家（Critic）神经网络，用于评估智能体策略的好坏。这两个网络的参数分别记为 $\theta^{i}_{\text{A}}$ 和 $\theta^{i}_{\text{C}}$，其中 $i$ 表示第 $i$ 个智能体。
    
3.  经验回放：维护一个经验回放缓存区，用于存储智能体的经验。每个经验包括状态 $s$、动作 $a$、奖励 $r$、下一个状态 $s'$ 和结束标志 $done$。
    
4.  训练过程：对于每个时间步 $t$ 和每个智能体 $i$，执行以下步骤：
    
    a. 从行动者神经网络中获取动作 $a_{t}^{i} = \pi(s_{t}^{i};\theta_{\text{A}}^{i}) + \epsilon_{t}^{i}$，其中 $\pi$ 表示行动者策略函数，$\theta_{\text{A}}^{i}$ 表示行动者神经网络的参数，$\epsilon_{t}^{i}$ 表示动作噪声。
    
    b. 执行动作 $a_{t}^{i}$，观察环境的奖励 $r_{t}^{i}$ 和下一个状态 $s_{t+1}^{i}$，并将经验 $(s_{t}^{i}, a_{t}^{i}, r_{t}^{i}, s_{t+1}^{i}, done_{t}^{i})$ 存储到经验回放缓存区中。
    
    c. 从经验回放缓存区中随机抽取一批经验 $(s_{j}^{i}, a_{j}^{i}, r_{j}^{i}, s_{j+1}^{i}, done_{j}^{i})$，并计算目标动作 $a_{j+1}^{i} = \pi(s_{j+1}^{i};\theta_{\text{A}}^{i'})$，其中 $\theta_{\text{A}}^{i'}$ 是目标行动者网络的参数。