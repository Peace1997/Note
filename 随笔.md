

两篇论文：

《Self-Learning based Distributed Resilient Consensus for Multi-Agent Systems》
通过强化学习方法解决多智能体弹性一致性（Multi-agent Resilient Consensus (MARC)）问题，在故障节点存在的情况下，以分布式方式实现状态一致性。提出了两种解决方法


《Fairness based Multi-task Reward Allocation in Mobile Crowdsourcing System》
为解决奖励分配的公平性，提出了三种奖励分配算法，更好的实现公平性和收敛性。


**论文十问

Q1论文试图解决什么问题？

Q2这是否是一个新的问题？

Q3这篇文章要验证一个什么科学假设？

Q4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？

Q5论文中提到的解决方案之关键是什么？

Q6论文中的实验是如何设计的？

Q7用于定量评估的数据集是什么？代码有没有开源？

Q8论文中的实验及结果有没有很好地支持需要验证的科学假设？

Q9这篇论文到底有什么贡献？

Q10 下一步呢？有什么工作可以继续深入？


1. 复现上次清华大学团队提出的MAANS代码
	1. 【环境配置】；Habitat 环境安装：安装了一个habitat环境，需要和数据集进行适配，我现在的话先在云服务器上多试错几次，为日后在我们自己服务器上安装做准备。
	2. 【源码学习】
		- 作者提供的源码中，少了创建环境的模块，作者针对该问题没有进行回复。
		- MAANS 框架有三大模块，每一块展开后很复杂，我重点关注其中 MSP 决策模块，这个模块是基于 MARL、CNN、Transformer 进行的，CNN 之前有过一定的实践经验，但是对于 Transformer 不是很熟悉。这个 Transformer 也是我们这个框架的基础结构之一。重点对这一块进行学习。
	3. 【Transformer学习】看了不少Transformer的课程，我认为台大李宏毅老师的Transformer相关课程讲的最清楚，然后使用Pytorch对基础Transformer进行复现。




协作探索任务：基于一致性邻接权重分配的思想，控制每个智能体之间的通信信息的权重，也就是神经网络输入前，加入一个通信信息分配的过程。










每个时间步中，在目标决策层，每个智能体根据环境的观测信息，通过MARL算法决策自身要前往的目标点；然后将目标点输入目标导航层，智能体根据全局路径规划算法和局部路径规划算法，向目标点移动一段距离（运行时间为9s），更新每个智能体的状态，若未完成整体任务要求，则继续迭代，由目标决策层重新分配目标点，目标导航层进行路径规划，直至所有智能体无冲突的到达所有目标点。




>每个时间步中每个智能体根据自身环境的观测信息，首先通过MARL算法决策要前往的目标点，再根据全局路径规划算法和局部路径规划算法，引导智能体向目标点移动一段距离，更新每个智能体的观测信息并重新分配目标点，直至所有智能体无冲突的到达所有目标点。


  

  

ROS作为机器人软件开发和控制的平台，广泛应用于真实机器人和仿真环境中。Gazebo与ROS紧密集成，Gazebo内置多种物理引擎，支持多种机器人模型和传感器（激光雷达、相机、惯性测量单元）的导入，可以更好的模拟真实机器人的行为，包括机器人的动力学、运动控制和传感器数据等。相较于其他论文[8-11]使用的小型格点环境，Gazebo环境因其可定制性和真实性，可以灵活的搭建所需的各种场景，降低算法在真实环境的迁移难度。因此在本文中选用Gazebo物理仿真平台创建了训练和测试的仿真环境。

  

> Gazebo内置多种物理引擎，支持多种机器人模型和传感器（激光雷达、相机、惯性测量单元）的导入，与ROS紧密集成，便于将仿真环境的训练结果导入真实机器人中进行测试。


经典多智能体强化学习算法 MADDPG，


对于评审意见：
1. 在本文2.1 节分层控制结构中，对分层控制结构的目标决策层和目标导航层之间的结合、目标点移动距离、目标点重新分配做了更详细的介绍。
2. 在本文 3.1 节实验设置中，增加了对 Gazebo 环境的介绍，以及 Gazebo 相较于现有工作强化学习环境的优势。
3. 在本文3.2节训练结果分析与3.3节测试结果分析中，增加了经典多智能体强化学习算法MADDPG的训练、测试的对比实验。



