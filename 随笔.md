

两篇论文：

《Self-Learning based Distributed Resilient Consensus for Multi-Agent Systems》
通过强化学习方法解决多智能体弹性一致性（Multi-agent Resilient Consensus (MARC)）问题，在故障节点存在的情况下，以分布式方式实现状态一致性。提出了两种解决方法


《Fairness based Multi-task Reward Allocation in Mobile Crowdsourcing System》
为解决奖励分配的公平性，提出了三种奖励 分配算法，更好的实现公平性和收敛性。


**论文十问

Q1论文试图解决什么问题？

Q2这是否是一个新的问题？

Q3这篇文章要验证一个什么科学假设？

Q4有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？

Q5论文中提到的解决方案之关键是什么？

Q6论文中的实验是如何设计的？

Q7用于定量评估的数据集是什么？代码有没有开源？

Q8论文中的实验及结果有没有很好地支持需要验证的科学假设？

Q9这篇论文到底有什么贡献？

Q10 下一步呢？有什么工作可以继续深入？


1. 复现上次清华大学团队提出的MAANS代码
	1. 【环境配置】；Habitat 环境安装：安装了一个habitat环境，需要和数据集进行适配，我现在的话先在云服务器上多试错几次，为日后在我们自己服务器上安装做准备。
	2. 【源码学习】
		- 作者提供的源码中，少了创建环境的模块，作者针对该问题没有进行回复。
		- MAANS 框架有三大模块，每一块展开后很复杂，我重点关注其中 MSP 决策模块，这个模块是基于 MARL、CNN、Transformer 进行的，CNN 之前有过一定的实践经验，但是对于 Transformer 不是很熟悉。这个 Transformer 也是我们这个框架的基础结构之一。重点对这一块进行学习。
	3. 【Transformer学习】看了不少Transformer的课程，我认为台大李宏毅老师的Transformer相关课程讲的最清楚，然后使用Pytorch对基础Transformer进行复现。




协作探索任务：基于一致性邻接权重分配的思想，控制每个智能体之间的通信信息的权重，也就是神经网络输入前，加入一个通信信息分配的过程。










每个时间步中，在目标决策层，每个智能体根据环境的观测信息，通过MARL算法决策自身要前往的目标点；然后将目标点输入目标导航层，智能体根据全局路径规划算法和局部路径规划算法，向目标点移动一段距离（运行时间为9s），更新每个智能体的状态，若未完成整体任务要求，则继续迭代，由目标决策层重新分配目标点，目标导航层进行路径规划，直至所有智能体无冲突的到达所有目标点。




>每个时间步中每个智能体根据自身环境的观测信息，首先通过MARL算法决策要前往的目标点，再根据全局路径规划算法和局部路径规划算法，引导智能体向目标点移动一段距离，更新每个智能体的观测信息并重新分配目标点，直至所有智能体无冲突的到达所有目标点。


  

  

ROS作为机器人软件开发和控制的平台，广泛应用于真实机器人和仿真环境中。Gazebo与ROS紧密集成，Gazebo内置多种物理引擎，支持多种机器人模型和传感器（激光雷达、相机、惯性测量单元）的导入，可以更好的模拟真实机器人的行为，包括机器人的动力学、运动控制和传感器数据等。相较于其他论文[8-11]使用的小型格点环境，Gazebo环境因其可定制性和真实性，可以灵活的搭建所需的各种场景，降低算法在真实环境的迁移难度。因此在本文中选用Gazebo物理仿真平台创建了训练和测试的仿真环境。

  

> Gazebo内置多种物理引擎，支持多种机器人模型和传感器（激光雷达、相机、惯性测量单元）的导入，与ROS紧密集成，便于将仿真环境的训练结果导入真实机器人中进行测试。


经典多智能体强化学习算法 MADDPG，


对于评审意见：
1. 在本文2.1 节分层控制结构中，对分层控制结构的目标决策层和目标导航层之间的结合、目标点移动距离、目标点重新分配做了更详细的介绍。
2. 在本文 3.1 节实验设置中，增加了对 Gazebo 环境的介绍，以及 Gazebo 相较于现有工作强化学习环境的优势。
3. 在本文3.2节训练结果分析与3.3节测试结果分析中，增加了经典多智能体强化学习算法MADDPG的训练、测试的对比实验。



策略梯度算法的核心就是计算损失函数关于策略函数参数的梯度，这个梯度指导我们更新策略函数的参数。具体来说，对于离散动作空间的情况，策略梯度的计算公式为：

$$\nabla_\theta J(\theta) = \sum_{t=0}^T \nabla_\theta \log \pi_\theta (a_t|s_t) G_t$$

其中，$J(\theta)$ 是策略函数 $\pi_\theta$ 的目标函数，$G_t$ 是从时间步 $t$ 开始的累计奖励（可能是回报或价值函数），$s_t$ 是状态，$a_t$ 是在状态 $s_t$ 下选择的行动，$\theta$ 是策略函数的参数。

对于连续动作空间的情况，可以使用基于高斯分布的策略函数，并使用梯度下降或其变种算法来最大化目标函数。具体来说，如果策略函数是基于高斯分布的，即：

$$\pi_\theta(a|s) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(a-\mu_\theta(s))^2}{2\sigma^2}\right)$$

其中，$\mu_\theta(s)$ 和 $\sigma$ 是策略函数的参数，则策略梯度的计算公式为：

$$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)]$$

其中，$Q^{\pi_\theta}(s,a)$ 是在状态 $s$ 下选择行动 $a$ 的价值函数。这个期望可以通过采样得到。



确定性策略梯度（Deterministic Policy Gradient，DPG）算法是一种策略梯度算法，它适用于连续动作空间的问题，并且比基于高斯分布的策略函数更稳定。

DPG算法主要包括两个部分：一个是确定性策略函数，另一个是 Q 函数（状态-行动价值函数）。与基于高斯分布的策略函数不同，确定性策略函数直接输出确定性行动，而不是采样。

确定性策略函数可以表示为：

$$a = \mu(s;\theta_\mu)$$

其中，$s$ 是状态，$\mu$ 是确定性策略函数，$\theta_\mu$ 是策略函数的参数。

Q 函数用于估计给定状态和行动的价值，即 $Q(s,a)$。DPG算法使用 Q 函数来更新策略函数的参数。具体来说，可以通过最小化 Q 函数与目标 Q 函数之间的差异来更新策略函数的参数，即：

$$\theta_\mu \leftarrow \theta_\mu + \alpha \nabla_{\theta_\mu} Q(s,a|\theta_Q) \nabla_a \mu(s;\theta_\mu)|_{a=\mu(s)}$$

其中，$\alpha$ 是学习率，$\nabla_{\theta_\mu} Q$ 是 Q 函数相对于 $\theta_\mu$ 的梯度，$\nabla_a \mu(s;\theta_\mu)|_{a=\mu(s)}$ 是确定性策略函数相对于行动的梯度，$\theta_Q$ 是 Q 函数的参数。

DPG 算法在更新策略函数的过程中，使用了 Q 函数的信息来指导策略更新，从而提高了算法的效率和稳定性。





MADDPG（Multi-Agent Deep Deterministic Policy Gradient）算法是一种基于 actor-critic 框架的多智能体强化学习算法。以下是MADDPG算法的具体实现步骤：

1.  环境建模：将多智能体系统的环境建模成一个 Markov 博弈模型。环境的状态由所有智能体的状态组成，动作也由所有智能体的动作组成。奖励函数应该考虑到智能体之间的协作关系，促进整个团队的性能提升。
    
2.  神经网络架构：为每个智能体建立两个神经网络：一个是行动者（Actor）神经网络，用于决策智能体的动作；另一个是评论家（Critic）神经网络，用于评估智能体策略的好坏。这两个网络的参数分别记为 $\theta^{i}_{\text{A}}$ 和 $\theta^{i}_{\text{C}}$，其中 $i$ 表示第 $i$ 个智能体。
    
3.  经验回放：维护一个经验回放缓存区，用于存储智能体的经验。每个经验包括状态 $s$、动作 $a$、奖励 $r$、下一个状态 $s'$ 和结束标志 $done$。
    
4.  训练过程：对于每个时间步 $t$ 和每个智能体 $i$，执行以下步骤：
    
    a. 从行动者神经网络中获取动作 $a_{t}^{i} = \pi(s_{t}^{i};\theta_{\text{A}}^{i}) + \epsilon_{t}^{i}$，其中 $\pi$ 表示行动者策略函数，$\theta_{\text{A}}^{i}$ 表示行动者神经网络的参数，$\epsilon_{t}^{i}$ 表示动作噪声。
    
    b. 执行动作 $a_{t}^{i}$，观察环境的奖励 $r_{t}^{i}$ 和下一个状态 $s_{t+1}^{i}$，并将经验 $(s_{t}^{i}, a_{t}^{i}, r_{t}^{i}, s_{t+1}^{i}, done_{t}^{i})$ 存储到经验回放缓存区中。
    
    c. 从经验回放缓存区中随机抽取一批经验 $(s_{j}^{i}, a_{j}^{i}, r_{j}^{i}, s_{j+1}^{i}, done_{j}^{i})$，并计算目标动作 $a_{j+1}^{i} = \pi(s_{j+1}^{i};\theta_{\text{A}}^{i'})$，其中 $\theta_{\text{A}}^{i'}$ 是目标行动者网络的参数。



论文将

- 设置了 6 种不同的输入来测试对智能体的影响。每个智能体具有不同的观察空间来研究不同输入的影响
- III. C 设计了设计了密集（dense）奖励来加速 Agent 的训练，建议按照相似的意义分类规整
- 在公式（1）存在公式与解释不对应的问题，公式中并未存在 r_s^t、s_{ss}^t 这一奖励项。对于 r_{tc}这项奖励并未详细解释。
- IV. A 的实验，测试了不同的输入对实验结果的影响，但并未进一步阐述为什么不同的输入参数会对训练结果产生影响。 



This article proposes a reinforcement learning method that combines global path planning, waypoint generator, and local path planning for autonomous navigation problems. The feasibility of the algorithm is verified through multiple experiments. Overall, the algorithm is described clearly and has certain practical significance. Some other concerns are as follows.

1. In Section III. C (Reward Functions) , a dense reward is designed to accelerate the training of the agent, and it is recommended to organize it according to similar meanings. At the same time, there is an issue with formula (1) where the formula does not correspond to the explanation. The reward terms r_s^t and s_{ss}^t do not exist in the formula. The reward term r_{tc} is not explained in detail.
2. In Section 4.A (Training Performance), different inputs were tested to investigate their impact on the experimental results, but it was not further explained why different input parameters would have an effect on the training results.
3. There are two citation issues on page three.

## Transformer

Transformer 主要用于特征提取；重新组合各个输入向量，得到更完美的特征。
模型都是自回归的[10]，即在生成下一个符号时将先前生成的符号作为附加输入。

对于每一步解码，模型都是自回归的[10]，即在生成下一个符号时将先前生成的符号作为附加输入。

Token向量
q: 查询向量，用于查询与其他输入向量之间的关系
k：被查向量
v:特征代表
```
Tokens是自然语言处理中最基本的单位，它们是文本中的单个单词、标点符号、数字等元素。在机器学习和深度学习中，tokens被用于表示文本信息，并在各种应用中扮演着重要的角色。

在自然语言处理中，q、k、v是Transformer模型中用于实现自注意力机制的三个向量。

-   Q（Query）向量表示查询向量，是用来和K向量进行点积操作以得到注意力权重的向量。在编码器中，Query向量是来自上一层的输出向量。
-   K（Key）向量表示键向量，用于与Query向量计算注意力权重。在编码器中，Key向量是来自上一层的输出向量。
-   V（Value）向量表示值向量，用于与注意力权重相乘并求和得到上下文向量，最终用于生成下一层的输出。在编码器中，Value向量也是来自上一层的输出向量。

使用Q、K、V向量的自注意力机制可以对输入的每个元素进行加权聚合，并产生对应的输出表示。这种机制可以有效地捕获文本中的上下文关系，进而提高各种自然语言处理任务的性能，例如语言建模、翻译和文本分类等。
```


Embedding：词转向量


位置编码问题：？？

在Transformer中可以支持行维度不同，但是列维度要相同，在MARL中，也就是保证每个智能体获取周围的智能体信息的数量是可以不同的，但是要保证每个智能体自身获取的状态信息是相同，




各位领导，各位同事，大家晚上好。
我是协同技术部群体智能协同控制研发工程师马佩鑫，入职半年以来，在公司和技术负责人的领导下，开展了群体控制组的相关工作，我重点负责智能编队控制算法研发。结合2023 年度述职报告，我将从工作完成情况、工作亮点、下一步工作安排和工作总结四个部分进行详细介绍。

首先，在工作完成方面：
- 我完成了基于强化学习的编队控制算法研发。在多飞行器编队控制任务中，重点需要解决编队形成和编队保持问题，我设计了基于 PPO 的多飞行器编队控制算法，该算法采用领航者-跟随者的编队控制结构，基于深度强化学习算法进行控制参数在线调整，生成飞行器的速度控制指令和滚转角控制指令。经蒙特卡洛仿真实验验证，在人字形、一字形、菱形和梯形四种编队场景中，算法各项指标均满足比测合同的要求。
- 第2项工作是保障三部完成二次比测及验收工作。依据比测要求，完成数字仿真样机在 Linux 端的部署，基于 gRPC 通信协议开发基于强化学习的编队控制训练子系统；基于 Libtorch 库开发同构和异构编队控制算法验证子系统，专家可以通过文件的方式自行修改编队构型、飞行器数量和起始坐标等参数。
- 第3项工作是配合公司完成智能控制项目结题。根据《智能控制算法技术开发合同》的内容，完成群体智能控制方向中多飞行器编队控制任务和飞行器多约束轨迹优化算法的开发并形成相应的报告。
- 第4项工作是完成成果转化与高校对接。其中，我完成了《一种混合有模型和无模型强化学习的飞行器编队控制方法》专利的撰写；配合完成强化学习知识库的构建和新人培训材料的编写。协助哈工大和西工大完成算法集成和可视化工作。

以上详细工作任务和交付成果如表所示：
- 针对智能编队控制算法开发了三套子系统：智能编队控制训练子系统、同构编队控制验证子系统和异构编队控制验证子系统。
- 根据比测的相关要求形成算法设计报告、测试报告等材料
- 根据公司的智能控制领域形成了相应的结题报告、算法说明报告等。
- 根据当前的算法开发进度和公司的要求形成的对应的成果转化，包括一篇专利和一些总结报告。
以上，是我这半年的工作完成情况。

第二部分，我将介绍接下来的这半年的工作亮点：
- 第一点，自研智能编队控制算法，顺利支撑三部项目比测和验收。编队控制的跟踪误差是30m，远小于合同指标要求中的。该算法可以扩展到任意数量的飞行器集群，具备快速形成编队构型和适应不同场景的能力，算法各项指标均满足合同的要求。同时，该算法也在二次比测和项目验收中进行现场展示，针对专家提出的变换编队构型和编队成员个数等要求也顺利满足。
- 第二点，积极配合公司进行结题验收，完成知识产权成果转化，形成一篇专利。根据合同的内容，开发飞行器编队控制和轨迹优化算法，完成结题材料的编写
- 第三点，保持向上的工作态度，不断学习和完善自我。每日撰写日报，对当日工作内容、研究进度、存在问题以及每日所学进行总结；能够准确理解、吸收技术负责人的任务需求，妥善完成公司和技术负责人安排的各项任务


第三部分我将介绍接下来的工作安排
1. 首先继续配合三部技术负责人完成第三次比测的相关任务。同时也配合16s进行智能编队控制算法的测试。目前已经完成了编队算法训练端和验证端的移植和初步验证测试，后续要更进行自动化测试（目前也在按董老师的要求进行开发）
2. 进一步优化智能编队控制算法，提高算法在多种场景的适应性
3. 积极配合技术负责人、组长以及副组长完成各项工作任务，及时调研当前强化学习、大模型以及飞行器智能控制的最新研究进展

最后，进行一个简要的总结。

自入职以来，积极遵守公司的规章制度，严格恪守保密要求，认真执行各项保密规定和措施。在日常工作中保质保量地完成技术负责人、组长和副组长分配的工作内容。坚持在深蓝学院等课程平台学习C++、强化学习、规划控制的相关内容。接下来，提高代码质量和可维护性，及时调研当前智能控制、强化学习的最新研究进展，进一步提高自己解决问题的能力。

以上就是我2023 年度述职汇报的内容，请各位领导批评指正



