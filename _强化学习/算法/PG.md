# 简述

***1. 策略***
在策略梯度方法中，策略通常表示为一个带参数的概率分布，即$\pi_θ(a∣s)$ 。

***2. 策略目标***
策略梯度方法的目标是最大化累积奖励的期望值，即： $J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}$

***3. 策略梯度***
为了优化策略，我们需要计算目标函数 $(\theta)$ 对策略参数 $\theta$ 的梯度： $\nabla_\theta J(\theta)$
根据**策略梯度定理**，这个梯度可以表示为：
$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(a|s) R(\tau) \right]$$
可以通过以下步骤更新策略参数：
- 采样：根据当前策略$\pi_{\theta}$, 采样一条轨迹$\tau$
- 计算回报：对于每个轨迹计算其累计奖励$R(\tau)$
- 梯度更新：根据策略梯度，使用回报$R(\tau)$作为权重，更新策略参数

> 回报作为权重的关键原因在于：每个状态下采取的动作的好坏需要通过其对**最终累积奖励**的影响来评估。

***4. 策略梯度优缺点***

优点：
- **策略的直接优化：** 相较于值函数的方法，策略梯度方法可以直接优化策略，不需要依赖于间接推导的动作价值函数。
- **处理连续运动空间：** 基于值函数的 DQN 算法仅适用于离散动作空间，策略梯度算法可以应用于连续动作空间。
- 

缺点：
- **贡献分配问题**：对于每个状态-动作对使用的是"全局奖励"作为它的权重，即以一条轨迹作为更新单位。这样的累积奖励分配方法，无法很好的体现出当前状态 -动作对对整个轨迹的贡献。
- **奖励总为正值的问题**：按照正奖励增加动作概率，负奖励减少动作概率的原则，如果所有动作奖励都为正，则按理所有的动作概率都该增加，这显然是无法完成。
- 