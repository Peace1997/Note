Online RL with Offline Data

# Introduction

RLPD（Reinforcement Learning with Prior Data) 是一种利用离线数据来加速强化学习在线学习的算法。其总体架构主要包括两个模块：数据交互模块和模型训练模块，以下也是这篇文章的主要创新点。

**1- 数据交互：** RLPD 算法采用一种名为“**对称采样**”的机制来合并在线数据和离线数据。
**2- 模型训练：** 采用 off-policy 无模型的强化学习算法训练（SAC），并采用以下方法提升稳定性：
- **Layer Normalization Mitigates Catastrophic Overestimation**：缓解过度估计的问题
- **Sample Efficient RL**：采用大规模集成，用于提高离线数据的利用率，特别是在稀疏奖励任务中。
- **Per-Environment Design Choices**： 针对不同环境，采用不同的网络架构、更新规则（是否使用 Clipped Double Q-Learning）和熵正则化。

# Overview

## 1. Symmetric sampling
传统的在线强化学习算法通常只利用在线收集的经验来训练策略。 RLPD 提出了一种新的采样策略，称为“**对称采样**”，它从**在线经验回放缓冲区和离线数据集中平均采样**一批数据用于训练。

这种简单的采样机制有效地将先验知识整合到在线学习过程中，这与以前依赖于离线预训练或显式模仿项来利用先验数据的 RL 方法相比，**无需进行复杂的超参数调整**。

## 2. Layer Normalization

将层归一化 (Layer Normalization) 应用于 Critic 网络可以有效地缓解非策略强化学习中普遍存在的灾难性过度估计问题。


>**灾难性过度估计：** 在使用函数逼近的非策略强化学习算法中，由于对状态-动作值函数 (Q 函数) 的过度估计，导致学习过程不稳定，甚至可能完全发散的现象。

## 3. Sample Efficient RL
RLPD 通过**在线贝尔曼备份**（Bellman backups）隐式地整合离线数据。这意味着 RLPD 不是直接使用离线数据进行预训练，而是通过在线学习过程中对离线数据的转换进行学习，从而隐式地学习利用离线数据的价值。提高了计算效率，避免离线预训练可能带来的超参数调整问题。

由于 RLPD 隐式的整合离线数据，这就需要有效地利用离线数据，那么在执行贝尔曼备份的过程中必须尽可能地高效，即需要**提高样本利用率**。如果备份过程不够高效，RLPD 就无法充分发挥离线数据的优势。

***解决样本效率问题的方法***：
- **增加每次环境步骤的更新次数**：通过增加 UTD（更新-数据比率，UTD），可以使离线数据更快地“备份”，即更快地被学习。
- **随机集成蒸馏** (Random Ensemble Distillation)：，为了避免高 UTD 导致的过拟合，RLPD 采用随机集成蒸馏的方法。该方法通过使用多个评论家网络，这种方法允许**每个评论家从不同的角度学习**，减少了过拟合的风险，并进一步提高了样本效率。
- **图像增强**: 对于视觉任务，为了避免从图像进行 TD 学习时出现的过拟合问题，RLPD 进一步引入了**随机移位增强**（random shift augmentations）方法

>**更新-数据比率（Update-to-Data Ratio, 简称 UTD）**：是强化学习中衡量**算法利用采样数据进行学习的频率**的一种指标。它描述了在每次与环境交互采集新数据后，算法在同一批数据上执行更新的次数。

> **随机移位增强 (random shift augmentations)：** 是一种在深度强化学习中用于图像数据处理的增强技术，这种方法通过对输入图像进行随机的像素位移来人为地增加训练数据的多样性。

## 4. Per-Environment Design Choices
由于 RL 算法需要针对不同的环境特性调整算法的相关设计。例如有些环境需要更深的网络，有些环境需要不同的正则化方法。

RLPD 中的 Per-Environment Design Choices：
- **Subset of Critics**：在实践中，可以首先尝试减少评论家网络的目标数量，例如从 2 个（CDQ（Clipped Double Q-learning））减少到 1 个。
- **Entropy Regularization**： 最大熵强化学习 (MaxEnt RL) 方法在奖励稀疏且需要探索的环境中表现出色。 然而，在某些环境中，移除熵项可能能够提高性能
- **多层感知机 (MLP) 结构**：网络架构对深度强化学习的性能有显著影响。RLPD 发现，在某些环境中，使用更深层的 MLP 网络（如三层）可能比使用较浅的网络（如两层）更有效。

# Contribution

RLPD  （基于先验数据的在线强化学习）的主要创新点和贡献在于其高效**利用离线数据来加速在线强化学习**，同时保持了算法的简洁性和计算效率，它通过对现有离策略强化学习算法进行最小但关键的修改，这些创新点包括：
1. ***高效利用离线数据***：
	- 对于 Off-Policy RL 算法有效地**利用离线数据可以加速在线学习**的速度，不依赖于离线数据的预训练，通过在线学习隐式的整合离线数据，简化了算法的实现并提高了计算效率。
	- 该方法可以利用**各种类型离线数据集**，无论是少量专家演示还是大量次优轨迹数据。
2. ***基于 Off-Policy RL 的适配选择***（不同的数据使用不同的方法）
	- **对称采样**（Symmetric Samping）：在训练过程中，RLPD 均匀的在在线数据和离线数据中采样，从而**平衡地利用**这两种数据来源。
	- **层归一化**（LayerNorm）：在 Critic 网络的更新中使用层归一化，避免值函数的过度估计，这在数据有限或分布狭窄的情况下。
	- **大集成**（Large Ensembles）：由于 RLPD 隐式的整合离线数据，这就需要有效地利用离线数据，提升样本利用率，可以采用**随机集成蒸馏**的方法，利用多个 Critic 网络集成来提高样本效率，允许**每个评论家从不同的角度学习**，减少了过拟合的风险。尤其是在稀疏奖励任务中。
	-  **随机移位增强**（Random shift augmentations）: 为了解决在从像素数据进行时序差分学习时出现的过拟合问题。
