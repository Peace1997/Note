
# 一、简述

***运动控制的难点：***
双足机器人的控制具有很高的技术难度，尤其是步态控制和平衡问题。因为机器人在移动过程中，外力方面只受到重力和地面的作用力，而重力和地面作用力不能直接控制，只能转而**控制机器人关节的驱动力来控制机器人的行走**，这样给双足机器人的控制增加了很大难度。

## 1. 动力学模型

机器人的结构比较复杂，因此，在研究步态算法时，常常对机器人模型做简化处理，最初具有代表性的简化模型就是倒立摆模型（ Inverted Pendulum Model，IPM）。


**倒立摆（Linear Inverted Pendulum Model，LIPM）** 和**弹簧负载倒立摆（SLIP）**。但是这样的模型过于简单，一来没考虑腿的质量，二来将身体简化为质点，也没考虑身体姿态的影响，后来发展出来了 centroidal dynamics，以及更复杂得 full-body dynamics，由此也衍生出了不同的控制方法。

### a. 倒立摆模型
倒立摆模型是用于人形机器人**平衡控制和质心运动分析**的最基本模型。将机器人等效为一个摆动质量和固定支撑点构成的系统，适合大量理论与控制方法分析。





# 二、控制方法

无论是 RL 方法还是基于模型（MBC）的方法，控制的本质就是面对**动态系统**去设计**策略**以满足一些**限制**。

# MBC && RL

## 方法对比
***算力运用：*** 
- MBC： MBC 的策略优化是在线，在线计算机器人要执行哪些行为，它没有离线计算的能力，即**在线优化机器人的行为**；
- RL ：RL 的方法是基于轨迹数据去学习策略，有充足的时间进行策略优化（离线更新），从而找到最优策略。

***状态估计***：
- MBC：与控制一样重要，得需要知道机器人的状态，清楚它此刻所在的位置
- RL：再一定程度上可以规避掉这个问题，
	1. 利用仿真器提供的环境信息，它可以同时学习策略和状态估计器（或通过辅助隐式的学习状态估计）；
	2. 利用 Teacher-Student 架构，通过特权信息，教师网络制导学生网络进行学习。

***RL 优势***：
1. **离线优化**：策略的优化可以在离线阶段进行，策略学习好后固定神经网络，可以直接进行推理，避免在线优化。
2.  **绕开状态估计**：利用仿真器的环境信息，通过特权信息、Teacher-Student 架构，可以绕开传统控制中必须要解决的状态估计问题。

***MBC 优势***：
1. **在线优化**： RL 中训出策略后，这个策略网络就固定了，需要根据 Sim 2 Real 的效果重新进行训练调节，如果仿真和真机差距过大，误差就会更大。MBC 可以一边让机器人运动，一边进行计算。
2. **结构性优势：*** 结构性意味着控制方法具有清晰、可解析的数学框架或约束条件，这意味着在给定有限的采样数据或仿真次数的条件下，控制方法能够更快、更准确地收敛到符合期望的行为，**采样效率更高**。 RL 算法大都采用 PPO，因为是 on-policy，因此它的采样效率就可以不用在意，但可能会收敛到我们不想要的状态；


***MBC 结合 RL***
Control 的安全性融入 RL 中，或者借助 Control 良好的结构使强化学习变得更具稳健性。
限制当前 RL 算法的一个点就是仿真器（Sim 2 Real Gap），我们很难去创造一个完美的仿真器去解决所有的问题，所以一定要

 **CAJun**：CAJun 由高层次中心化策略和低层次腿控制器组成。在高层，由 RL 策略输出基体速度、步态周期（gait timing），摆动腿的位置（swing foot position）；在低层，MBC 基于步态周期去跟踪摆动腿的位置指令和基体速度指令。 

> CAJun: Continuous Adaptive Jumping using a Learned Centroidal Controller

**Real 2 Sim**： 不在固定不变的仿真器里学习，而是到现实中获取数据来改进仿真器，之后再基于改进后的仿真器进行学习。

**世界模型+MPC**：先学习一个世界模型（World model），然后再利用 MPC 去进行控制操作。即在现实环境里把动力学模型学习出来，之后再运用基于采样的模型预测控制（Sampling-based MPC）去开展后续的 Control

>DIAL- MPC: https://github. Com/LeCAR-Lab/dial-mpc. Git


## MBC 方法
在对物理规律进行采样时，其实就是在做基于模型的控制。

MPC：

我们运用 MPC 时，往往会对模型进行简化，也就是处理**简化模型的长时域最优化问题**。在这个过程中，需要进行建模，比如先将其简化成**单刚体模型**，复杂一点的话就变成**机器人模型**。在此基础上，通过预测来保障机器人**状态轨迹的可预测性**，进而确保机器人的稳定性。但做完简化模型那一步后，大家发现存在问题，因为简化后的模型很难真实反映机器人全身动力学的特点。

WBC：

研究进入了第二阶段，也就是处理复杂模型的短时域最优化问题，就是所谓的全身运动控制（WBC）。在这个阶段，要建立机器人的**全身动力学模型**，然后计算出当前的最优控制以保证实时性，用全身运动控制（WBC）来弥补模型预测控制（MPC）因简化模型而产生的问题。


# Sim 2 Real

之所以存在 Reality Gap 是因为我们学习的模型的环境与实际环境是存在一定差距的，那么在一个存在未知信息的环境，如何减少这个差距呢，最通用、最具扩展性的方法就是：模型与真实环境进行交互，尽可能的把未知信息获取回来，然后进行优化。

如果要尽可能提高机器人的可靠性，就需要实现与环境的交互，环境交互不能仅仅通过一种被动的数据学习（收集一些离线数据），那么要实现与环境的交互，就必须得用强化学习。



# Manipulation && Locomotion

## 挑战 

***Locomotion***
对于机器人的 Locomotion，主要的挑战在于**机器人本体的不确定**，这种不确定性相对有限，例如机器人可能踩到石子，或者路面出现打滑等情况。从机器人本体的角度来看，这些不确定性的影响是可以量化的。在控制领域，我们有一整套数学工具来应对这些问题，其中之一就是**鲁棒控制**（Robust control）。鲁棒控制的核心思想是对不确定性进行定量建模，进而确定**最坏情况的界限**（Worst-case bound）。只要实际的不确定性在这个界限范围内，设计出的控制策略就是有效的。


***Manipulation***
Manipulation 的主要挑战来自外部世界的不确定性，而不是机器人本体的不确定性。机器人本体是一个经过精心设计的确定性系统，但**外部世界的复杂性**是无限的（Unbounded complexity）。这些不确定性可能来自视觉、物理交互、触觉感知等多个方面，而对这些因素进行全面、精确的建模几乎是不可能的。尽管我们可以对其中的某些部分进行建模，取得一些成果，但始终无法完全掌控这些外部复杂性。这正是 Manipulation 问题需要强化学习（RL）的原因所在。RL 通过与外界的交互，能够更好地适应外部环境中的不确定性。

从本质上来说，Manipulation 和 Locomotion 是两类截然不同的问题：Locomotion 可以通过控制理论中现有的工具（如鲁棒控制）较好地解决，而 Manipulation 则需要通过 RL 等数据驱动的方式，去应对外部世界的复杂性和不确定性。


## Manipulation

***技术路线***
- 模仿学习 + 真机：以扩散策略（Diffusion policy）或者其他以 Aloha 为代表的模仿学习路线，能产生一系列比较好用的策略（Policy）。就是先收集大量的演示数据（Demonstration），然后采用行为克隆的方式。目前较少采用离线强化学习的路线。
- RL+真机：Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning
- RL + 仿真： 先预训练，在做微调
- 仿真优化：添加一些可微的元素进去，例如通过可微分模拟的方式来构建虚拟环境的世界模型。诸如粒子动力学（Particle Dynamics）之类。