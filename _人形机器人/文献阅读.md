
# Humanoid-Gym: 人形机器人零样本迁移
#zero-shot #Nvidia-Isaac-Gym  
Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim 2 Real Transfer - 基于强化学习的人形机器人零样本迁移

2024年3月5日，星动纪元联合清华大学、上海期智研究院开源了人形机器人强化学习训练框架Humanoid-Gym。

***论文亮点：***
- 开源了人形机器人端到端强化学习训练框架：Humanoid-Gym
- 在 Nvidia Isaac Gym 环境设计不同地形并增加动态随机域
- 对 Mujoco 环境进行精细化校准使其接近真实环境




# Concurrent Training ：并行训练控制策略和状态估计器
**Concurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion —— 为动态和稳健的腿部运动同时训练控制策略和状态估计器**

#sim2real #ppo


***主要思想：***
该框架包含两个网络结构：策略网络结构和状态估计网络。其中，策略网络结构输出关节期望位置；状态估计网络用于输出机器人状态信息。
该在强化学习 AC 框架基础上，增加**历史状态信息**，通过一个状态估计网络对机器人的线速度、脚的高度和接触概率进行估计。并将状态估计信息输入到控制策略产生最后的动作，该状态估计器与控制策略同时进行训练。

![[Pasted image 20240419171922.png]]

***论文亮点***
由于本体的状态信息对机器人运动控制是计较重要了，为实现 sim2real，增加历史状态信息和状态估计网络，训练完成后保存状态估计网络模型和策略网络模型。经验证，该方法可以有效实现 sim2real

***其他技巧***
- 动态随机化：动态随机化对于 sim 2 real 也是有很大帮助的，其中包括
	- Observation noise：观测噪声是在训练过程中添加的。例如，在真实机器人上，关节速度的测量来自于关节位置的数值微分，会导致关节速度观测到误差
	- Motor frictions: 为不同部位的执行器增加不同的噪声
	- PD controller gains: 我们分别为位置增益 P 和速度增益 D 添加均匀噪声，以减轻电机摩擦和阻尼的影响。
	- Foot positions and collision geometry：足部位置观测和碰撞几何形状随机化，可以减少测量误差和橡胶脚变形的影响。
	- Ground friction：地面摩擦力随机化，可以使机器人在低摩擦力的湿滑路面和高摩擦力的沥青路面进行运行。




# NMPC-DCBFRL： 无模型的控制与有模型安全性导航
**Bridging Model-based Safety and Model-free  Reinforcement Learning through System  Identification of Low Dimensional Linear Models -- 通过低维线性模型的系统辨识将基于模型的安全性和无模型强化学习相结合**


 #System-Identification  #model-base #model-free

***主要思想：***
这篇论文通过系统辨识的方法对强化学习控制的动态非线性的闭环系统建模为低维线性模型，并在该线性模型基础上应用基于模型的方法（NMPC-DCBF）进行局部规划，确保无模型强化学习运动控制系统的安全性。

![[Pasted image 20240814113656.png|425]]
![[Pasted image 20240814143226.png|450]]

***论文亮点：***
- 提供了一种有模型算法和无模型强化学习算法相结合的方式
- 将整个无模型强化学习算法闭环系统通过系统辨识建模为一个线性模型，有助于上层基于模型的算法进行安全性规划

***评价：***
- 这个线性模型的是需要有条件的，
	1. 需要有一个较好的 RL 策略
	2. 步态频率（输入频率）不能超过某个阈值（文中为：0.6hz），当步态频率超过0.6 Hz 时,Cassie 机器人的系统就会表现出明显的非线性特性。



# AMP：加入参考运动步态进行对抗性训练

***Abstract***：
论文提出一种对抗性运动先验（Adversarial Motion Priors，AMP）的框架，指导物理模拟角色的应该执行怎样的任务，以及以什么样的运动风格执行该任务。**AMP 算法能够在完成高级任务目标的同时，模仿数据集中的运动风格**。该框架利用**对抗性模仿学习**（Adversarial Imitation Learning.），通过无结构的运动数据样本训练一个对抗性判别器，作为运动风格的先验，指导**强化学习算法**训练物理模拟角色完成各种任务。
![[Pasted image 20241130114644.png|500]]

***Contribution***

 **1. 对抗性运动先验架构**：该方法将目标条件强化学习与对抗运动先验相结合，使角色能够从大型非结构化运动数据集中学习，鼓励角色在执行高级目标任务（期望速度、期望目标点）时采用类似于数据集中运动的行为。
> 该框架不需要像传统的动作捕捉技术那样手动设计跟踪目标或进行基于相位的运动同步

**2. 稳定对抗性训练的技巧**
	- **从观察中模仿**（Imitation from Observations）： 将 GAIL 扩展到只有状态可观察的设置，允许使用仅包含状态信息的运动片段进行训练，无需提供专家动作。
	- **最小二乘判别器**（Least-Squares Discriminator）： 采用最小二乘 GAN（LSGAN）的损失函数，以解决标准 GAN 目标中梯度消失的问题，从而提高训练的稳定性和结果的质量 。
	- **判别器观测**（Discriminator Observations）： 精心设计判别器使用的特征集，以提供对确定给定运动特征至关重要的信息，从而为策略提供有效的反馈
	- **梯度惩罚**（Gradient Penalty）： 使用梯度惩罚来减轻对抗性训练过程中的不稳定性，从而提高训练的稳定性和性能。

______
***Overview of AMP***
AMP 算法通过结合**目标条件强化学习**（Goal-Conditioned Reinforcement Learning ）和**生成对抗模仿学习** （Generative Adversarial Imitation Learning） 的方法，用于从大型非结构化运动数据集中学习运动先验，并控制角色的行为风格。

- **判别器**：用于预测输入的状态转换是来自真实运动数据集的 "真实" 样本，还是由角色策略生成的 "虚假" 样本。
	- 输入：是连续的状态转换 ($s_t, s_{t+1}$)，描述了角色在两个连续时间步长的运动。
	- 输出：策略网络生成的数据与真实数据集的数据的相似性评分。

- **生成器**：由强化学习算法构成，结合了任务奖励和风格奖励，使用近端策略优化 (PPO) 算法进行训练。

- **奖励函数**：任务奖励 $r^G$ 和风格奖励 $r^S$ 的线性组合
	- 任务奖励$r^G$ ： 特定于任务，用于定义角色应该满足的高级目标
	- 风格奖励$r^S$：由判别器提供，用于鼓励角色产生类似于数据集中运动的运动
$$
r\left(\mathrm{~s}_t, \mathrm{a}_t, \mathrm{~s}_{t+1}, \mathrm{~g}\right)=w^G r^G\left(\mathrm{~s}_t, \mathrm{a}_t, \mathrm{~s}_t, \mathrm{~g}\right)+w^S r^S\left(\mathrm{~s}_t, \mathrm{~s}_{t+1}\right) .
$$

_________

***1. Imitation from Observations***
GAIL 通常需要状态-动作对作为输入，由于采集的运动片段数据集并未提供状态迁移的动作，因此 AMP 只需要状态转换 $(s_t, s_{t+1})$ 作为输入。这使得 AMP 可以直接从运动片段数据中学习，而无需访问专家的动作信息。

$$
\underset{D}{\arg \min }-\mathbb{E}_{d^{\mathcal{M}}\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\log \left(D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)\right)\right]-\mathbb{E}_{d^\pi\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\log \left(1-D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)\right)\right] .
$$

***2. Least-Squares Discriminator***
在标准 GAN 中常采用 Sigmoid 交叉熵作为损失函数，随着 Sigmoid 函数饱和，可能会出现梯度消失的情况。因此，在 AMP 采用**最小二乘 GAN (LSGAN) 的损失函数**，
$$
\underset{D}{\arg \min } \mathbb{E}_{d^{\mathcal{M}}\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\left(D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)-1\right)^2\right]+\mathbb{E}_{d^\pi\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\left(D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)+1\right)^2\right] .
$$

判别器网络的评分不是 1 和-1 的离散值的，而是在 -1 到 1 之间的**连续值**。对于风格奖励的设置，设定为 \[0,1]。
$$
r^S\left(\mathrm{~s}_t, \mathrm{~s}_{t+1}\right)=\max \left[\begin{array}{ll}
0, & \left.1-0.25\left(D\left(\mathrm{~s}_t, \mathrm{~s}_{t+1}\right)-1\right)^2\right] .
\end{array}\right.
$$

Sigmoid 交叉熵：
$$
\underset{D}{\arg \min }-\mathbb{E}_{d^{\mathcal{M}}\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\log \left(D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)\right)\right]-\mathbb{E}_{d^\pi\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\log \left(1-D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)\right)\right] .
$$
详细 GAN 描述： [[GAN]]


***3. Discriminator Observations***
  
AMP 使用观察映射 $\phi(s)$ 从状态中提取与确定运动特征相关的特征，然后再将这些特征输入到判别器中，判别器的输入为：
**这些特征包括：**
- 基体的线速度和角速度
- 关节位置
- 关节速度
- 末端执行器（例如手和脚）的位置（笛卡尔空间）

对观察数据进行映射，可以提取出通用性运动特征，然后将这些特征输入到判别器中，这样做的好处是：通过观测数据的特征提取，判别器可以适配不同的问题。


***4. Gradient Penalty***

为解决生成器出现的**过度拟合和模式崩溃**的问题，该文提出了**梯度惩罚**的方法，该惩罚项与真实数据流形上**判别器输出的梯度范数的平方成正比**。这样做可以避免判别器过度强大，导致其对生成器生成的样本过于敏感，
$$\begin{aligned}
\arg\operatorname*{min}_{D} & \mathbb{E}_{d^{M}(\mathrm{s,s^{\prime}})}\left[\left(D(\Phi(\mathrm{s}),\Phi(\mathrm{s^{\prime}}))-1\right)^{2}\right] \\
 & +\mathbb{E}_{d^{\pi}(s,s^{\prime})}\left[\left(D\left(\Phi(s),\Phi(s^{\prime})\right)+1\right)^{2}\right] \\
 & +\frac{w^{\mathrm{gp}}}{2}\mathbb{E}_{d^{M}(\mathrm{s,s^{\prime}})}\left[\left\|\nabla_{\phi}D(\phi)\right|_{\phi=(\Phi(\mathrm{s}),\Phi(\mathrm{s^{\prime}}))}\right\|^{2}
\end{aligned}$$
- $\frac{w^{\mathrm{gp}}}{2}$为人为设定的系数


***AMP & GAIL***

**相同点：**

- **两者都使用对抗性判别器**： AMP 和 GAIL 都使用一个对抗性判别器来区分专家演示的行为和策略生成的行为。判别器充当奖励函数，鼓励策略生成与专家演示相似的行为。
- **两者都使用强化学习**： AMP 和 GAIL 都使用强化学习算法（例如 PPO）来训练策略，以最大化从判别器获得的奖励。

**不同点：**

- **输入数据**： GAIL 通常需要状态-动作对作为输入，而 AMP 只需要状态转换 $(s_t, s_{t+1}$) 作为输入。 这使得 AMP 可以直接从运动片段数据中学习，而无需访问专家的动作信息。
- **目标函数**： GAIL 最初的目标是最小化策略生成的数据分布与专家演示的数据分布之间的 Jensen-Shannon 散度。 AMP 则采用最小二乘 GAN (LSGAN) 的损失函数，旨在最小化 Pearson χ2 散度.
- **梯度惩罚**： AMP 使用梯度惩罚来增强训练稳定性，而 GAIL 并没有明确要求使用梯度惩罚。梯度惩罚通过惩罚数据流形上的非零梯度，防止判别器过度拟合，并使策略能够更有效地探索状态空间。


# ASE: 












# VMP：通过自监督技术学习潜在运动特征

VMP 是一种基于学习的运动跟踪框架，智能体在按照运动参考轨迹进行运动的同时遵守物理定律。VMP 算法将运动编码和策略训练分离成两个独立的阶段。

![[Pasted image 20241205135745.png]]

***Contribution***
- **生成潜在的时空特征的编码序列**：与单一模仿对应帧的行为/状态相比，VMP 算法通过 VAE 对运动片段编码成一个潜在编码序列，每个潜在钱吗对应运动片段中的一个帧。


***运动编码***
通过变分自编码器（VAE）来学习一个潜在的运动空间。

输入：运动切片数据库中的随机采样的运动窗口（连续运动帧）
输出：潜在运动空间$z_t$



***策略训练***

- 输入：VAE 重构的潜在运动空间$z_t$、运动窗口的中心帧$m_t$
- 输出：运动窗口中心帧应该采取的动作$a_t$










# DreamWaQ: 仅依靠本体感知学习地形和机身状态

**Abstract**：
DreamWaQ（Dream Walking for Quadrupedal Robots）是一种四足机器人运动框架。它采用**非对称 Actor-Critic** 架构来学习隐式地学习地形的信息，通过**上下文辅助估计器网络** **CENet (context-aided estimator network)** 来估计机器人的身体状态。
![[Pasted image 20241030182518.png|500]]

**Contribution：**
- **非对称 Actor-Critic**：不需要外部传感器感知环境信息，利用架构，使机器人能够仅通过本体感知推断地形属性，如高度图、摩擦力、恢复系数和障碍物。
- **上下文辅助估计器网络（Context-Aided Estimator Network）**：联合学习估计机器人状态和推断环境的潜在表示，提高状态估计的准确性。
- **自适应引导策略（AdaBoot）**：根据训练过程中奖励的变异系数 (CV) 来自适应地调整引导的概率。在 AdaBoot 中，引导指的是使用 CENet 估计的身体状态和环境上下文信息，而不是使用模拟器提供的真实值。
> 引导是指使用估计值代替真实值来训练策略网络

-------------
**1. 非对称 Actor-Critic**

策略网络 (Actor) 接收时间序列的部分观测值 $o^H_t$ 作为输入，而价值网络 (Critic) 则接收完整的状态信息 $s_t$ 作为输入。2这种非对称的输入信息使得策略网络能够在训练过程中隐式地学习推断环境信息，从而在没有外部传感器的情况下适应各种地形。

----------------
**2. 上下文辅助估计器网络 CENet**

CENet 由一个单编码器和多头解码器架构组成。
- **编码器**： 将时间序列的观测数据$O^H_t$ 编码为机器速度速度指令和上下文向量$Z_t$。
- **多头解码器**： 
	- 估计机体线速度$V_t$
	- 重建下一时间步的$Z_t$

**网络结构：**
- **共享编码器**： 身体速度估计模型和自动编码器模型共享一个统一的编码器，简化了网络架构，并在推理过程中同步运行。
- **自动编码器模型**：使用 $\beta-\text{VAE}$ 作为自动编码器架构


**优势**：
- 联合学习正向和反向动力学：
	- 估计机器人的身体线速度 $v_t$，这可以被视为学习机器人的**正向动力学**，即根据当前状态和控制输入预测未来的状态。
![[Pasted image 20241101114530.png]]


>机器人正向动力学是指根据给定的关节位置、速度和加速度等输入，计算机器人末端执行器的位置、速度和加速度等输出。
>机器人反向动力学则是指根据给定的末端执行器位置、速度和加速度等输入，计算机器人各关节的位置、速度和加速度等输出。


-------------

# WoCoCo: 序列化接触全身运动控制

**主要思想**
论文介绍了一种名为 WoCoCo (Whole-Body Control with Sequential Contacts)的全新框架，旨在利用**强化学习**方法对具有**序列式接触的人形机器人进行全身运动控制**
![[Pasted image 20241011154144.png]]

---------------
**创新点：**
- **基于接触阶段的任务分解：**
    - 定义了接触目标和任务目标：接触目标规定了机器人哪些部位需要与环境发生接触，而任务目标则规定了在完成接触目标的前提下，机器人还需要满足哪些额外条件。
    - 优势：将原本复杂的长视野任务分解成多个相对简单的短视野子任务，从而降低学习优难度并提高学习效率。
- **好奇心奖励**：
    - WoCoCo 提出一种基于哈希的随机神经网络 (random neural network based hash) 的方法，通过在状态空间上进行**计数**来衡量状态的新颖性。

------------
**任务建模**
![[Pasted image 20241011155401.png]]
- 任务无关的奖励：在不同的任务场景中可以共用，第一项奖励 $r_{WoCoCo}$ 主要包括三项：接触奖励、阶段奖励、好奇心奖励。
![[Pasted image 20241011155621.png|500]]
	- 接触奖励：将序列化任务分解后完成每个接触任务的奖励（密集型）
	- 阶段奖励：累计完成的接触任务越多获得的奖励越大（离散型）
	- 好奇心奖励：鼓励机器人对新状态进行探索
- 任务相关的奖励：不同的任务场景（跑酷、搬运、舞蹈、攀岩），需要针对性的调整几项任务奖励。

-------------------- 
**接触任务分解：**
任务分解的依据是机器人需要按顺序完成这些阶段以完成整个任务。接触阶段的定义取决于机器人与环境交互所需的特定接触序列。接触任务主要指的是**末端执行器**与环境的接触，而不是关节层面的接触。

例如：
- 在跑酷跳跃任务中，每个垫脚石都对应一个接触阶段，其中实现正确的脚接触定义了接触目标，而保持上半身姿势则构成了任务目标。
- 在箱子移动任务中，第一个接触阶段的目标是将手放在箱子的两侧，而第二个接触阶段的目标是保持手与箱子侧面的接触，并将箱子运送到目的地附近。

------------------------
**好奇心奖励**：

![[Pasted image 20241014212226.png]]
![[Pasted image 20241014212240.png]]
![[Pasted image 20241014212343.png]]

 通过固定神经网络，可以将输入状态数据映射到一个 Hash Bucket （5）。好奇心奖励基于机器人访问同一索引桶（ID of bucket）中的状态进行计数，访问的次数越多，奖励越低（6），从而鼓励机器人探索新的状态。

——————————————————————————————————————

**Sim2Real**
- **课程学习**：
    - 训练的第一阶段：不采用域随机化的方法，并加入好奇心奖励
    - 训练的第二阶段：加入域随机化直至模型收敛
    - 训练的第三阶段：每 2000 次迭代将正则化奖励项的权重增加 20%，直到权重翻倍，从而引发更保守的行为。
- **正则化奖励**：用于对机器人的某些行为进行惩罚，来引导机器人学习更安全、更自然的动作。
![[Pasted image 20241011155321.png]]



# CoMOPPO : 一种受约束的多目标强化学习方法

**Abstract**
本文提出一种受约束的多目标强化学习算法 Constrained multi-objective PPO (CoMOPPO)，在任务层面，通过将任务分解，并设计阶段性奖励，实现复杂任务；在算法层面，本文提出一种新的优势函数（考虑奖励优势函数和成本优势函数），并通过奖励标准化和优势函数标准化，提高训练的稳定性。

***创新点：***
- 阶段性奖励
- 受约束的多目标强化学习策略

***问题建模：***
$$
\begin{gathered}
\max _\pi J_{R_i}(\pi) \forall i \in\{1, \ldots, N\} \\
\text { s.t. } J_{C_j}(\pi) \leq d_j /(1-\gamma) \forall j \in\{1, \ldots, M\}
\end{gathered}
$$
where $J_{R_i}(\pi):=\mathbb{E}_{\tau \sim \pi}\left[\sum_t \gamma^t R_i\left(s_t, a_t, s_{t+1}\right)\right]$, and $d_j$ is a threshold of the $j$ th constraint. 
The target of the CMORL problem finds a **constrained-Pareto (CP) optimal policy**.

> Pareto 最优策略是指在满足所有约束条件的情况下，无法在所有目标上都找到更优策略


***阶段性奖励：***
替换传统的权重加权方法，最大化与每个奖励项相对应的多个目标，并满足与安全相关项相对应的约束。
**难点**：对于需要顺序执行的动作序列，有时需要动态调整奖励权重。
**解决**：将任务分段，函数，并对奖励和成本函数
下图为设定（人为设定）的分阶段的后空翻任务：
![[Pasted image 20241118205108.png|425]]

***受约束的多目标强化学习策略***
**优势函数**：
- **奖励优势函数$A_{R_i}^{\pi_t}$:** 鼓励智能体采取能获得更多奖励的动作，引导智能体学习最佳策略，以最大化累积奖励。
- **成本优势函数$A_{C_j}^{\pi_t}$:** 惩罚智能体采取会导致更多成本的动作，限制智能体的行为，使其在追求奖励的同时避免违反约束条件。

$$
A^{\pi_t}(s, a):=\sum_i \nu_{t, i} A_{R_i}^{\pi_t}(s, a)-\sum_j \lambda_{t, j} A_{C_j}^{\pi_t}(s, a)
$$
其中$\nu$和$\lambda$是对应优势函数的权重，为了更好的体现权重对优势函数的影响，因此需要保持两个优势函数的量纲的一致性，因此提出了奖励标准化和

**奖励标准化**：由于设置中每个奖励函数的尺度不同，需要将其调至相同量纲水平，因此对每个奖励和阶段进行奖励归一化，并使用归一化的奖励训练价值函数。
>奖励标准化是为了解决不同奖励函数之间尺度差异的问题，使其在计算总奖励时贡献相对均衡。**针对的对象是同一类型的奖励**


**优势函数标准化**：即使对奖励进行了归一化，奖励和成本的优势函数仍然可能具有不同的尺度。如果不进行标准化，策略更新可能会过度偏向于最大化目标而忽略约束条件，导致训练过程不稳定。
> 优势函数标准化是为了解决奖励优势函数和成本优势函数之间比例差异的问题，确保策略更新时既能追求奖励最大化，又能有效地满足约束条件。 **针对的对象是不同类型的奖励。**

$$
A^\pi=\frac{A_R^\pi}{\operatorname{Std}\left[A_R^\pi\right]}-\eta \sum_j \frac{A_{C_j}^\pi}{\operatorname{Std}\left[A_{C_j}^\pi\right]} \mathbf{1}_{\left(J_{C_i}(\pi)>d_i\right)},
$$
通过指示函数 ($J_{C_i}(π) > d_i$)，只有当约束条件被满足时，即当前存在较大的惩罚项，才会将成本的优势函数计入聚合优势函数中， $J_{C_i}(π)$ 为累计期望回报。


***Sim 2 Real 方法***
- **Domain randomization**：motor strength and offset, gravity, friction, restitution, noise in joint positions and velocities, and base orientation.
- **Teacher-Students Learning**：在训练过程中，教师策略和学生策略的动作以固定的周期与环境进行交互，这样可以更有益于减少学生策略与教师-学生策略分布的差异。



# CTS: 



# HIL-SERL: 人机协同强化学习
[[RLPD]]



论文介绍了一种名为 HIL-SERL 的机器人操控系统，该系统利用**人机协同强化学习**来训练机器人完成各种精细且灵巧的操控任务。该系统通过整合**人类演示和纠正**、**强化学习算法**以及**系统级选择**，提升机器人的训练速度和复杂。


创新点：
- 加入人类演示数据和人为纠正数据，人类直接对操作进行纠正可以有效提高学习成本，尤其是从头学习的任务

![[Pasted image 20241209201252.png]]





SERL











**Intelligent proximal-policy-optimization-based decision-making system for humanoid robots —— 基于近端策略优化的仿人机器人智能决策系统**
(Advanced Engineering Informatics CCF B)

#ppo

发表时间：2023

主要内容：

针对仿人机器人自主运动决策和控制问题，提出了一种基于 PPO 的分层决策系统架构，上层负责高层次运动任务和步态规划，下层负责详细的关节级运动控制。
- 在上层，首先由**任务规划模块**，根据任务目标 (如移动目的地)规划出大致的运动序列；然后再由 **PPO 算法**基于上层状态空间（如目标位置、步态等），输出合适的**运动原语**（预先定义了一组基本的运动原语 (motion primitives), 如向前迈步、转弯、俯身等基本动作。）序列作为下层输入。
- 在下层，首先由 **运动原语执行模块**，根据上层选择的原语, 生成对应的关节目标轨迹；再由**约束解算器**，针对机器人的动力学约束和关节极限, 对轨迹进行优化调整, 输出满足约束的最终关节轨迹；然后利用**PPO 策略网络**，基于下层状态空间 (当前关节角度、速度等), 直接输出每个关节的力矩控制命令；最后通过**随机采样模块**， 在执行过程中, 通过蒙特卡罗随机采样预测未来可能的状态, 提高决策的鲁棒性。

> 蒙特卡罗随机采样在本文的应用：
> - 对于价值估计：再每一步决策时,系统并不是只考虑当前状态的确定性预测，而是通过蒙特卡洛采样 N 种可能的下一状态，即对于当前状态 s 和动作 a,下一状态 s'遵循某个条件概率分布 P(s'|s,a)，然后将这 N 个采样状态的价值进行平均,作为当前动作 a 在状态 s 下的近似价值估计值。
> - 对于动作输出： 不是单纯地选择当前动作价值最大的动作, 而是从可选动作集合中, 选择具有最优预期价值的动作, 其中预期价值就是通过蒙特卡罗采样估计得到



#sim2real  