
该方法利用模型的先验知识与无模型的实时学习来提高智能体的决策效率和性能。
在 MB-MF RL 方法中，包含一个参数化的环境模型，该模型使用与环境交互的数据进行训练，并将这些真实交互数据保存至状态转移缓存区中。环境模型包括状态转移概率模型和奖励预测模型。

阶段 1：
- 初始化环境模型：环境模型包含状态转移概率模型和奖励预测模型。
- 无模型算法学习：初始化策略网络模型，使用无模型的强化学习算法 SAC 与仿真环境进行交互学习一个初始策略模型，然后将该策略模型迁移至真实环境中进行部署测试，收集该策略与环境交互的状态转移数据。


阶段 2：
- 使用真机中收集的真实交互数据，学习环境模型

阶段 3：
- 前向模拟：利用环境模型进行前向模拟，规划最优动作序列，这里采用基于模型的规划，优化有限时间步的动作序列并执行第一个动作
- 策略网络优化：使用无模型的强化学习算法（SAC），在仿真环境下进行交互训练，在训练过程中会结合基于环境模型的模拟交互数据、真机交互数据、仿真环境的探索数据。

阶段 4：
环境模型的迭代更新：继续使用优化好的策略网络模型进行真机的测试，收集新的数据，从而改进环境模型的预测精度。




