先把"初代 UMI"讲清楚，后面所有变体都是在它的设计空间里做 trade-off

![[Pasted image 20251230195234.png]]
UMI 的核心设计可以概括成三句话：

**第一，手持3D打印夹爪+GoPro**。手里拿的是一个平行夹爪（软指尖），扳机控制开合，夹爪前面挂一台GoPro，配超广角鱼眼镜头，还在两边贴了物理小镜子，直接在一张图里搞出多视角"隐式双目"。整套东西完全可以装进背包，去哪儿都能收数据。

**第二，只用"手腕相机"对齐人和机器人**。人演示时是"GoPro+夹爪"的第一视角，真机部署时，在机器人末端挂同样的GoPro+夹爪，看出去的画面几乎一模一样。这就把人类和机器人统一到同一个观察空间，极大减小了embodiment gap。

**第三，用 Diffusion Policy 学"相对轨迹+延迟补偿"**。UMI 把从视觉 SLAM+IMU 估出来的6DoF 末端轨迹，转成"相对轨迹"，不依赖绝对世界坐标，在训练和部署时都显式建模传感、推理、执行延迟，保证真实机器人动作和当时的视觉是对齐的。Policy 本身用的是 Diffusion Policy，一口气预测一个短时轨迹段，天然适合多模态的人类数据。

效果上，一代UMI已经能做到：动态、双臂、长时序的真实任务（比如倒饮料、切菜、复杂装配），只要换训练数据就能换任务，同一套数据可以零样本迁移到不同自由度的机械臂上，成功率在OOD环境里还能保持在70%左右。

**关键insight**：UMI不是在做"更强的网络"，而是把"人类如何用手和世界交互"这件事装进了一个**高保真、低摩擦的接口**里，再交给一个足够强的生成式policy去拟合。