
# 机器人超参数设置

域随机化的超参数主要包括三类：
- 机器人本体建模的随机化
- 通信延迟的随机化
- 外部扰动的随机化

## 机器人质心（com_displacement）`
 `com_offset_x` &  `com_offset_y`

质心位置变化会改变零力矩点(ZMP)分布，人在行走时躯干 COM 横向摆动幅度约±5cm，，范围±6cm 可以对应典型重心偏移场景。由于机器人结构本身会与 URDF 会有些许出入，因此不同机器人的质心会有些许差异，需要进行微调。


**前后质心**：初次训练时，暂时不设置质心的偏移，根据机器人站立时机器人机身偏前和偏后的状态，在 Sim2Sim 仿真环境（例如 Mujoco）调节机器人的质心去复现这样的姿态。例如，真机机器人质心偏前时，可以在训练时将质心往前调。
`常用范围：[-0.1, 0.1]`

**左右质心**：同前后质心一样，根据训练的结果，在 Sim2Sim 环境中设定固定的直线速度，观察机器人左右偏移情况，如果真机和 Sim2Sim 的表现是一致的，则说明左右质心没有明显偏移。例如，仿真中直行，真机中向左偏，则训练时将质心设置为左偏。
`常用范围：[0，0.03]`

```
L12的URDF文件质心默认偏移了[0.08]

L12-01 训练时质心训练偏移：
x: [0.04,0.08]  --因为默认偏移- > [-0.04,0.0]
y: 0.018

L12-02 训练时质心偏移
用同一套模型，在L12-02表现是偏后，所以02质心较01质心仍然偏后。

y: 0.012

```

## 地面摩擦系数（friction）

摩擦系数覆盖从冰面(μ≈0.1)到橡胶地面(μ≈1.5)的极端情况。上限2.0模拟特殊防滑涂层。

`常用范围：[0.1，2]`
## 机身负载 （payload_mass）

负载质量直接影响系统总惯量矩阵，考虑到负载需求以及有无电池（5kg）的测试，范围\[-6.0,6.0\] kg 覆盖了典型附加负载（如携带物品或工具）

`常用范围：[-6，6]`
## 恢复系数（restitution）

恢复系数基本与地面弹性的正相关性成立，但受材料相态、表面形貌、温度等多因素调制。

**恢复系数定义**为碰撞后相对速度与碰撞前相对速度的比值：
$$
e = \frac{v_2' - v_1'}{v_1 - v_2}
$$

在时域上，该参数精确刻画了动能耗散过程：
- 当$e=1$时（理想弹性碰撞），系统动能守恒，满足：
$$
\frac{1}{2}m_1v_1^2 + \frac{1}{2}m_2v_2^2 = \frac{1}{2}m_1v_1'^2 + \frac{1}{2}m_2v_2'^2
$$
- 当$e=0$时（完全塑性碰撞），动能损失最大：
$$
\Delta KE = \frac{m_1m_2}{2(m_1+m_2)}(v_1-v_2)^2
$$

对于人形机器人，落地冲击时的能量耗散特性直接影响控制策略：
- 木质地板（e≈0.5）允许 10-15%动能通过形变回收
- 橡胶垫（e≈0.3）将 80%以上冲击能转化为热能
- 沙地（e≈0.1）实现 96%以上的能量吸收


恢复系数 $e$ 与材料弹性模量 $E$ 的定量关系可表示为：
$$
e \propto \sqrt{\frac{E}{\rho}} \times \frac{1}{\sqrt{\sigma_y}}
$$
其中 $\rho$ 为密度，$\sigma_y$ 为屈服强度。这显示弹性模量越高、屈服强度越低的材料，确实恢复系数越大。


| 材料类型   | 弹性模量 (GPa) | 典型恢复系数   | 能量耗散率  |
| ------ | ---------- | -------- | ------ |
| 橡胶（轮胎） | 0.01-0.1   | 0.3-0.4  | 60-70% |
| 硬木地板   | 10-15      | 0.5-0.6  | 30-40% |
| 钢板     | 200        | 0.7-0.8  | 10-15% |
| 气凝胶    | 0.001      | 0.05-0.1 | 90-95% |


**5. 特殊案例解析**
**现象**：湿滑冰面（低摩擦μ=0.1）却具有较高恢复系数（e=0.65）
**机理**：
- 低温使水膜结晶，材料进入玻璃态转变区（Tg≈-20℃）
- 弹性模量突增导致碰撞接触时间缩短至5ms
- 振动能量在声子传播中被耗散的比例降低

**控制对策**：
- 主动增加足底阻尼器行程（>8mm）
- 调整步态相位差 $\Delta\phi = \frac{\pi}{2}\sqrt{1-e}$
- 限制关节加速度 $\ddot{\theta}_{max} = 15(1-e)\ rad/s^2$


`常用范围：[0.0，0.5]` ，后面如何想在硬地面表现更好，会增加最高值至 0.8




## 电机强度参数（motor_strength）

电机强度参数代表驱动系统的参数性能，其核心参数包括电机峰值扭矩、减速比、传动效率及整体惯量负载。其数学表示为：
$$
S_m = \frac{\tau_{max} \cdot N \cdot \eta}{\sqrt{J_{rotor} + \frac{J_{load}}{N^2}}}
$$
其中：
- $\tau_{max}$：电机峰值扭矩（Nm）
- $N$：减速比
- $\eta$：传动效率
- $J_{rotor}$：电机转子惯量（kg·m²）
- $J_{load}$：负载惯量（kg·m²）

**谐波减速器 vs. 行星减速器**

| 参数维度    | 谐波减速器         | 行星减速器         |
| ------- | ------------- | ------------- |
| 典型减速比范围 | 50:1 ~ 160:1  | 3:1 ~ 10:1    |
| 效率（η）   | 80%-90%       | 90%-98%       |
| 回差      | <1 arcmin     | 3-10 arcmin   |
| 功率密度    | 0.8-1.2 kW/kg | 1.5-2.5 kW/kg |
| 轴向长度    | 0.8-1.2×直径    | 1.5-2×直径      |
`常用范围：[0.7,1.4]`

## 连杆质量（link_mass）
质量参数 20% 波动模拟制造公差和材料损耗。其中，铝合金连杆典型质量公差：±15%。


`常用范围：[0.8，1.2]`


## 电机偏移（motor_offset）
电机偏移可能会体现在以下几个方面：
- **位置偏移**: 目标关节或位置的实际值与指令值之间存在偏差。
- **输出力矩偏差**: 电机施加的力矩与指令力矩之间存在误差。
- **速度偏移**: 电机旋转的实际速度与设定值之间存在不一致。

在算法训练过程中，我们采用位置偏移，即把电机偏移$\Delta q$加在强化学习输出的动作之后，从而使计算的目标力矩增加或减少。
$$
    q_{\text{}}(t) = q_{\text{target}}(t) + \Delta q
$$


`常用范围：[-0.035,0.045]`

## 关节摩擦（joint_friction）

计算关节的摩擦力：
$$ f_{\text{joint}} = F_c \cdot \text{sign}(\dot{q}) + F_v \cdot \dot{q} + F_s \cdot h(|\dot{q}|, \epsilon) $$
将摩擦力作用在电机的力矩输出中：
$$ \tau_{\text{real}} = \tau_{\text{target}} - f_{\text{joint}} $$

其中，$f(q, \dot{q})$ 是关节摩擦的关键部分。摩擦力的建模一般包括以下三种类型：
1. **库仑摩擦（Coulomb Friction）**  
   一种速度无关的摩擦，大小为常值，方向与关节速度 $\dot{q}$ 相反： $$
   f_{\text{Coulomb}} = \text{sign}(\dot{q}) \cdot F_c
   $$其中，$F_c$ 是库仑摩擦系数，$\text{sign}(\dot{q})$ 表示速度的符号函数。

在实际应用过程中，通常利用 $\tanh$ 函数，处理关节速度的符号特性。当$\dot{q}$ 较小时， $\tanh$ 近似线性；当速度增大时， $\tanh$函数饱和于 1 或-1，从而使该项趋于一个固定值：
$$
 \text{Term}_1 = F_c \cdot \tanh\left(\frac{\dot{q}}{Q_s}\right)
 $$
     这样就可以模拟库仑摩擦的恒定摩擦力，保证在高速时不会无限增大。
 
1. **粘滞摩擦（Viscous Friction）**  
   与关节速度成正比的摩擦力：
   $$
   f_{\text{Viscous}} = F_v \cdot \dot{q}
   $$
   其中，$F_v$ 是粘滞摩擦系数。

 在实际应用过程中，直接按速度乘以系数 $F_v$，模拟粘滞摩擦力，即摩擦力线性随速度变化：
 $$
 \text{Term}_2 = F_v \cdot \dot{q}
 $$
 这部分在速度较大时同样会增加，但没有饱和效果。


2. **静摩擦（Static Friction）**  
   阻止关节启动的静态摩擦，在速度接近零时出现：
   $$
   f_{\text{Static}} = F_s \quad \text{if} \quad |\dot{q}| < \epsilon
   $$
   其中，$F_s$ 是静摩擦阈值，$\epsilon$ 是低速阈值。

综合起来，一个完整的摩擦模型可以表示为：
$$
f(q, \dot{q}) = F_c \cdot \text{sign}(\dot{q}) + F_v \cdot \dot{q} + F_s \cdot h(|\dot{q}|, \epsilon)
$$

其中，$h(|\dot{q}|, \epsilon)$ 是静摩擦的作用范围函数。

3. **摩擦随机化**
为了应对现实摩擦力的不确定性，可以在仿真中对摩擦参数进行随机化，增强控制策略的鲁棒性。随机化方法可以是：
- 均匀分布：$F_r \sim U(F_{\text{min}}, F_{\text{max}})$
- 正态分布：$F_r= \sim \mathcal{N}(\mu, \sigma)$
---- 

摩擦力会抵消部分力矩输出，导致关节实际输出不能完全响应控制指令。在启动、低速运动和停止时尤为显著。

例如：
$$
\tau_{\text{实际}} = \tau_{\text{target}} - (F_c + F_s) - F_v \cdot \dot{q}
$$

常用取值范围：
```Python
1-4 行星大关节
[0.03,0.08] -- 行星关节1-4

5-6 踝关节
行星关节
Star_Fc = 0.45
Star_Qs = 0.1
Star_Fv = 0.023 


谐波关节：
Left_Fc = 2.8
Left_Qs = 0.1
Left_Fv = 0.21
Right_Fc = 2.8
Right_Qs = 0.1
Right_Fv = 0.21

随机化摩擦系数：

```
`




## 转动惯量 （joint_armature）
遍历每个关节，并从各自的 armature 常用取值范围中均匀随机采样：

`常用取值范围： [0.008, 0.12]`

## 外部扰动（disturbance）
对机器人的三个方向随机生成扰动力，将力和力矩施加在刚体上，通常用于模拟外部扰动。

`常用取值范围： [-500, 500]`



## 随机推力（push_robots）
对机器人的基体线速度和角速度增加扰动，将随机化元素直接施加在机器人的基体状态中。外部扰动和随机推力都存在一定间隔。

`常用范围：push_vel_xy = [-0.3,0.3] push_ang_vel = [-0.4,0.4]


## 控制增益（kp_kd）

随机化比例控制增益和微分比例控制增益
`常用范围[0.8,1.2]`


## 动作延迟（action_delay）
模拟真实机器人中从控制指令产生到实际执行之间存在的延迟现象。实现的方式是增加一个**延迟动作队列**，在每步执行的时候，将当前动作存储到延迟队列中，并从队列中随机取出延迟时间对应的动作进行应用。

`常用范围：[5,30]`
具体延迟可根据实际情况进行调整
## 系统延迟 （sys_delay）

系统延迟包括，IMU 延迟和电机状态延迟。其中 IMU 影响基体状态信息，电机状态延迟影响关节位置和速度。

`常用范围：IMU [1,10]  电机状态延迟[1,5]`


## 上肢速度扰动（upperbody_speed）
对左右臂的关节速度设置扰动，当强化仅考虑控制下半身运动时，可以把上肢的运动作为扰动，这样强化站立状态就可以支持上半身播放动作或遥操作。

上肢运动的扰动通常可以读取一个固定的参考播放动作序列数据集，而增大或减小从数据集中取值的间隔可以影响上肢运动的速度，我们实现时可以考虑随机化一个间隔的范围，从而控制上半身运动的速度。

`常用范围：[0,12]`



# 机器人奖励函数
## 综合奖励函数设计分析

从以上分类和分析可以看出，这套奖励函数体系是一个精心设计的多目标优化系统，具有以下特点：

### 1. 多维度控制
奖励函数涵盖了**姿态控制、步态规划、轨迹跟踪、安全限制和能效优化**五个维度，全面约束机器人的行为。

### 2. 层次化设计
- **基础层**: 安全性与限制类奖励确保基本操作安全
- **运动层**: 姿态与步态类奖励塑造基本运动模式
- **任务层**: 速度跟踪类奖励满足特定任务需求
- **优化层**: 能效与平滑类奖励优化运动质量

### 3. 缩放参数设计原则
- **敏感度调整**: 不同奖励项使用不同的指数衰减系数调整敏感度
- **指数-线性组合**: 对关键指标使用指数-线性组合奖励，平衡小误差精确度和大误差梯度
- **阈值与截断**: 使用最大/最小阈值和截断值控制奖励范围，防止极端情况
- **分段函数**: 对一些复杂行为使用分段函数明确定义不同状态的奖励

### 4. 运动学原理应用
- **生物力学映射**: 模仿人类步行的生物力学特性
- **物理约束**: 确保运动符合物理规律，避免不自然状态
- **动态平衡**: 通过多种机制确保动态平衡
- **能量效率**: 通过惩罚高力矩、高速度和高加速度优化能耗

这套奖励函数系统通过精心调整的参数和综合考虑的设计，引导强化学习算法找到在稳定性、通用性和能效之间的最佳平衡，实现人形机器人的自然、高效步行。

##  一、姿态与平衡控制类奖励

### 1. 关节位置奖励（joint_pos） @


$$r = e^{-2\|j_{pos} - j_{target}\|} - 0.2 \cdot \min(\|j_{pos} - j_{target}\|, 0.5)$$

**作用意义**：鼓励机器人保持其关节位置接近目标位置，实现精确的姿态控制。

**计算方法**：
- 计算当前关节位置 (`joint_pos`)与目标位置 (`pos_target`)之间的差异向量
- 使用指数项 $e^{-2\|diff\|}$ 对小误差给予高奖励
- 使用线性惩罚项 $-0.2\|diff\|_{clamp(0,0.5)}$ 对大误差进行惩罚

**运动学意义**：
- 确保机器人能够准确追踪预设的关节轨迹
- 指数项确保小误差时有高奖励，鼓励精确定位
- 线性惩罚项确保大偏差时有足够的梯度信号引导学习

**代码实现**
```python
def _reward_joint_pos(self):
    joint_pos = self.dof_pos.clone()
    pos_target = self.ref_dof_pos.clone()
    diff = joint_pos - pos_target
    r = torch.exp(-2 * torch.norm(diff, dim=1)) - 0.2 * torch.norm(diff, dim=1).clamp(0, 0.5)
    return r
```


###  2. 默认关节位置奖励 ( default_joint_pos ) @

**公式解释**:
$$r = e^{-100 \cdot \max(\|yaw\_roll\| - 0.3, 0)} - 0.01 \cdot \|j_{pos} - j_{default}\|$$

**作用**:
维持关节在默认位置附近，特别关注足踝的偏航和横滚角度，这对保持步态的稳定至关重要。

**缩放参数与逻辑**:
- 偏航横滚指数系数`-100`: 对足踝旋转偏差敏感度高
- 偏航横滚阈值`0.3`: 允许小角度偏差
- 关节位置差异系数`0.01`: 关节位置差异的惩罚相对较小

**运动学逻辑**:
- 足踝的偏航 (yaw)和横滚 (roll)角度对步行稳定性至关重要
- 通过计算每只脚踝在机器人朝向坐标系中的旋转，剔除朝向因素影响
- 限制足踝的侧向转动防止脚部不自然地内翻或外翻
- 允许足踝在俯仰方向上的自由度，便于适应地形

**代码实现**
```python
def _reward_default_joint_pos(self):
    joint_diff = self.dof_pos - self.default_joint_pd_target
    heading_rot = torch_utils.calc_heading_quat_inv(self.base_quat)
    knee_rot = self.rigid_state[:, self.ankle_indices, 3:7]
    knee_rot_0 = quat_mul(heading_rot, knee_rot[:, 0])
    knee_rot_1 = quat_mul(heading_rot, knee_rot[:, 1])
    knee_rot_0 = get_euler_xyz_tensor(knee_rot_0)
    knee_rot_1 = get_euler_xyz_tensor(knee_rot_1)
    left_yaw_roll = knee_rot_0[:, [0,2]]
    right_yaw_roll = knee_rot_1[:, [0,2]]
    left_yaw_roll[:, :] *= 0.
    right_yaw_roll[:, :] *= 0.
    yaw_roll = torch.norm(left_yaw_roll, dim=1) + torch.norm(right_yaw_roll, dim=1)
    yaw_roll = torch.clamp(yaw_roll - 0.3, 0, 50)
    return torch.exp(-yaw_roll * 100) - 0.01 * torch.norm(joint_diff, dim=1)
```




### 3. 方向奖励（orientation）

$$r = \frac{e^{-10 \cdot \|euler_{xy}\|} + e^{-20 \cdot \|g_{proj,xy}\|}}{2}$$

其中：
- $euler_{xy}$：基座欧拉角的俯仰和横滚分量
- $g_{proj,xy}$：投影重力向量的水平分量


**作用意义**：鼓励机器人保持躯干水平，防止倾斜。

**计算方法**：
- 使用欧拉角的前两个分量 (roll 和 pitch)评估躯干倾斜
- 使用投影重力向量评估偏离垂直的程度
- 对这两个指标使用指数奖励函数

**缩放参数与逻辑**:
- 欧拉角惩罚系数`10`: 控制对姿态偏差的敏感度
- 重力投影惩罚系数`20`: 控制对重力偏差的敏感度（比欧拉角更敏感）
- 采用平均值组合两种测量方法，提高鲁棒性

**运动学逻辑**:
- 使用两种互补的方法测量躯干倾斜度: 欧拉角和重力投影
- 欧拉角评估躯干的绝对倾斜（俯仰和横滚）
- 重力投影评估重力在水平面上的分量，提供了另一种衡量躯干不垂直程度的方式
- 两种方法的平均值提供了更鲁棒的姿态评估

**代码实现**
```python
def _reward_orientation(self):
    quat_mismatch = torch.exp(-torch.sum(torch.abs(self.base_euler_xyz[:, :2]), dim=1) * 10)
    orientation = torch.exp(-torch.norm(self.projected_gravity[:, :2], dim=1) * 20)
    return (quat_mismatch + orientation) / 2.
```



### 4. 基体高度奖励 （base_height）


**公式解释**:
$$r = e^{-100 \cdot |h_{base} - h_{target}|}$$
其中 $h_{base}$ = 基座高度 - 支撑足平均高度 + 0.05

**作用**:
确保机器人维持在适当的高度，避免过度下蹲或伸展。

**缩放参数与逻辑**:
- 指数衰减系数`-100`: 对高度偏差高度敏感
- 高度补偿`0.05`: 考虑足部厚度的偏移量
- 目标高度:`self.cfg.rewards.base_height_target`: 预设的理想基座相对高度

**运动学逻辑**:
- 相对于支撑足高度计算基座高度，使该指标在不平地面上依然有效
- 使用步态相位 (stance_mask)识别当前的支撑足，仅考虑支撑足的高度
- 适当的基座高度对于能量效率和稳定性至关重要，过低会增加能量消耗，过高会增加不稳定性


**代码实现：**

```python
def _reward_base_height(self):
    stance_mask = self._get_gait_phase()
    measured_heights = torch.sum(self.rigid_state[:, self.feet_indices, 2] * stance_mask, dim=1) / torch.sum(stance_mask, dim=1)
    base_height = self.root_states[:, 2] - (measured_heights - 0.05)
    return torch.exp(-torch.abs(base_height - self.cfg.rewards.base_height_target) * 100)
```


### 5. 基体加速度奖励（base_acc）@

**公式解释**:
$$r = e^{-3 \cdot \|a_{root}\|}$$

**作用**:
惩罚基座的高加速度，鼓励平滑运动，减少突然的加速和减速。

**缩放参数与逻辑**:
- 指数衰减系数`-3`: 对加速度的中等敏感度，平衡平滑性需求和运动灵活性
- 使用范数计算合成加速度大小，考虑所有方向的加速度

**运动学逻辑**:
- 突然的加速或减速会导致动力学不稳定，增加控制难度
- 平滑的基座运动有助于维持重心稳定，减少关节马达的负担
- 通过减少加速度可以实现更自然、更高效的步态

**代码实现**
```python
def _reward_base_acc(self):
    root_acc = self.last_root_vel - self.root_states[:, 7:13]
    rew = torch.exp(-torch.norm(root_acc, dim=1) * 3)
    return rew
```


## 二、步态与足部行为类

### 1. 足部距离奖励 （feet_distance）@


**公式解释**:
$$r = \frac{e^{-100 \cdot |d_{min}|} + e^{-100 \cdot |d_{max}|}}{2}$$
其中：
- $d_{min} = \text{clamp}(d_{feet} - d_{min\_thresh}, -0.5, 0)$
- $d_{max} = \text{clamp}(d_{feet} - d_{max\_thresh}, 0, 0.5)$

**作用**:
控制两足之间的水平距离，防止过近（不稳定）或过远（步幅过大）。

**缩放参数与逻辑**:
- 指数衰减系数`-100`: 对足部间距偏差非常敏感
- 最小距离阈值`self.cfg.rewards.min_dist`: 足部间的最小安全距离
- 最大距离阈值`self.cfg.rewards.max_dist`: 足部间的最大合理距离
- 最小/最大距离偏差上限`±0.5`: 限制惩罚范围，防止过大惩罚

**运动学逻辑**:
- 足部间距是侧向稳定性的关键指标
- 过小的足部间距会失去侧向稳定性，增加侧翻风险
- 过大的足部间距导致非自然步态，需要更多能量消耗，也增加关节负担
- 使用双边软约束 (两个指数项)同时惩罚过小和过大的间距

**代码实现：**
```python
def _reward_feet_distance(self):
    foot_pos = self.rigid_state[:, self.feet_indices, :2]
    foot_dist = torch.norm(foot_pos[:, 0, :] - foot_pos[:, 1, :], dim=1)
    fd = self.cfg.rewards.min_dist
    max_df = self.cfg.rewards.max_dist
    d_min = torch.clamp(foot_dist - fd, -0.5, 0.)
    d_max = torch.clamp(foot_dist - max_df, 0, 0.5)
    return (torch.exp(-torch.abs(d_min) * 100) + torch.exp(-torch.abs(d_max) * 100)) / 2
```

### 2. 膝盖距离奖励 （knee_distance）@

**公式解释**:
$$r = \frac{e^{-100 \cdot |d_{min}|} + e^{-100 \cdot |d_{max}|}}{2}$$
其中：
- $d_{min} = \text{clamp}(d_{knee} - d_{min\_thresh}, -0.5, 0)$
- $d_{max} = \text{clamp}(d_{knee} - d_{max\_thresh}/2, 0, 0.5)$

**作用**:
控制膝盖之间的水平距离，确保膝关节位置合理，避免交叉或过宽。

**缩放参数与逻辑**:
- 指数衰减系数`-100`: 对膝盖间距偏差非常敏感
- 最小距离阈值: 与足部相同
- 最大距离阈值: 足部最大距离的一半，反映膝盖应有更小的横向距离

**运动学逻辑**:
- 膝盖间距直接影响腿部驱动器的受力情况
- 膝盖过近会导致腿部肌肉/驱动器受力不均匀，增加侧向力
- 膝盖间距应小于足部间距，符合人类腿部生物力学结构
- 最大阈值仅为足部最大阈值的一半，反映了这种生物力学约束

**代码实现：**

```python
def _reward_knee_distance(self):
    foot_pos = self.rigid_state[:, self.knee_indices, :2]
    foot_dist = torch.norm(foot_pos[:, 0, :] - foot_pos[:, 1, :], dim=1)
    fd = self.cfg.rewards.min_dist
    max_df = self.cfg.rewards.max_dist / 2
    d_min = torch.clamp(foot_dist - fd, -0.5, 0.)
    d_max = torch.clamp(foot_dist - max_df, 0, 0.5)
    return (torch.exp(-torch.abs(d_min) * 100) + torch.exp(-torch.abs(d_max) * 100)) / 2
```



### 3. 足部滑动惩罚 （foot_slip） @

```python
def _reward_foot_slip(self):
    contact = self.contact_forces[:, self.feet_indices, 2] > 5.
    foot_speed_norm = torch.norm(self.rigid_state[:, self.feet_indices, 7:9], dim=2)
    rew = torch.sqrt(foot_speed_norm)
    rew *= contact
    return torch.sum(rew, dim=1)
```

**公式解释**:
$$r = \sum_{i \in \{feet\}} \sqrt{\|v_{feet,i,xy}\|} \cdot I(f_{contact,i,z} > 5)$$
其中 $I(\cdot)$ 是指示函数，条件为真时为 1，否则为 0。

**作用**:
惩罚足部在接触地面时的水平滑动，促进稳定的支撑相。

**缩放参数与逻辑**:
- 平方根变换: 对速度使用 $\sqrt{v}$ 转换，减轻对大速度的惩罚程度
- 接触阈值`5.0`: 判定足部是否与地面接触的力阈值

**运动学逻辑**:
- 支撑相的足部应稳固地接触地面，不应滑动
- 足部滑动会导致能量损失、控制不稳和平衡问题
- 这是一个**负向奖励**（惩罚），值越大表示滑动越严重
- 仅对接触地面的足部计算滑动惩罚，非接触状态无需考虑滑动

### 4. 增强型足部滑动惩罚 （foot_slip2）

**公式解释**:
$$r = I(E)$$
其中 $E$ 是以下四个条件的合并:
1. $\|f_{feet,0}\| > 100 \land h_{feet,0} > 0.1$
2. $\|f_{feet,1}\| > 100 \land h_{feet,1} > 0.1$
3. $\|f_{feet,0}\| > 20 \land \|v_{feet,0,xy}\| > 0.5$
4. $\|f_{feet,1}\| > 20 \land \|v_{feet,1,xy}\| > 0.5$

**作用**:
比基本足部滑动惩罚更复杂，检测四种异常情况：足部悬空但受力和足部接触但高速滑动。

**缩放参数与逻辑**:
- 高力阈值`100`: 判定足部是否受到显著力作用
- 高度阈值`0.1`: 判定足部是否离地
- 低力阈值`20`: 足部接触的最低力阈值
- 速度阈值`0.5`: 判定足部是否高速滑动的阈值

**运动学逻辑**:
- 检测物理不合理的情况：足部离地但受力（情况 1 和 2）
- 检测不稳定情况：足部接触但高速滑动（情况 3 和 4）
- 二值奖励形式（0 或 1）作为错误指示器，用于识别异常状态
- 这是检测模拟环境中的物理不合理状态或机器人不稳定步态的重要指标

**代码实现：**
```python
def _reward_foot_slip2(self):
    feet_force = torch.norm(self.contact_forces[:, self.feet_indices, :3], dim=2)
    feet_z = self.rigid_state[:, self.feet_indices, 2]
    foot_speed_norm = torch.norm(self.rigid_state[:, self.feet_indices, 7:9], dim=2)

    error_buf = torch.logical_and(feet_force[:, 0] > 100, feet_z[:, 0] > 0.1)
    error_buf |= torch.logical_and(feet_force[:, 1] > 100, feet_z[:, 1] > 0.1)
    error_buf |= torch.logical_and(feet_force[:, 0] > 20, foot_speed_norm[:, 0] > 0.5)
    error_buf |= torch.logical_and(feet_force[:, 1] > 20, foot_speed_norm[:, 1] > 0.5)

    reward = torch.where(error_buf, 1., 0.)
    return reward
```



### 5. 足部俯仰控制 （foot_pitch）

**公式解释**:
$$r = e^{-10 \cdot (pitch_{left}^2 + pitch_{right}^2)}$$

**作用**:
鼓励足部保持水平姿态，减少俯仰角度，确保足底与地面良好接触。

**缩放参数与逻辑**:
- 指数衰减系数`-10`: 控制对足部俯仰角的敏感度
- 平方项: 对大角度偏差施加更强的惩罚

**运动学逻辑**:
- 足部保持水平有助于稳定站立和行走
- 过大的俯仰角会导致足部仅部分接触地面，减少支撑面积，降低稳定性
- 水平的足部接触有助于体重均匀分布，减少局部压力
- 注意：代码中有一个潜在错误，`error = torch.square(foot_rot_0) + torch.square(foot_rot_0)`可能应该是`error = torch.square(foot_rot_0) + torch.square(foot_rot_1)`


**代码实现：**
```python
def _reward_foot_pitch(self):
    foot_rot = self.rigid_state[:, self.feet_indices, 3:7]
    foot_rot_0 = foot_rot[:, 0]
    foot_rot_1 = foot_rot[:, 1]
    foot_rot_0 = get_euler_xyz_tensor(foot_rot_0)[:, 1]
    foot_rot_1 = get_euler_xyz_tensor(foot_rot_1)[:, 1]

    error = torch.square(foot_rot_0) + torch.square(foot_rot_0)
    reward = torch.exp(error * -10.)
    return reward
```

### 6. 足部悬空时间奖励 （feet_air_time）

**公式解释**:
$$r = \sum_{i \in \{feet\}} \min(t_{air,i}, 0.5) \cdot I(t_{air,i} > 0 \land (contact_{i} \lor last\_contact_{i}))$$
其中 $t_{air,i}$ 是第 i 只脚的累积悬空时间。

**作用**:
鼓励足部在摆动相有适当的悬空时间，促进更长的步幅。

**缩放参数与逻辑**:
- 最大悬空时间限制`0.5`: 限制过长悬空时间的奖励，防止机器人长时间单脚站立
- 时间增量`self.dt`: 每个时间步累加的悬空时间
- 接触阈值`5.0`: 判定足部是否接触地面的垂直力阈值

**运动学逻辑**:
- 足部悬空时间是步长和步速的关键指标
- 使用状态保存变量 (`self.feet_air_time`)跟踪每只脚的悬空时间
- 当足部重新接触地面时 (`first_contact`)给予奖励，奖励大小与悬空时间成正比
- 接触地面后重置悬空时间计数 (`self.feet_air_time *= ~self.contact_filt`)
- 鼓励机器人做出完整的步态周期，而不是频繁地抬起和放下同一只脚

**代码实现：**
```python
def _reward_feet_air_time(self):
    contact = self.contact_forces[:, self.feet_indices, 2] > 5.
    self.contact_filt = torch.logical_or(contact, self.last_contacts)
    self.last_contacts = contact
    first_contact = (self.feet_air_time > 0.) * self.contact_filt
    self.feet_air_time += self.dt
    air_time = self.feet_air_time.clamp(0, 0.5) * first_contact
    self.feet_air_time *= ~self.contact_filt
    return air_time.sum(dim=1)
```

### 7. 足部接触数量奖励 （feet_contact_number）

**公式解释**:
$$r = \sum_{i \in \{feet\}} I(contact_i = phase_i)$$
其中 $phase_i$ 在低速时为 1，高速时为指定步态相位。

**作用**:
确保足部的接触状态与期望的步态相位一致。

**缩放参数与逻辑**:
- 接触阈值`5.0`: 判定足部是否接触地面的垂直力阈值
- 速度阈值`0.1`: 区分低速和高速模式的前向速度命令阈值
- 奖励值`1.0`: 每个足部接触状态与步态相位匹配时的奖励

**运动学逻辑**:
- 在低速时 (`|cmd_x| <= 0.1`)鼓励双足同时接触地面，增强稳定性
- 在高速时根据预定义的步态相位确定每只脚的期望接触状态
- 这种设计使机器人能够根据速度自动切换步态模式
- 例如，低速时使用双足支撑步态提高稳定性，高速时使用交替单足支撑步态提高效率

**代码实现：**

```python
def _reward_feet_contact_number(self):
    contact = self.contact_forces[:, self.feet_indices, 2] > 5.
    stance_mask = self._get_gait_phase()
    stance_mask = torch.where(torch.abs(self.commands[:, 0:1]) <= 0.1, stance_mask*0.+1., stance_mask)
    reward = torch.where(contact == stance_mask, 1, 0.)
    return torch.sum(reward, dim=1)
```

### 8. 膝关节限位奖励 （knee_limit）


**公式解释**:
$$r = \frac{I(knee_{left} < 0.15) + I(knee_{right} < 0.15)}{2}$$

**作用**:
防止膝关节过度伸展，保持膝关节弯曲状态。

**缩放参数与逻辑**:
- 膝关节阈值`0.15`: 最大允许的膝关节伸展角度
- 二值奖励`1.0/0.0`: 满足/不满足条件时的奖励值

**运动学逻辑**:
- 人形机器人的膝关节过度伸展（锁膝）会导致多种问题:
  1. 锁膝状态下无法吸收地面冲击
  2. 可能导致关节损伤
  3. 减少控制灵活性
- 保持适度弯曲的膝关节可以:
  1. 更好地响应外部扰动
  2. 提高运动控制的精确度
  3. 模拟人类自然步态中的膝关节状态

**代码实现：**
```python
def _reward_knee_limit(self):
    knee_joint_left = self.dof_pos[:, 3]
    knee_joint_right = self.dof_pos[:, 9]
    reward_left = torch.where(knee_joint_left < 0.15, 1., 0.)
    reward_right = torch.where(knee_joint_right < 0.15, 1., 0.)
    return (reward_left + reward_right)/2.
```


### 9. 足部离地高度奖励 （feet_clearance）

**公式解释**:
$$r = \sum_{i \in \{feet\}} I(|h_{feet,i} - h_{target}| < 0.01) \cdot (1-phase_i)$$
其中 $h_{feet,i}$ 是足部累积的高度变化，$phase_i$ 是步态相位 (1 为支撑相)。

**作用**:
鼓励摆动相的足部达到目标抬高高度，确保足够的离地间隙，避免绊倒。

**缩放参数与逻辑**:
- 高度阈值`0.01`: 判定足部高度是否接近目标高度的容差
- 高度偏移`0.05`: 考虑足部厚度的补偿值
- 目标高度`self.cfg.rewards.target_feet_height`: 期望的足部抬高高度

**运动学逻辑**:
- 在摆动相 (swing_mask=1)，足部需要适当抬高以避免绊倒
- 累积记录足部高度变化 (`self.feet_height`)，当接触地面时重置
- 奖励足部在摆动相能精确达到目标高度，而非过高或过低
- 过低的抬高高度可能导致绊倒，过高则浪费能量

**代码实现：**
```python
def _reward_feet_clearance(self):
    contact = self.contact_forces[:, self.feet_indices, 2] > 5.
    feet_z = self.rigid_state[:, self.feet_indices, 2] - 0.05
    delta_z = feet_z - self.last_feet_z
    self.feet_height += delta_z
    self.last_feet_z = feet_z
    swing_mask = 1 - self._get_gait_phase()
    rew_pos = torch.abs(self.feet_height - self.cfg.rewards.target_feet_height) < 0.01
    rew_pos = torch.sum(rew_pos * swing_mask, dim=1)
    self.feet_height *= ~contact
    return rew_pos
```


## 三、运动轨迹跟踪类

### 1. 速度匹配指数奖励 （vel_mismatch_exp）

**公式解释**:
$$r = \frac{e^{-10 \cdot v_{base,z}^2} + e^{-5 \cdot \|w_{base,xy}\|}}{2}$$

**作用**:
鼓励机器人维持稳定的水平运动，最小化垂直方向的速度和非偏航方向的角速度。

**缩放参数与逻辑**:
- 线速度平方系数`10`: 对垂直方向速度的惩罚强度
- 角速度系数`5`: 对横滚和俯仰角速度的惩罚强度

**运动学逻辑**:
- 稳定的步行应当主要在水平面内进行，垂直方向的速度应最小化
- 快速的横滚和俯仰角运动通常表示不稳定性或即将跌倒
- 通过惩罚垂直速度，鼓励机器人保持身体高度稳定，减少上下震荡
- 通过惩罚非偏航角速度，鼓励机器人保持躯干稳定，避免过度摇晃

**代码实现：**
```python
def _reward_vel_mismatch_exp(self):
    lin_mismatch = torch.exp(-torch.square(self.base_lin_vel[:, 2]) * 10)
    ang_mismatch = torch.exp(-torch.norm(self.base_ang_vel[:, :2], dim=1) * 5.)
    c_update = (lin_mismatch + ang_mismatch) / 2.
    return c_update
```


### 2. 速度跟踪强制奖励 （track_vel_hard）

**公式解释**:
$$r = \frac{e^{-10 \cdot \|v_{cmd,xy} - v_{base,xy}\|} + e^{-10 \cdot |w_{cmd,z} - w_{base,z}|}}{2} - 0.2 \cdot (\|v_{cmd,xy} - v_{base,xy}\| + |w_{cmd,z} - w_{base,z}|)$$

**作用**:
强制机器人精确跟踪指令速度，同时使用指数和线性组合奖励形式。

**缩放参数与逻辑**:
- 指数衰减系数`10`: 控制对速度误差的敏感度
- 线性惩罚系数`0.2`: 为大误差提供持续的梯度信号
- 指数项和线性项结合: 在小误差区域提供高敏感度，同时保证大误差区域有足够梯度

**运动学逻辑**:
- 跟踪线速度命令确保机器人按预期速度在平面上移动
- 跟踪角速度命令确保机器人按预期速率转向
- 使用指数-线性组合奖励，既鼓励精确跟踪又确保有足够学习梯度
- 这种组合奖励在小偏差时通过指数项给予高奖励，大偏差时通过线性项维持梯度

**代码实现：**
```python
def _reward_track_vel_hard(self):
    lin_vel_error = torch.norm(self.commands[:, :2] - self.base_lin_vel[:, :2], dim=1)
    lin_vel_error_exp = torch.exp(-lin_vel_error * 10)
    ang_vel_error = torch.abs(self.commands[:, 2] - self.base_ang_vel[:, 2])
    ang_vel_error_exp = torch.exp(-ang_vel_error * 10)
    linear_error = 0.2 * (lin_vel_error + ang_vel_error)
    return (lin_vel_error_exp + ang_vel_error_exp) / 2. - linear_error
```


### 3. 线速度跟踪奖励 （tracking_lin_vel）

**公式解释**:
$$r = e^{-\sigma \cdot \|v_{cmd,xy} - v_{base,xy}\|^2}$$

**作用**:
鼓励机器人准确跟踪水平面内的线速度命令。

**缩放参数与逻辑**:
- 跟踪敏感度系数`self.cfg.rewards.tracking_sigma`: 控制对速度误差的惩罚强度
- 平方误差: 对大偏差施加更强的惩罚

**运动学逻辑**:
- 精确的线速度控制是人形机器人任务执行的基础
- 使用平方误差惩罚，大偏差受到更强烈的惩罚
- 指数函数确保奖励值在[0,1]范围内，有利于与其他奖励项结合
- 与通用的速度跟踪奖励相比，这个奖励专注于线速度，可以单独调整线速度跟踪的权重

**代码实现：**
```python
def _reward_tracking_lin_vel(self):
    lin_vel_error = torch.sum(torch.square(self.commands[:, :2] - self.base_lin_vel[:, :2]), dim=1)
    return torch.exp(-lin_vel_error * self.cfg.rewards.tracking_sigma)
```


### 4. 角速度跟踪奖励 （tracking_ang_vel）

**公式解释**:
$$r = e^{-\sigma \cdot (w_{cmd,z} - w_{base,z})^2}$$

**作用**:
鼓励机器人准确跟踪偏航角速度命令。

**缩放参数与逻辑**:
- 跟踪敏感度系数`self.cfg.rewards.tracking_sigma`: 控制对角速度误差的惩罚强度
- 平方误差: 对大偏差施加更强的惩罚

**运动学逻辑**:
- 精确的角速度控制对于转向和姿态调整至关重要
- 仅关注偏航角速度 (z 轴)，因为这是水平面内转向的主要参数
- 将角速度跟踪与线速度跟踪分离，允许单独调整权重
- 两者结合使机器人能够精确执行复杂的运动轨迹

**代码实现：**
```python
def _reward_tracking_ang_vel(self):
    ang_vel_error = torch.square(self.commands[:, 2] - self.base_ang_vel[:, 2])
    return torch.exp(-ang_vel_error * self.cfg.rewards.tracking_sigma)
```


### 5. 低速控制奖励 （low_speed）

**公式解释**:
$$r = \begin{cases}
-1.0, & \text{if } |v_{base,x}| < 0.5 \cdot |v_{cmd,x}| \\
0.0, & \text{if } |v_{base,x}| > 1.2 \cdot |v_{cmd,x}| \\
1.2, & \text{if } 0.5 \cdot |v_{cmd,x}| \leq |v_{base,x}| \leq 1.2 \cdot |v_{cmd,x}| \\
-2.0, & \text{if } \text{sign}(v_{base,x}) \neq \text{sign}(v_{cmd,x})
\end{cases} \cdot I(|v_{cmd,x}| > 0.1)$$

**作用**:
确保机器人速度在命令速度的合理范围内，特别强调方向正确性。

**缩放参数与逻辑**:
- 低速阈值`0.5`: 定义速度过低的判断标准（命令速度的 50%）
- 高速阈值`1.2`: 定义速度过高的判断标准（命令速度的 120%）
- 惩罚值`-1.0`: 速度过低的惩罚
- 中性值`0.0`: 速度过高时给予小的惩罚
- 奖励值`1.2`: 速度在期望范围内的奖励
- 方向错误惩罚`-2.0`: 方向错误的严重惩罚
- 命令阈值`0.1`: 仅当速度命令足够大时应用此奖励

**运动学逻辑**:
- 速度方向比精确的速度大小更重要，因此错误方向受到最严厉惩罚
- 速度过低比速度过高更严重，因为过低表示动力不足或控制不良
- 速度过高虽不理想但可接受，因此给予中性奖励而非惩罚
- 仅对有意义的速度命令 (>0.1)应用此奖励，避免静止时的噪声影响
- 这种分段式奖励设计比连续奖励函数更清晰地表达了优先级


**代码实现：**
```python
def _reward_low_speed(self):
    absolute_speed = torch.abs(self.base_lin_vel[:, 0])
    absolute_command = torch.abs(self.commands[:, 0])
    speed_too_low = absolute_speed < 0.5 * absolute_command
    speed_too_high = absolute_speed > 1.2 * absolute_command
    speed_desired = ~(speed_too_low | speed_too_high)
    sign_mismatch = torch.sign(self.base_lin_vel[:, 0]) != torch.sign(self.commands[:, 0])
    reward = torch.zeros_like(self.base_lin_vel[:, 0])
    reward[speed_too_low] = -1.0
    reward[speed_too_high] = 0.
    reward[speed_desired] = 1.2
    reward[sign_mismatch] = -2.0
    return reward * (self.commands[:, 0].abs() > 0.1)
```

## 四、关节限制与安全类

### 1. 关节位置限位奖励 （dof_pos_limits）

**公式解释**:
$$r = \sum_i \max(-(\theta_i - \theta_{i,\min}), 0) + \max((\theta_i - \theta_{i,\max}), 0)$$
其中特别修改了膝关节的限制：$\theta_{3,\min} += 0.15$ 和 $\theta_{9,\min} += 0.15$

**作用**:
惩罚关节位置接近或超出限位的情况，防止关节损伤。

**缩放参数与逻辑**:
- 膝关节限制修正`0.15`: 特别提高膝关节的下限，防止膝关节过度伸展
- 使用线性惩罚: 超出限制越多，惩罚越大

**运动学逻辑**:
- 实际机器人关节有物理限制，超出可能导致机械损伤
- 特别提高膝关节下限，防止锁膝现象
- 这是一个负向奖励（惩罚），数值越小越好
- 通过 sum 累加所有关节的惩罚，确保所有关节都在安全范围内

**代码实现：**
```python
def _reward_dof_pos_limits(self):
    dof_pos_limits_ = self.dof_pos_limits.clone()
    dof_pos_limits_[3, 0] += 0.15
    dof_pos_limits_[9, 0] += 0.15
    out_of_limits = -((self.dof_pos - dof_pos_limits_[:, 0]).clip(max=0.)) # lower limit
    out_of_limits += (self.dof_pos - dof_pos_limits_[:, 1]).clip(min=0.)
    return torch.sum(out_of_limits, dim=1)
```


### 2. 关节速度限位奖励 （dof_vel_limits）

**公式解释**:
$$r = \sum_i \min(\max(|\dot{\theta}_i| - \dot{\theta}_{i,\max}, 0), 2)$$
其中特别修改了某些关节的速度限制。

**作用**:
惩罚关节速度超出限制的情况，防止高速运动导致的损伤。

**缩放参数与逻辑**:
- 特定关节速度限制调整: 将索引 4、5、10、11 的关节限制设为 4
- 最大惩罚限制`2.0`: 限制每个关节的最大惩罚值，防止单个关节问题主导总奖励

**运动学逻辑**:
- 高速关节运动可能导致控制不稳、振荡和机械损伤
- 某些关节（如足踝相关关节）需要更严格的速度限制
- 截断最大惩罚可以防止极端情况下梯度爆炸
- 这是一个负向奖励（惩罚），数值越小越好

**代码实现：**

```python
def _reward_dof_vel_limits(self):
    dof_vel_limits_ = self.dof_vel_limits.clone()
    dof_vel_limits_[4] *= 0.
    dof_vel_limits_[5] *= 0.
    dof_vel_limits_[10] *= 0.
    dof_vel_limits_[11] *= 0.
    dof_vel_limits_[4] += 4.
    dof_vel_limits_[5] += 4.
    dof_vel_limits_[10] += 4.
    dof_vel_limits_[11] += 4.
    out_of_limits = (torch.abs(self.dof_vel) - dof_vel_limits_).clip(min=0., max=2.)
    return torch.sum(out_of_limits, dim=1)
```

### 3. 碰撞惩罚 （collision）

**公式解释**:
$$r = \sum_{i \in \{penalised\}} I(\|f_{contact,i}\| > 0.1)$$

**作用**:
惩罚机器人特定部位与环境的碰撞，鼓励避开障碍物和防止跌倒。

**缩放参数与逻辑**:
- 接触力阈值`0.1`: 判定是否发生有意义碰撞的力阈值
- 惩罚值`1.0`: 每个碰撞点的惩罚值
- 被惩罚部位`self.penalised_contact_indices`: 不应与环境接触的身体部位索引

**运动学逻辑**:
- 足部以外的身体部位通常不应与环境接触
- 这种接触可能表示跌倒、碰撞或不稳定姿态
- 使用二值化判断（力大于阈值）简化碰撞检测
- 对所有被惩罚部位的碰撞计数，反映碰撞的广泛程度
- 这是一个负向奖励（惩罚），数值越小越好

**代码实现：**
```python
def _reward_collision(self):
    return torch.sum(1.*(torch.norm(self.contact_forces[:, self.penalised_contact_indices, :], dim=-1) > 0.1), dim=1)
```

### 4. 站立奖励 （stand）

**公式解释**:
$$r = 0.2 \cdot \min(\|0.5 \cdot (j_{pos} - j_{default})\|, 5) \cdot I(|v_{cmd,x}| \leq 0.1)$$

其中，膝关节 (3,9)的差异被额外乘以 4 倍加权。

**作用**:
在低速或静止状态下鼓励机器人保持标准站立姿态。

**缩放参数与逻辑**:
- 整体差异缩放`0.5`: 降低一般关节差异的影响
- 膝关节加权`4.0`: 强调膝关节位置对站立姿态的重要性
- 奖励系数`0.2`: 控制站立奖励的整体幅度
- 最大奖励限制`5.0`: 防止极端姿态导致过大惩罚
- 速度阈值`0.1`: 仅在低速命令时应用此奖励

**运动学逻辑**:
- 站立姿态是机器人的基本状态，应保持最佳的设计姿态
- 膝关节对稳定站立尤为重要，因此给予更高权重
- 只在静止或极低速状态下应用此奖励，避免与行走动作冲突
- 这是一个负向奖励（惩罚），关节偏离默认位置越多，惩罚越大
- 当机器人有明确的移动命令时，允许更大的姿态自由度以实现高效行走

**代码实现：**
```python
def _reward_stand(self):
    diff = (self.dof_pos - self.default_dof_pos)*0.5
    diff[:, [3, 9]] *= 4.
    r = 0.2 * torch.norm(diff, dim=1).clamp(0, 5.)
    reward = torch.where(torch.abs(self.commands[:, 0]) <= 0.1, r, r*0.)
    return reward
```



## 五、能量效率与平滑性类

### 1. 关节力矩奖励 （torques）

**公式解释**:
$$r = \sum_{i} \tau_i^2$$

**作用**:
惩罚高关节力矩，鼓励能量效率高的运动模式。

**缩放参数与逻辑**:
- 平方关系: 对大力矩给予更强的惩罚，鼓励使用分布式小力矩而非集中式大力矩

**运动学逻辑**:
- 最小化力矩直接关系到能量消耗，力矩平方与功率消耗有关
- 低力矩操作减轻电机负担，延长硬件寿命
- 大力矩往往导致运动急剧、抖动和不稳定
- 这是一个负向奖励（惩罚），数值越小越好
- 总力矩平方和是能量消耗的良好近似

**代码实现：**
```python
def _reward_torques(self):
    return torch.sum(torch.square(self.torques), dim=1)
```

### 2. 关节速度奖励 （dof_vel）

**公式解释**:
$$r = \sum_{i} \dot{\theta}_i^2$$

**作用**:
惩罚高关节速度，鼓励平稳、节能的运动。

**缩放参数与逻辑**:
- 平方关系: 对高速运动给予更强的惩罚，防止剧烈运动

**运动学逻辑**:
- 高关节速度通常意味着高能耗和不稳定运动
- 关节速度的平方与关节动能成正比
- 低速运动通常更平稳、更可控，也更节能
- 这是一个负向奖励（惩罚），数值越小越好
- 考虑所有关节的总体运动情况，而不仅仅关注单个关节

**代码实现：**
```python
def _reward_dof_vel(self):
    return torch.sum(torch.square(self.dof_vel), dim=1)
```

### 3. 关节加速度奖励 （dof_acc）

**公式解释**:
$$r = \sum_{i} \left(\frac{\dot{\theta}_{i,prev} - \dot{\theta}_{i,curr}}{\Delta t}\right)^2$$

**作用**:
惩罚高关节加速度，鼓励平滑的运动轨迹。

**缩放参数与逻辑**:
- 平方关系: 对高加速度给予更强的惩罚
- 时间步长`self.dt`: 用于正确计算物理加速度

**运动学逻辑**:
- 高加速度意味着突然的运动变化，可能导致以下问题:
  1. 机械振动和结构应力
  2. 控制不稳定性和过冲
  3. 能源浪费和电机过载
- 平滑的加速度轮廓有助于:
  1. 减少机械磨损
  2. 提高控制精度
  3. 降低能耗
- 这是一个负向奖励（惩罚），数值越小越好

**代码实现：**
```python
def _reward_dof_acc(self):
    return torch.sum(torch.square((self.last_dof_vel - self.dof_vel) / self.dt), dim=1)
```

### 4. 动作平滑度奖励 （action_smoothness）

**公式解释**:
$$r = \|a_{t} - a_{t-1}\|^2 + \|a_{t} + a_{t-2} - 2a_{t-1}\|^2 + 0.05 \cdot \|a_{t}\|_1$$

**作用**:
鼓励动作的平滑变化和小幅值，减少控制输入的突变和抖动。

**缩放参数与逻辑**:
- 一阶差分项`term_1`: 惩罚连续动作间的突变
- 二阶差分项`term_2`: 惩罚加速度变化 (加加速度)，促进更平滑的轨迹
- 幅值惩罚系数`0.05`: 控制对动作绝对值大小的轻微惩罚

**运动学逻辑**:
- 一阶差分确保连续时间步之间的动作变化平滑
- 二阶差分 (类似物理中的加加速度)确保加速度的变化也是平滑的
- 小幅值惩罚 (L 1 范数)鼓励使用小的控制输入，减少能耗
- 这三项结合确保:
  1. 平滑的动作轨迹
  2. 高阶平滑性 (避免急加速和急减速)
  3. 经济的控制输入
- 这是一个负向奖励（惩罚），数值越小越好

**代码实现：**

```python
def _reward_action_smoothness(self):
    term_1 = torch.sum(torch.square(self.last_actions - self.actions), dim=1)
    term_2 = torch.sum(torch.square(self.actions + self.last_last_actions - 2 * self.last_actions), dim=1)
    term_3 = 0.05 * torch.sum(torch.abs(self.actions), dim=1)
    return term_1 + term_2 + term_3
```

### 5. 足部接触力奖励 （feet_contact_forces）

**公式解释**:
$$r = \sum_{i \in \{feet\}} \min(\max(\|f_{contact,i}\| - f_{max}, 0), 600)$$

**作用**:
惩罚过大的足部接触力，防止硬着陆和不必要的冲击。

**缩放参数与逻辑**:
- 最大接触力阈值`self.cfg.rewards.max_contact_force`: 定义可接受的最大接触力
- 最大惩罚限制`600`: 防止极端情况下的过大惩罚

**运动学逻辑**:
- 过大的接触力意味着硬着陆或不稳定接触
- 大冲击力可能导致:
  1. 机器人硬件损伤
  2. 控制不稳定
  3. 能量损失
- 只惩罚超过阈值的力，小于阈值的力不受惩罚
- 使用上限截断防止极端情况下的训练不稳定
- 这是一个负向奖励（惩罚），数值越小越好

**代码实现：**
```python
def _reward_feet_contact_forces(self):
    return torch.sum((torch.norm(self.contact_forces[:, self.feet_indices, :], dim=-1) - self.cfg.rewards.max_contact_force).clip(0, 600), dim=1)
```






- **摩擦系数** [[人形机器人本体#摩擦特性]]
	- 静摩擦
	- 动态摩擦

- **减速器建模：**
	- 谐波关节：谐波关节通常具有较高的减速比，机器人通常具有较高的扭矩，但是关节的实际转速通常会较低，谐波关节利用率，通常会进行参数辨识，对库伦摩擦力的相关参数进行建模，
	- 
- **KP、KD 设置**：
	1. 悬空正弦，看什么 kpkd 参数不会引起机器的震动
	2. 悬空正弦摸到一组大概的参数后，
- **延迟**
	- L 12 的上下行延迟：5 ms；电机响应延迟：5 ms
	- 如果仿真延迟大于真机延迟：机器人可能会“过度预测”或反应过快，导致多余或不必要的动作，出现“过调节”的现象，因为仿真时延较高的情况下，策略会有意地预测和适应延迟，而在实机中这种预测会导致多余的修正
	- 如果仿真延迟小于真机延迟：这种策略可能无法及时响应环境变化，强化学习算法获得的反馈也会滞后，导致反应不足或动作延迟，导致机器人动作与反馈无法匹配，从而导致策略性能下降，尤其是在动态环境或需要快速响应的任务中。
- **Sim 2 Real Gap**：根据真机的表现和数据曲线，去调整仿真的参数（摩擦和惯量）, 使其和真机相对应，根据调整的这些参数再去重新设置训练的参数。
- **减速器内外环不对齐**：当发生较大冲击后，减速器的内外环编码器读数发生偏移。大冲击力会对减速器内部的齿轮组或轴产生扭转，使得原本对齐的内外环产生相对位移。
	- 解决方法： 
	1. 通过软件设置检测到冲击后，自动重新对齐内外环编码器。
	2. 使用双编码器（分别测量内环和外环的位置）来检测误差，并通过算法进行误差补偿。

**机器测试流程**：
- 失能状态：摆动各个关节方向，看是否各个关节方向与期望方向相对应
- 使能状态：
	- 单关节正弦测试
	- 零位站立切换
	- 悬空 RL 测试
	- 落地 RL 测试

**机器人位控和力控方案**：