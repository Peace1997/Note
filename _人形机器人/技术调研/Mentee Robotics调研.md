
# 简介

Mentee Robotics 是一家来自以色列的人形机器人企业，它在 2022 年正式成立。其开发的人形机器人名叫 MenteeBot，该机器人嵌入了人工智能大脑（LLM），可以实现从口头命令到复杂任务的完整端到端循环，包括导航、运动、场景理解、对象检测和定位、抓取以及自然语言理解。
![[Pasted image 20240912102438.png]]

**MenteeBot 参数**
- Dimensions： 175 cm 、70 kg
- Speed：1.5 m/s
- Can carry up to ： 25 kg
- DoF ： 40
- Interacts through： Voice

![[menteebot.gif]]


**应用场景：**
![[Pasted image 20240910151330.png]]

# 技术路线


![[Pasted image 20240912140546.png]]



![[Pasted image 20240912140612.png]]
- Motion Controller，该控制器是一个实时运行的 CPU，专门用于执行我们在模拟器中学习到的动作策略。
- Main Jetson：专注于处理核心感知任务，如摄像头信息的处理、机器人位置的状态估计等
- Secondary Jetson： 运行大型语言模型、语音处理、神经计算等。

# Closing the Sim-to-Real Gap with RL

## 两组实验
- 实验一：假设存在一种策略，能让机器人在现实世界中有效工作。若让机器人在现实世界中执行一系列动作，并记录每一个动作，然后将其放回初始位置，重复执行相同的动作序列。
- 实验二：假设能够让机器人在模拟器中自由行走，并记录下它所采取的一系列动作。随后，我们尝试在现实世界中复制这一动作序列。
![[Pasted image 20240912142127.png]]


第一个实验结果表明：由于物理系统中存在诸多噪声，固定的控制策略无法应对复杂多变的现实世界。
第二个实验结果表明：无论模拟器的质量如何，其内部的物理机制都无法与现实世界完美匹配，这就是所谓的“模拟与现实之间的差距”。

## Reality Gap

模拟器的工作模式，它基于当前物理世界的状态和预设的动作来预测下一个状态，这是一个将当前状态和动作映射到未来状态的过程，而现实世界，同样如此运作。
**这种“现实差距”实际上是指模拟器内部预测的下一个状态与现实世界中实际发生的状态之间的差异。**

![[Pasted image 20240911205516.png]]


## Fix the Gap

### 从模拟器角度
一种比较常见的方法就是**域随机化**方法，通过向模拟器中添加噪声来降低对模拟与现实差异的敏感度。有时，仅仅通过调整模拟器的参数来更好地匹配现实并不足够。

其他文献中提供了一些解决方案，“从模拟到现实，再回到模拟”的循环迭代方法，这些方法大多试图通过调整模拟器的参数来使其更好地匹配现实世界。
![[Pasted image 20240912103527.png]]


这种差距视为状态和动作的函数，就可以利用机器学习等方法来学习和理解这种函数关系。

> 当机器人在模拟环境中执行某个动作后，观察其在现实环境中对应的实际效果，计算出两者之间的差距，然后根据这个差距来调整机器人在类似状态下执行该动作的方式或参数。不断重复这个过程，机器人就可以逐渐学习到如何更好地适应现实环境，减小模拟与现实之间的差距，提高其在现实世界中的性能和表现。


### 从动作空间角度

他们采用了一种**学习修正策略**的方法，在准备执行某个动作时，可以通过一个网络对动作进行微调，从而缩小模拟与现实之间的差距。这样，问题就从“最小化模拟与现实差距”转化为“学习动作策略”。通过操控动作空间，以提升模拟与现实的匹配度。
![[Pasted image 20240912114617.png]]
![[Pasted image 20240912115849.png]]

- 给定特定的状态和动作输入，它会输出一个称为"c"的修正项。
- MenteeBot 不再直接采取原始动作，而是将修正项添加到原始动作中，得到一个新的动作。
- 然后，在模拟器中应用这个新动作，并通过一个能最小化模拟与现实差距的奖励函数来学习优化。
- 当 MenteeBot 学习到这个修正策略，就可以在模拟器中重新训练策略。
- 当在现实世界中应用这种新策略时，将不再需要修正项，因为现实世界本身不需要任何补偿。

学习到的补偿动作是**真机到仿真**的差距，因此在模拟器中重新训练得到的策略就是针对于真实环境进行训练的。

将现实世界中学到的内容应用在模拟器中：

![[reality_gap2.gif]]

使用监督学习来弥合模拟与现实之间的差距：
![[reality_gap3.gif]]

应用修正策略并在模拟器中播放现实世界中的数据时
![[reality_gap1.gif]]




**总结：**
- 从模拟器角度补偿，本质是修正真机到模拟器中下一状态的差异，优化的对象是模拟器本身的物理机制。
- 从动作空间角度补偿，本质是修正真机到模拟器中动作的差异，进而影响下一状态的变化，优化的对象是动作本身。