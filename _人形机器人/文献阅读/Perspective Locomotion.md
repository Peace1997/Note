
现有方法分类：
- 端到端训练方法：直接通过深度图像或点云输入训练策略（如使用 Transformer 融合感知与本体感觉），但受限于图像采样效率与仿真-现实。
- 两阶段训练方法：先训练教师策略（利用仿真中的特权信息），再通过蒸馏（如 DAgger）将知识迁移到学生策略。

# VideoMimic

本文提出一个名为 VideoMimic 的全身运动控制框架，人形机器人能够通过单目视频学习运动技能，并在真实环境中自主完成上下楼梯、坐起等复杂全身动作。

***Contribution***：
- **4D 人-环境联合重建和动作重定向**：将人体运动数据与场景数据同时从单目视频中重建，适配物理仿真器，并将人体运动重定向至人形机器人。
- ==**基于环境感知的自主任务选择运动控制策略**==：通过局部高度图（11x 11）和自身本体感知数据驱动控制策略，实现***环境感知与自主动作选择***。
- **单一泛化的策略蒸馏**：将多个任务、多视频的复杂策略蒸馏为单一统一的策略，实现真实部署的计算需求。

## Data Acquisition
从单目 RGB 视频中自动生成可供仿真环境训练使用的机器人控制参考数据。

 ***1. Preprocess：***
- **人体重建**：
	- 使用 Grounded SAM 2 进行人物检测与跨帧关联
	- 通过 VIMO 提取每帧 SMPL 模型的姿态参数、形状参数和关节位置等信
	- 通过 ViTPose 估计 2 D 关节点
	- 通过 BSTRO 完成足部接触状态的预测
- **场景重建**：
	- 通过 MegaSaM 或 Mon ST 3 R 生成每帧场景的稠密点云，这些点云未必具备真实世界尺度。
- **人体与世界坐标系的初步对齐**
	- 采用 SLAHMR 的方法，利用 SfM 预测的焦距和 2D/3D 骨骼长度比，估算相似性尺度因子，得到初步的全局轨迹。

***2. Reconstruction***
对人-场景的进行联合重构优化

**优化的变量**
- 人体的全局位姿和关节空间姿态
- 场景点云尺度

**优化目标**：
- 基于 SMPL 的高度作为形状尺度参考
- 联合优化目标：
	- 3D 关节空间人体关键点的距离
	- 2D 投影误差
	- 时间平滑正则化

**求解器**： 使用 JAX 实现 Levenberg-Marquardt 优化器

***3. Simulation Data***

**重力对齐**：使用 GeoCalib 调整坐标系，使 z 轴与重力方向对齐，满足物理引擎需求，这个旋转矩阵同时应用于人体关键点和静态场景的几何数据

**点云处理网络**：去除背景噪声，减少点云数据量，使用 NKSR 将点云表面表示成轻量级 mesh 文件（减少数据量但保留几何信息），

**动作重定向**：将优化后的人体轨迹映射到机器人模型。
- *优化变量*：全身关节角度、机体关键点位姿
- *损失函数*：
	- 位置损失；计算目标为两个相邻关节的相对距离，这样可以更好的提供结构特性
	- 姿态损失：计算目标为相邻关节的相对关节角度
	- 接触约束损失：减少足底打滑等非物理现象出现，其中接触信号由 BSTRO 模型自动估计
	- 其他的约束条件还包括：关节限位、时间平滑项（控制 base 位姿变化和角度变化）、控制膝关节 yaw 角发生非稳定姿态。

## Policy Learning 

**Observation**：
- 本体状态：过去 5 帧
	- 关节角度
	- 关节速度
	- 机体角速度
	- 重力投影
	- 上一个动作
- 目标状态：
	- 目标关节角度
	- 机体滚转/俯仰
	- 机体相对 x-y 位移和偏航角（机器人与目标）
- 局部高度图

**Rewards**
- 数据驱动奖励
	-  关节位置和关节速度差值
	- *足底接触状态误差*
- 物理可执行性限制奖励


**训练阶段**
- Stage 1: MoCap 预训练
	- 使用 LAFAN MoCap 数据与训练
	- 目的是使策略在面对视频重建噪声时具备初步技能
- Stage 2: 场景条件化跟踪
	-   加入高度图输入，提升环境适应能力
	- 基于 DeepMimic 的范式进行随机采样动作训练
- Stage 3：策略蒸馏
	- 蒸馏至去除目标关节角度、Roll/Pitch 等目标纤细，仅保留
		- 本体状态
		- 高度图
		- 机身姿态（控制指令）
	- 这样做的好处是：自主决定动作、不依赖于任务标签
- Stag 4: RL 微调
	- 在蒸馏策略的基础上，进一步进行 PPO 微调（解决去除目标观测后的）
	- 提升训练样本量，加入质量较低的参考轨迹
	- 这一过程可以有效提升实际部署的表现

***自主任务选择*** ：
在去掉了显示的任务标签后，无法通过任务本身获取当前时刻我需要做什么任务，而是以依赖三种信息来决定：
- **本体状态**：确定当前身体状态
- **局部 11 x 11 高度图**：捕获躯干周围的 1.1 m 范围内的地形特征，自动检测任务的地形
- **机身方向**：机器人被指令朝向控制朝那个方向运动，这个信息是唯一的任务驱动信息。

训练时通过大量的上下文与动作对应的样本，学会了某个高度图+本体状体+朝向组合的最优行，即**动作的隐式推理**。
- 环境的几何特征成为*行为的选择依据*
- 机身方向指令作为*运动意图*
- 过去本体感知信息描述*现在的状态以及是否接近任务完成*

因此**任务的推理被内化进策略网络**，机器人根据环境特征、本体感知和任务指令自主完成跨步走、爬楼梯和坐。
