

# PACE：基于优化的 Sim2Real 参数辨识
[Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots](https://arxiv.org/abs/2509.06342)
> ETH 2025.9

本文离线辨识 + 零样本迁移的方法对永磁同步电机进行能耗建模，采用四项简洁奖励函数，并通过自底向上的参数建模与验证来实现稳健迁移（从单个执行器到全机器人，再到跨平台）。


- **离线数据采集**：只需约 20 ～60 s 秒悬挂状态下的关节 PD 控制（chirp signal），收集关节轨迹数据。
- **参数辨识**：使用进化算法（CMA-ES）拟合一组关键参数（惯量、摩擦、死区、延迟等）来对齐仿真与真实的自由摆动轨迹。
- **策略训练与迁移**：在辨识后的仿真环境中训练 RL 策略，直接部署到实机即可 **零样本运行**，不需要在真实机器人上进行额外微调或在线学习 。
- **能效建模**：通过永磁同步电机（PMSM）的物理能耗模型，让奖励函数与真实能耗一致，避免了传统方法中复杂的手工奖励调试。

---
## 离线数据采集

通过少量、覆盖策略频率的范围的高质量信号激发系统，记录可用于辨识的关节/驱动响应。

- 固定基座：把机器人悬挂/固定避免腿之间的耦合，保证单关节/多关节动力学响应可分离记录。
- 激励信号：对所有关节同时施加 chirp 信号，频率从低到高扫过，仿真训练的频率最好覆盖到策略能激发的最高频率的一半。采用 chrip 信号作为关节输入，频率范围 0.1-10 hz，采样频率为 2.5 khz。


为降低辨识难度，使用较少的关节 PD 增益，使闭环极点落在较低频段，便于在安全激励贷款内捕获动力学特征。


>[! NOTE]
Chirp Signal 是一种连续的正弦信号，其频率会从一个较低值逐渐扫频到较高值，通过扫频覆盖控制器可能用到的运动频率范围，从而捕捉电机与机械结构的动态响应特性，系统性的测量和拟合其动力学特征。

---
## 参数辨识

通过拟合一组紧凑且有物理意义的参数，期望在仿真中得到代表真实系统的环境参数。

***（1）参数辨识的对象***
- 有效惯量
- 粘性阻尼
- 库伦摩擦
- 关节偏置
- 控制/通信延迟

***（2）优化目标：***
将记录的真实目标关节轨迹在仿真速度速率下重放，得到仿真轨迹$q_{sim}$, 并计算其和真实轨迹的$q_{real}$的均方误差。
文章利用 CMA-ES 来 **全局优化动力学参数集合**，即在候选参数空间内迭代搜索。
优化目标：
$$\theta^* = \arg \min_\theta \; \mathcal{L}(\text{sim}(\theta), \text{real})$$
其中 $\theta$ 就是要辨识的参数集（$I_a, d, \tau_f, \text{delay}, \text{bias}$ 等）。

CMA-ES 优化流程

1. **初始化参数分布**
    - 给定均值 $\mu_0$（来自 CAD/厂家数据的初始估计
    - 给定协方差矩阵 $\Sigma_0$（表示搜索范围）
2. **采样候选解**
    - CMA-ES 在每次迭代中，从 $\mathcal{N}(\mu, \Sigma)$ 采样一批参数向量 $\theta_i$
3. **仿真评估**
    - 将 $\theta_i$ 带入仿真，执行与真实机器人相同的实验轨迹
    - 计算损失 $\mathcal{L}_i$
4. **选择与更新分布**
    - 根据损失排序，选择表现最优的候选解
    - 更新均值 $\mu$ 和协方差 $\Sigma$（逐渐收缩到最优区域）
5. **迭代收敛**
    - 多轮迭代后，$\mu$ 就收敛到最佳参数 $\theta^*$。

---
***（3）单驱动辨识 (Single-drive identification)**

在单关节-单连杆的简化模型中，设置连杆惯量为 0，主要用于拟合电枢惯量、粘性阻尼和库伦摩擦。

- 把机器人拆开，只看某个关节驱动（电机+减速器+输出轴）。
- 优化的参数主要是 **电机端动力学参数**：
    - **$I_a$（电枢惯量, per-joint armature/inertia）**：反映转子、齿轮系等在电机侧看到的等效惯量。
    - **$d$（阻尼/粘性摩擦, viscous damping）**：包括齿轮、轴承摩擦、以及电磁等效阻尼。
    - **$\tau_f$（库伦摩擦力矩, coulomb friction）**：静摩擦和干摩擦效应。
    - **$T_d$（延迟, global command delay）**：信号链路和电控延时。
- 目标是捕捉 **电机–关节的最小动力学单元**。


> 采用 chrip 信号作为关节输入，频率范围 0.1-10 hz，采样频率为 2.5 khz。

**全机器人辨识：**
将机器人基座固定置于空中，以消除地面接触的影响，辨识的参数包括：关节位置偏置、通信延迟以及所有关节动力学参数。

***（4）全机器辨识 (Full-robot identification)***

- 整个机器人在完整结构、惯性耦合和地面接触下运动。
- 优化的参数不仅包含单驱动辨识里的那些，还扩展了：
    - **关节偏置 (joint bias)**：指**零位误差**，即关节编码器的零点和实际几何零点之间的差异。例如理论 0° 时，实际连杆可能已经有 1°–2° 偏差。
    - **关节动力学参数**：指 **在全机器人环境下体现出来的等效动力学特性**，包括：
        - 等效惯量（不光是电机端 $I_a$，还包含连杆 CAD 惯量、补偿效应）
        - 阻尼、摩擦
        - 电机常数、摩擦模型参数
    - 这些参数和单关节在实验台上测出来的值可能不同，因为整机耦合、补偿策略、结构振动会改变表现。


## 策略学习
将参数辨识拟合的动力学参数使仿真与真实对齐进行策略训练，训练过程中只使用四项奖励，并进行动作限制保护；在训练过程中不做动力学随机化，仅随机化任务扰动和地形以增强策略对场景的鲁棒性。

***（1）奖励描述***：
- **速度跟踪**$r_v$：鼓励机器人基体速度接近给定的速度
- **能量消耗** $r_e$：基于对 PMSM 电机的物理能耗进行建模，结合电气耗散与机械功率以及重力势能项进行
- **碰撞惩罚** $r_c$：惩罚关节越界以及腿部与环境碰撞的场景
- **足底接触速度惩罚** $r_{ftd}$ ：在脚着地瞬间，惩罚那段短的历史窗口内脚的最大速度（以避免通过强冲击制动产生磨损/噪声）


***（2）惩罚/熵调度***
- **惩罚调度**：训练初期大惩罚会阻碍策略的探索，因此论文采用指数衰减型$k_{decay}=e^{-\lambda t}$，$k=1-k_{decay}$的奖励函数调度，早期能量消耗会被弱化，策略先学会跟踪，再进行能量/冲击约束。
$$
r = c_v r_v + c_cr_c+k(c_er_e+ c_{ftd}r_{ftd})
$$

- **熵调度**：探索对早期有益但不利于后期的精调，因此采用平滑 Tanh 类的调度将熵系数从初始值$E_0$逐渐退火到$E_{\infty}$

$$
\varepsilon(t) =  \varepsilon_{\infty} + \epsilon (\varepsilon_0-\varepsilon_{\infty})， \epsilon=\frac{1}{2} -\frac{1}{2}\text{tanh}(\eta(t-T_{\varepsilon}))
$$
- $\varepsilon_0$：初始熵系数（鼓励探索）
- $\varepsilon_{\infty}$：最终熵系数（较小，保证收敛）
- $T_{\varepsilon}$：调度的中点，当$t=T_{\varepsilon}$附近，tanh=0，正好位于过渡的中心点
	- $t \gg T_{\varepsilon}$ ,tanh $approx$ 1, 因此$\varepsilon(t) \approx E_{\infty}$
	-  $t \ll T_{\varepsilon}$ ,tanh approx 1, 因此$\varepsilon(t) \approx E_0$
- $\eta$：平滑因子，控制过渡的陡峭程度



现有方法分类：
- 端到端训练方法：直接通过深度图像或点云输入训练策略（如使用 Transformer 融合感知与本体感觉），但受限于图像采样效率与仿真-现实。
- 两阶段训练方法：先训练教师策略（利用仿真中的特权信息），再通过蒸馏（如 DAgger）将知识迁移到学生策略。

# VideoMimic：视频重定向与自主任务决策

本文提出一个名为 VideoMimic 的全身运动控制框架，人形机器人能够通过单目视频学习运动技能，并在真实环境中自主完成上下楼梯、坐起等复杂全身动作。

***Contribution***：
- **4 D 人-环境联合重建和动作重定向**：将人体运动数据与场景数据同时从单目视频中重建，适配物理仿真器，并将人体运动重定向至人形机器人。
- ==**基于环境感知的自主任务选择运动控制策略**==：通过局部高度图（11 x 11）和自身本体感知数据驱动控制策略，实现***环境感知与自主动作选择***。
- **单一泛化的策略蒸馏**：将多个任务、多视频的复杂策略蒸馏为单一统一的策略，实现真实部署的计算需求。

## Data Acquisition
从单目 RGB 视频中自动生成可供仿真环境训练使用的机器人控制参考数据。

 ***1. Preprocess：***
- **人体重建**：
	- 使用 Grounded SAM 2 进行人物检测与跨帧关联
	- 通过 VIMO 提取每帧 SMPL 模型的姿态参数、形状参数和关节位置等信
	- 通过 ViTPose 估计 2 D 关节点
	- 通过 BSTRO 完成足部接触状态的预测
- **场景重建**：
	- 通过 MegaSaM 或 MonST 3 R 生成每帧场景的稠密点云，这些点云未必具备真实世界尺度。
- **人体与世界坐标系的初步对齐**
	- 采用 SLAHMR 的方法，利用 SfM 预测的焦距和 2 D/3 D 骨骼长度比，估算相似性尺度因子，得到初步的全局轨迹。

***2. Reconstruction***
对人-场景的进行联合重构

**优化的变量**
- 人体的全局位姿和关节空间姿态
- 场景点云尺度

**优化目标**：
- 基于 SMPL 的高度作为形状尺度参考
- 联合优化目标：
	- 3 D 关节空间人体关键点的距离
	- 2 D 投影误差
	- 时间平滑正则化

**求解器**： 使用 JAX 实现 Levenberg-Marquardt 优化器

***3. Simulation Data***

**重力对齐**：使用 GeoCalib 调整坐标系，使 z 轴与重力方向对齐，满足物理引擎需求，这个旋转矩阵同时应用于人体关键点和静态场景的几何数据

**点云处理网络**：去除背景噪声，减少点云数据量，使用 NKSR 将点云表面表示成轻量级 mesh 文件（减少数据量但保留几何信息），

**动作重定向**：将优化后的人体轨迹映射到机器人模型。
- *优化变量*：全身关节角度、机体关键点位姿
- *损失函数*：
	- 位置损失；计算目标为两个相邻关节的相对距离，这样可以更好的提供结构特性
	- 姿态损失：计算目标为相邻关节的相对关节角度
	- 接触约束损失：减少足底打滑等非物理现象出现，其中接触信号由 BSTRO 模型自动估计
	- 其他的约束条件还包括：关节限位、时间平滑项（控制 base 位姿变化和角度变化）、控制膝关节 yaw 角发生非稳定姿态。

## Policy Learning 

**Observation**：
- 本体状态：过去 5 帧
	- 关节角度
	- 关节速度
	- 机体角速度
	- 重力投影
	- 上一个动作
- 目标状态：
	- 目标关节角度
	- 机体滚转/俯仰
	- 机体相对 x-y 位移和偏航角（机器人与目标）
- 局部高度图

**Rewards**
- 数据驱动奖励
	-  关节位置和关节速度差值
	- *足底接触状态误差*
- 物理可执行性限制奖励


**训练阶段**
- Stage 1: MoCap 预训练
	- 使用 LAFAN MoCap 数据与训练
	- 目的是使策略在面对视频重建噪声时具备初步技能
- Stage 2: 场景条件化跟踪
	-   加入高度图输入，提升环境适应能力
	- 基于 DeepMimic 的范式进行随机采样动作训练
- Stage 3：策略蒸馏
	- 蒸馏至去除目标关节角度、Roll/Pitch 等目标纤细，仅保留
		- 本体状态
		- 高度图
		- 机身姿态（控制指令）
	- 这样做的好处是：自主决定动作、不依赖于任务标签
- Stag 4: RL 微调
	- 在蒸馏策略的基础上，进一步进行 PPO 微调（解决去除目标观测后的）
	- 提升训练样本量，加入质量较低的参考轨迹
	- 这一过程可以有效提升实际部署的表现

***自主任务选择*** ：
在去掉了显示的任务标签后，无法通过任务本身获取当前时刻我需要做什么任务，而是以依赖三种信息来决定：
- **本体状态**：确定当前身体状态
- **局部 11 x 11 高度图**：捕获躯干周围的 1.1 m 范围内的地形特征，自动检测任务的地形
- **机身方向**：机器人被指令朝向控制朝那个方向运动，这个信息是唯一的任务驱动信息。

训练时通过大量的上下文与动作对应的样本，学会了某个高度图+本体状体+朝向组合的最优行，即**动作的隐式推理**。
- 环境的几何特征成为*行为的选择依据*
- 机身方向指令作为*运动意图*
- 过去本体感知信息描述*现在的状态以及是否接近任务完成*

因此**任务的推理被内化进策略网络**，机器人根据环境特征、本体感知和任务指令自主完成跨步走、爬楼梯和坐。




# Trinity：分层大模块化运动控制

***Introduction***
Trinity 系统采用模块化层次结构设计，将人形机器人控制的复杂问题分解为不同层次，主要由三个核心技术模块组成：

1. **强化学习 (Reinforcement Learning, RL)模块**：负责低层次运动控制
2. **视觉语言模型 (Visual Language Model, VLM)模块**：负责环境感知和场景理解
3. **大型语言模型 (Large Language Model, LLM)模块**：负责任务理解和规划

![[Pasted image 20250322113004.png]]

这种模块化设计允许各组件独立优化，同时协同工作，提高系统整体性能和适应性。

```
用户指令（Human Prompt） +  VLM(场景理解) → LLM(任务理解) → 任务规划（Task Planner） → RL Policy 、 Arm Planner 、 Hand Controller → 机器人执行
```

通过这种集成方式，Trinity 能够：
- 将高层次的自然语言指令转化为具体的运动计划
- 实时感知环境变化并调整运动策略
- 在复杂环境中执行多样化任务

***RL***
采用了对抗性运动先验 (Adversarial Motion Priors, AMP)技术，使机器人执行更自然、人类化的动作。

***VLM***
采用 **ManipVQA** 作为 VLM 框架，结合了视觉编码器和语言编码器，实现视觉问答和环境理解：
- **输入**：相机捕获的图像/视频流（I）、用户语言查询（Q）
- **输出**：场景理解结果、物体属性描述、空间关系判断
$$
P(R|I,Q) = f_{VLM}(I,Q)
$$

1. **视觉编码器**：
    - 处理 RGB 图像和深度信息
    - 提取物体特征、空间关系和场景结构
2. **语言编码器**：
    - 处理文本查询和指令
    - 生成语义嵌入表示
3. **多模态融合结构**：
    - 交叉注意力机制融合视觉和语言特征
    - 生成综合环境认知表示

***LLM***
LLM 模块作为 Trinity 系统的中央规划组件，负责将 VLM 的感知结果与用户指令相结合，生成完整的任务执行计划：
- **输入**：用户自然语言指令 (Human Prompt)、VLM 的环境感知结果（P）
- **输出**：具体的技能执行序列、高层任务规划 （T）



LLM 规划模块将高层任务指令翻译成具体的执行步骤：
1. **指令理解**：分析自然语言指令，识别核心目标和约束
2. **感知整合**：结合 VLM 提供的环境信息进行规划
3. **技能选择**：从技能库中选择合适的基本动作
4. **执行监控**：监控执行过程，必要时进行重规划

**【Human Prompt】**
- Task description
- Skill library
- Workspace limitations
- Safety constraints
- Prior kinematic knowledge (ref: [Kinematic-aware prompting for generalizable articulated object manipulation with llms](https://arxiv.org/abs/2311.02847))

（Kinematic-aware Prompting）：将关节物体的运动学先验信息（. URDF）融入到语言模型（LLM）的提示中，使模型在生成的动作符合机器人的运动学逻辑。



*技能库设计*：
- 手臂技能：抓取、放置、推拉等
- 手部技能：精细抓取、按压、旋转等
- 身体技能：行走、蹲下、转身等
*规划算法*：
- 基于技能库进行层次化任务分解
- 考虑工作空间约束和安全准则


# RLPF
`RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control` -2025.6
> 北京大学

Text-to-motion
