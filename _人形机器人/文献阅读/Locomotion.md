
# Humanoid-Gym: 人形机器人零样本迁移
#zero-shot #Nvidia-Isaac-Gym  
Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim 2 Real Transfer - 基于强化学习的人形机器人零样本迁移

2024年3月5日，星动纪元联合清华大学、上海期智研究院开源了人形机器人强化学习训练框架Humanoid-Gym。

***论文亮点：***
- 开源了人形机器人端到端强化学习训练框架：Humanoid-Gym
- 在 Nvidia Isaac Gym 环境设计不同地形并增加动态随机域
- 对 Mujoco 环境进行精细化校准使其接近真实环境




# Concurrent Training ：并行训练控制策略和状态估计器
**Concurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion —— 为动态和稳健的腿部运动同时训练控制策略和状态估计器**

#sim2real #ppo


***主要思想：***
该框架包含两个网络结构：策略网络结构和状态估计网络。其中，策略网络结构输出关节期望位置；状态估计网络用于输出机器人状态信息。
该在强化学习 AC 框架基础上，增加**历史状态信息**，通过一个状态估计网络对机器人的线速度、脚的高度和接触概率进行估计。并将状态估计信息输入到控制策略产生最后的动作，该状态估计器与控制策略同时进行训练。

![[Pasted image 20240419171922.png]]

***论文亮点***
由于本体的状态信息对机器人运动控制是计较重要了，为实现 sim2real，增加历史状态信息和状态估计网络，训练完成后保存状态估计网络模型和策略网络模型。经验证，该方法可以有效实现 sim2real

***其他技巧***
- 动态随机化：动态随机化对于 sim 2 real 也是有很大帮助的，其中包括
	- Observation noise：观测噪声是在训练过程中添加的。例如，在真实机器人上，关节速度的测量来自于关节位置的数值微分，会导致关节速度观测到误差
	- Motor frictions: 为不同部位的执行器增加不同的噪声
	- PD controller gains: 我们分别为位置增益 P 和速度增益 D 添加均匀噪声，以减轻电机摩擦和阻尼的影响。
	- Foot positions and collision geometry：足部位置观测和碰撞几何形状随机化，可以减少测量误差和橡胶脚变形的影响。
	- Ground friction：地面摩擦力随机化，可以使机器人在低摩擦力的湿滑路面和高摩擦力的沥青路面进行运行。

# DreamWaQ: 仅依靠本体感知学习地形和机身状态

**Abstract**：
DreamWaQ（Dream Walking for Quadrupedal Robots）是一种四足机器人运动框架。它采用**非对称 Actor-Critic** 架构来学习隐式地学习地形的信息，通过**上下文辅助估计器网络** **CENet (context-aided estimator network)** 来估计机器人的身体状态。
![[Pasted image 20241030182518.png|500]]

**Contribution：**
- **非对称 Actor-Critic**：不需要外部传感器感知环境信息，利用架构，使机器人能够仅通过本体感知推断地形属性，如高度图、摩擦力、恢复系数和障碍物。
- **上下文辅助估计器网络（Context-Aided Estimator Network）**：联合学习估计机器人状态和推断环境的潜在表示，提高状态估计的准确性。
- **自适应引导策略（AdaBoot）**：根据训练过程中奖励的变异系数 (CV) 来自适应地调整引导的概率。在 AdaBoot 中，引导指的是使用 CENet 估计的身体状态和环境上下文信息，而不是使用模拟器提供的真实值。
> 引导是指使用估计值代替真实值来训练策略网络

-------------
**1. 非对称 Actor-Critic**

策略网络 (Actor) 接收时间序列的部分观测值 $o^H_t$ 作为输入，而价值网络 (Critic) 则接收完整的状态信息 $s_t$ 作为输入。2 这种非对称的输入信息使得策略网络能够在训练过程中隐式地学习推断环境信息，从而在没有外部传感器的情况下适应各种地形。

----------------
**2. 上下文辅助估计器网络 CENet**

CENet 由一个单编码器和多头解码器架构组成。
- **编码器**： 将时间序列的观测数据$O^H_t$ 编码为机器速度速度指令和上下文向量$Z_t$。
- **多头解码器**： 
	- 估计机体线速度$V_t$
	- 重建下一时间步的$Z_t$

**网络结构：**
- **共享编码器**： 身体速度估计模型和自动编码器模型共享一个统一的编码器，简化了网络架构，并在推理过程中同步运行。
- **自动编码器模型**：使用 $\beta-\text{VAE}$ 作为自动编码器架构


**优势**：
- 联合学习正向和反向动力学：
	- 估计机器人的身体线速度 $v_t$，这可以被视为学习机器人的**正向动力学**，即根据当前状态和控制输入预测未来的状态。
![[Pasted image 20241101114530.png]]


>机器人正向动力学是指根据给定的关节位置、速度和加速度等输入，计算机器人末端执行器的位置、速度和加速度等输出。
>机器人反向动力学则是指根据给定的末端执行器位置、速度和加速度等输入，计算机器人各关节的位置、速度和加速度等输出。








# NMPC-DCBFRL： 无模型的控制与有模型安全性导航
**Bridging Model-based Safety and Model-free  Reinforcement Learning through System  Identification of Low Dimensional Linear Models -- 通过低维线性模型的系统辨识将基于模型的安全性和无模型强化学习相结合**


 #System-Identification  #model-base #model-free

***主要思想：***
这篇论文通过系统辨识的方法对强化学习控制的动态非线性的闭环系统建模为低维线性模型，并在该线性模型基础上应用基于模型的方法（NMPC-DCBF）进行局部规划，确保无模型强化学习运动控制系统的安全性。

![[Pasted image 20240814113656.png|425]]
![[Pasted image 20240814143226.png|450]]

***论文亮点：***
- 提供了一种有模型算法和无模型强化学习算法相结合的方式
- 将整个无模型强化学习算法闭环系统通过系统辨识建模为一个线性模型，有助于上层基于模型的算法进行安全性规划

***评价：***
- 这个线性模型的是需要有条件的，
	1. 需要有一个较好的 RL 策略
	2. 步态频率（输入频率）不能超过某个阈值（文中为：0.6hz），当步态频率超过0.6 Hz 时,Cassie 机器人的系统就会表现出明显的非线性特性。



# ASE: 












# WoCoCo: 序列化接触全身运动控制

**主要思想**
论文介绍了一种名为 WoCoCo (Whole-Body Control with Sequential Contacts)的全新框架，旨在利用**强化学习**方法对具有**序列式接触的人形机器人进行全身运动控制**
![[Pasted image 20241011154144.png]]

---------------
**创新点：**
- **基于接触阶段的任务分解：**
    - 定义了接触目标和任务目标：接触目标规定了机器人哪些部位需要与环境发生接触，而任务目标则规定了在完成接触目标的前提下，机器人还需要满足哪些额外条件。
    - 优势：将原本复杂的长视野任务分解成多个相对简单的短视野子任务，从而降低学习优难度并提高学习效率。
- **好奇心奖励**：
    - WoCoCo 提出一种基于哈希的随机神经网络 (random neural network based hash) 的方法，通过在状态空间上进行**计数**来衡量状态的新颖性。

------------
**任务建模**
![[Pasted image 20241011155401.png]]
- 任务无关的奖励：在不同的任务场景中可以共用，第一项奖励 $r_{WoCoCo}$ 主要包括三项：接触奖励、阶段奖励、好奇心奖励。
![[Pasted image 20241011155621.png|500]]
	- 接触奖励：将序列化任务分解后完成每个接触任务的奖励（密集型）
	- 阶段奖励：累计完成的接触任务越多获得的奖励越大（离散型）
	- 好奇心奖励：鼓励机器人对新状态进行探索
- 任务相关的奖励：不同的任务场景（跑酷、搬运、舞蹈、攀岩），需要针对性的调整几项任务奖励。

-------------------- 
**接触任务分解：**
任务分解的依据是机器人需要按顺序完成这些阶段以完成整个任务。接触阶段的定义取决于机器人与环境交互所需的特定接触序列。接触任务主要指的是**末端执行器**与环境的接触，而不是关节层面的接触。

例如：
- 在跑酷跳跃任务中，每个垫脚石都对应一个接触阶段，其中实现正确的脚接触定义了接触目标，而保持上半身姿势则构成了任务目标。
- 在箱子移动任务中，第一个接触阶段的目标是将手放在箱子的两侧，而第二个接触阶段的目标是保持手与箱子侧面的接触，并将箱子运送到目的地附近。

------------------------
**好奇心奖励**：

![[Pasted image 20241014212226.png]]
![[Pasted image 20241014212240.png]]
![[Pasted image 20241014212343.png]]

 通过固定神经网络，可以将输入状态数据映射到一个 Hash Bucket （5）。好奇心奖励基于机器人访问同一索引桶（ID of bucket）中的状态进行计数，访问的次数越多，奖励越低（6），从而鼓励机器人探索新的状态。

-----

**Sim2Real**
- **课程学习**：
    - 训练的第一阶段：不采用域随机化的方法，并加入好奇心奖励
    - 训练的第二阶段：加入域随机化直至模型收敛
    - 训练的第三阶段：每 2000 次迭代将正则化奖励项的权重增加 20%，直到权重翻倍，从而引发更保守的行为。
- **正则化奖励**：用于对机器人的某些行为进行惩罚，来引导机器人学习更安全、更自然的动作。
![[Pasted image 20241011155321.png]]



# CoMOPPO : 一种受约束的多目标强化学习方法

**Abstract**
本文提出一种受约束的多目标强化学习算法 Constrained multi-objective PPO (CoMOPPO)，在任务层面，通过将任务分解，并设计阶段性奖励，实现复杂任务；在算法层面，本文提出一种新的优势函数（考虑奖励优势函数和成本优势函数），并通过奖励标准化和优势函数标准化，提高训练的稳定性。

***创新点：***
- 阶段性奖励
- 受约束的多目标强化学习策略

***问题建模：***
$$
\begin{gathered}
\max _\pi J_{R_i}(\pi) \forall i \in\{1, \ldots, N\} \\
\text { s.t. } J_{C_j}(\pi) \leq d_j /(1-\gamma) \forall j \in\{1, \ldots, M\}
\end{gathered}
$$
where $J_{R_i}(\pi):=\mathbb{E}_{\tau \sim \pi}\left[\sum_t \gamma^t R_i\left(s_t, a_t, s_{t+1}\right)\right]$, and $d_j$ is a threshold of the $j$ th constraint. 
The target of the CMORL problem finds a **constrained-Pareto (CP) optimal policy**.

> Pareto 最优策略是指在满足所有约束条件的情况下，无法在所有目标上都找到更优策略


***阶段性奖励：***
替换传统的权重加权方法，最大化与每个奖励项相对应的多个目标，并满足与安全相关项相对应的约束。
**难点**：对于需要顺序执行的动作序列，有时需要动态调整奖励权重。
**解决**：将任务分段，函数，并对奖励和成本函数
下图为设定（人为设定）的分阶段的后空翻任务：
![[Pasted image 20241118205108.png|425]]

***受约束的多目标强化学习策略***
**优势函数**：
- **奖励优势函数$A_{R_i}^{\pi_t}$:** 鼓励智能体采取能获得更多奖励的动作，引导智能体学习最佳策略，以最大化累积奖励。
- **成本优势函数$A_{C_j}^{\pi_t}$:** 惩罚智能体采取会导致更多成本的动作，限制智能体的行为，使其在追求奖励的同时避免违反约束条件。

$$
A^{\pi_t}(s, a):=\sum_i \nu_{t, i} A_{R_i}^{\pi_t}(s, a)-\sum_j \lambda_{t, j} A_{C_j}^{\pi_t}(s, a)
$$
其中$\nu$和$\lambda$是对应优势函数的权重，为了更好的体现权重对优势函数的影响，因此需要保持两个优势函数的量纲的一致性，因此提出了奖励标准化和

**奖励标准化**：由于设置中每个奖励函数的尺度不同，需要将其调至相同量纲水平，因此对每个奖励和阶段进行奖励归一化，并使用归一化的奖励训练价值函数。
>奖励标准化是为了解决不同奖励函数之间尺度差异的问题，使其在计算总奖励时贡献相对均衡。**针对的对象是同一类型的奖励**


**优势函数标准化**：即使对奖励进行了归一化，奖励和成本的优势函数仍然可能具有不同的尺度。如果不进行标准化，策略更新可能会过度偏向于最大化目标而忽略约束条件，导致训练过程不稳定。
> 优势函数标准化是为了解决奖励优势函数和成本优势函数之间比例差异的问题，确保策略更新时既能追求奖励最大化，又能有效地满足约束条件。 **针对的对象是不同类型的奖励。**

$$
A^\pi=\frac{A_R^\pi}{\operatorname{Std}\left[A_R^\pi\right]}-\eta \sum_j \frac{A_{C_j}^\pi}{\operatorname{Std}\left[A_{C_j}^\pi\right]} \mathbf{1}_{\left(J_{C_i}(\pi)>d_i\right)},
$$
通过指示函数 ($J_{C_i}(π) > d_i$)，只有当约束条件被满足时，即当前存在较大的惩罚项，才会将成本的优势函数计入聚合优势函数中， $J_{C_i}(π)$ 为累计期望回报。


***Sim 2 Real 方法***
- **Domain randomization**：motor strength and offset, gravity, friction, restitution, noise in joint positions and velocities, and base orientation.
- **Teacher-Students Learning**：在训练过程中，教师策略和学生策略的动作以固定的周期与环境进行交互，这样可以更有益于减少学生策略与教师-学生策略分布的差异。



# CTS: 



# HIL-SERL: 人机协同强化学习
[[RLPD]]

论文介绍了一种名为 HIL-SERL 的机器人操控系统，该系统利用**人机协同强化学习**来训练机器人完成各种精细且灵巧的操控任务。该系统通过整合**人类演示和纠正**、**强化学习算法**以及**系统级选择**，提升机器人的训练速度和复杂。


创新点：
- 加入人类演示数据和人为纠正数据，人类直接对操作进行纠正可以有效提高学习成本，尤其是从头学习的任务。
- 

![[Pasted image 20241209201252.png]]





SERL

# HumanPlus：全身实时遥操作

***Abstract***
本文介绍了一种名为 HumanPlus 的完整全身遥操作系统，旨在让人形机器人从人类数据中学习运动和自主技能。该系统利用人类的运动数据训练**低级控制策略**，使机器人能够**实时跟随人类的全身和手部动作**（即 Humanoid Shadowing and Imitation）。通过这种阴影模仿，操作人员可以遥控机器人收集真实世界中各种任务的数据。随后，使用收集到的数据进行**监督行为克隆**，训练基于**第一视觉的技能策略**，从而使机器人能够自主完成任务。


![[Pasted image 20241229111220.png]]

***Contribution***：
- **利用大规模人类数据学习阴影模仿**：利用大规模人类运动数据集（AMASS），通过人体姿态估计和重定向以及强化学习算法训练低级控制策略（即 Humanoid Shadowing Transformer）。通过低层控制策略可以收集用于技能学习的数据。
- **利用模仿学习学习复杂自主技能：** 结合动作预测和前向动力学预测的 Transformer 架构，高效地从少量演示中学习复杂的自主技能。
- **低成本、实时全身遥操作：** 利用单个 RGB 摄像头即可实现对人形机器人的全身控制，无需昂贵的动作捕捉设备。

***Shadowing of Human Motion***
实时捕捉人类的动作并将其转换为人形机器人动作，该过程可以为收集用于学习自主技能的数据。完整流程如下：

![[Pasted image 20241230111731.png]]

**1. 人体姿态估计**： 使用相机捕获人类运动并转换为人类机体和手部的姿态估计。
- *动作捕捉*：使用单个 RGB 相机捕捉人类的运动 （60 hz）
- *人体姿态估计*：利用 **WHAM**（World-Grounded Humans with Accurate Motion）算法，实时估计人体姿态和全局变换。其中 WHAM 算法使用**SMPL-X 模型**进行人体姿态参数化。 （25 hz）
- *手部姿态估计*：利用 HaMeR 算法，实时估计手部姿态。其中 HaMeR 算法使用 MANO 模型进行手部姿态参数化。

**2. 姿态重定向**：将估计的人类机体和手部的姿态重定向到人形机器人中
- *人体姿态重定向*：将 SMPL-X 模型中的欧拉角映射到人形机器人模型中。
- *手部姿态重定向*：将每个手指中间关节的旋转欧拉角映射到人形机器人手部，并计算前臂和手部全局方向的相对旋转的到手腕角度。

**3. 低级控制策略**：使用 Humanoid Shadowing Transformer 策略控制人形机器人
-  *策略结构*：该策略是一个仅**解码器的 Transformer**，输入包括人形机器人的**本体感知**和**目标姿态**，输出是人形机器人**目标关节位置**。使用 PPO 算法在模拟环境中训练该策略（50hz）
- *输入*：
	- 本体感知：如基体状态、关节位置、关节速度和上次动作
	- 目标姿态：通过重定向人类姿态得到的，来自于预先处理的 AMASS 数据集。数据如目标前进和横向速度、目标横滚和俯仰角、目标偏航速度和目标关节角度）
- *输出*：对于身体关节使用一个 1000Hz 的 PD 控制器转换为力矩；对于手部的目标关节直接穿传递给 PD 控制器。

**4. 数据收集**：利用低层控制策略收集用于技能学习的数据
- 通过低级控制策略，人形机器人能够实时跟随人类的动作，人形机器人通过其头部安装的双目 RGB 相机收集以自我为中心的视觉数据，这些数据用于后续的技能模仿学习。

**算法优势**：
- **全身控制**：能够控制人形机器人的每一个关节，实现全身运动和操作。
- **低成本**：仅需一个 RGB 相机，无需昂贵的动作捕捉系统或 VR 设备，只需一个操作员通过视觉观察即可进行控制，无需专业技能。
- **实时性**：高帧率的姿态估计和重定向，以及快速响应的低级控制策略，保证了系统的实时性。
- **任务无关性**：通过强化学习训练的低级策略具有良好的鲁棒性，适配于各种任务，包括快速运动、重物操作以及复杂环境下的运动。


***Imitation of Human Skills***
通过利用实时的 Shadowing of Human Motion 获取真实世界数据，并结合改进的 Transformer 模型 HIT，使人形机器人能够高效地学习复杂的自主技能，并展现出良好的鲁棒性和泛化能力。

**数据来源**：
- *数据*：模仿学习的数据来自通过阴影模仿系统（Shadowing of Human Motion）收集的真实世界数据。具体来说，人类操作员通过遥操作机器人执行各种任务，机器人在此过程中记录其**自身的视觉**（通过双目 RGB 相机）和**本体感知数据**。
- *优点*：这种方式避免了在模拟环境中进行复杂场景和软物体建模的需求，直**接从真实世界获取数据**，弥补了模拟环境与现实之间的差距。


**模仿学习算法**：
- *架构*：系统采用了 **Humanoid Imitation Transformer** (HIT)，这是一个专门为模仿学习设计的 decoder-only Transformer 模型。 HIT 建立在 Action Chunking Transformer 的基础上进行了改进。
- *输入*： 两个以自我中心 RGB 相机的图像特征、本体感知信息和固定的位置嵌入作为输入，仅使用解码器进行操作。它根据输入的固定位置嵌入，预测一系列的目标姿态
- *输出*：目标姿态的预测，还包括与未来状态对应的图像特征预测。
- *学习*： HIT 使用监督行为克隆方法进行训练，通过模仿人类演示来学习技能策略。通过在图像特征空间上引入 **L 2 特征损失**，HIT 被迫预测与实际目标姿态序列执行后对应的图像特征标记，从而有效地融合了目标姿态预测和前向动力学预测
- *部署*：在部署时，HIT以25Hz的频率在机器人上运行，将预测的目标姿势异步地发送给低级 Humanoid Shadowing Transformer5。预测的未来图像特征标记在部署时被丢弃

**算法优势**： 
- **高效学习**：HIT 只需要少量的人类演示（最多 40 个）即可学习复杂的技能，成功率在 60-100% 之间。
- **结合视觉信息**： HIT 利用双目 RGB 相机提供的视觉信息，避免了单目视觉的深度信息缺失问题，从而提高了操作的精确性。
- **避免过拟合：** 通过前向动力学预测和 L2特征损失，HIT 避免了过度依赖本体感受，提高了泛化能力。前向动力学预测方法通过规范化图像特征空间来增强性能，并防止基于视觉的技能策略忽略图像特征并过度拟合本体感知。
-   **Transformer 架构**：可以有效地学习长序列数据，并具备预测未来状态的能力

# Basic Motion Framework

## A-RMA: 双足机器人的快速运动适应
`Adapting Rapid Motor Adaptation for Bipedal Robots` - 2022
> University of California Berkeley && Carnegie Mellon University

本文提出了一种名为 A-RMA 的控制策略，该策略将 RMA 扩展至双足机器人，通过三阶段训练过程，旨在解决双足机器人在复杂环境中动态行走的问题。

***First-Phase: Base Policy Training***
- **目标**：训练一个能够适应不同环境的基础策略，该策略能够外部向量（extrinsics vector）编码环境信息（如摩擦、质量分布等），以实现快速适应。
- **方法**：通过模仿参考动作初始化策略，随后逐渐降低对参考动作的依赖
	- 


***Second-Phase: Adaptation Module Training***
**目标**：训练一个适应模块，基于本体感知历史（proprioceptive history） 在线估计外部向量，以实现对外部向量的估计。


***Third-Phase: Base Policy Finetuning***
**目标**：解决使用模块中估计外部向量不完美的的问题，通过微调基础策略，使其适应



## AMP：加入参考运动步态进行对抗性模型学习训练

***Abstract***：
论文提出一种对抗性运动先验（Adversarial Motion Priors，AMP）的框架，指导物理模拟角色的应该执行怎样的任务，以及以什么样的运动风格执行该任务。**AMP 算法能够在完成高级任务目标的同时，模仿数据集中的运动风格**。该框架利用**对抗性模仿学习**（Adversarial Imitation Learning.），通过无结构的运动数据样本训练一个对抗性判别器，作为运动风格的先验，指导**强化学习算法**训练物理模拟角色完成各种任务。
![[Pasted image 20241130114644.png|500]]

***Contribution***

 **1. 对抗性运动先验架构**：该方法将目标条件强化学习与生成对抗模仿学习方法相结合，使角色能够从大型非结构化运动数据集中学习，鼓励角色在执行高级目标任务（期望速度、期望目标点）时采用类似于数据集中运动的行为。
> 该框架不需要像传统的动作捕捉技术那样手动设计跟踪目标或进行基于相位的运动同步

**2. 稳定对抗性训练的技巧**
	- **从观察中模仿**（Imitation from Observations）： 将 GAIL 扩展到只有状态可观察的设置，允许使用仅包含状态信息的运动片段进行训练，无需提供专家动作。
	- **最小二乘判别器**（Least-Squares Discriminator）： 采用最小二乘 GAN（LSGAN）的损失函数，以解决标准 GAN 目标中梯度消失的问题，从而提高训练的稳定性和结果的质量 。
	- **判别器观测**（Discriminator Observations）： 精心设计判别器使用的特征集，以提供对确定给定运动特征至关重要的信息，从而为策略提供有效的反馈
	- **梯度惩罚**（Gradient Penalty）： 使用梯度惩罚来减轻对抗性训练过程中的不稳定性，从而提高训练的稳定性和性能。

______
***Overview of AMP***
AMP 算法通过结合**目标条件强化学习**（Goal-Conditioned Reinforcement Learning ）和**生成对抗模仿学习** （Generative Adversarial Imitation Learning） 的方法，用于从大型非结构化运动数据集中学习运动先验，并控制角色的行为风格。

- **判别器**：用于预测输入的状态转换是来自真实运动数据集的 "真实" 样本，还是由角色策略生成的 "虚假" 样本。
	- 输入：是连续的状态转换 ($s_t, s_{t+1}$)，描述了角色在两个连续时间步长的运动。
	- 输出：策略网络生成的数据与真实数据集的数据的相似性评分。

- **生成器**：由强化学习算法构成，结合了任务奖励和风格奖励，使用近端策略优化 (PPO) 算法进行训练。

- **奖励函数**：任务奖励 $r^G$ 和风格奖励 $r^S$ 的线性组合
	- 任务奖励$r^G$ ： 特定于任务，用于定义角色应该满足的高级目标
	- 风格奖励$r^S$：由判别器提供，用于鼓励角色产生类似于数据集中运动的运动
$$
r\left(\mathrm{~s}_t, \mathrm{a}_t, \mathrm{~s}_{t+1}, \mathrm{~g}\right)=w^G r^G\left(\mathrm{~s}_t, \mathrm{a}_t, \mathrm{~s}_t, \mathrm{~g}\right)+w^S r^S\left(\mathrm{~s}_t, \mathrm{~s}_{t+1}\right) .
$$

_________

***1. Imitation from Observations***
GAIL 通常需要状态-动作对作为输入，由于采集的运动片段数据集并未提供状态迁移的动作，因此 AMP 只需要状态转换 $(s_t, s_{t+1})$ 作为输入。这使得 AMP 可以直接从运动片段数据中学习，而无需访问专家的动作信息。

$$
\underset{D}{\arg \min }-\mathbb{E}_{d^{\mathcal{M}}\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\log \left(D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)\right)\right]-\mathbb{E}_{d^\pi\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\log \left(1-D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)\right)\right] .
$$

***2. Least-Squares Discriminator***
在标准 GAN 中常采用 Sigmoid 交叉熵作为损失函数，随着 Sigmoid 函数饱和，可能会出现梯度消失的情况。因此，在 AMP 采用**最小二乘 GAN (LSGAN) 的损失函数**，
$$
\underset{D}{\arg \min } \mathbb{E}_{d^{\mathcal{M}}\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\left(D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)-1\right)^2\right]+\mathbb{E}_{d^\pi\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\left(D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)+1\right)^2\right] .
$$

判别器网络的评分不是 1 和-1 的离散值的，而是在 -1 到 1 之间的**连续值**。对于风格奖励的设置，设定为 \[0,1]。
$$
r^S\left(\mathrm{~s}_t, \mathrm{~s}_{t+1}\right)=\max \left[\begin{array}{ll}
0, & \left.1-0.25\left(D\left(\mathrm{~s}_t, \mathrm{~s}_{t+1}\right)-1\right)^2\right] .
\end{array}\right.
$$

Sigmoid 交叉熵：
$$
\underset{D}{\arg \min }-\mathbb{E}_{d^{\mathcal{M}}\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\log \left(D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)\right)\right]-\mathbb{E}_{d^\pi\left(\mathrm{s}, \mathrm{~s}^{\prime}\right)}\left[\log \left(1-D\left(\mathrm{~s}, \mathrm{~s}^{\prime}\right)\right)\right] .
$$
详细 GAN 描述： [[GAN]]


***3. Discriminator Observations***
  
AMP 使用观察映射 $\phi(s)$ 从状态中提取与确定运动特征相关的特征，然后再将这些特征输入到判别器中，判别器的输入为：
**这些特征包括：**
- 基体的线速度和角速度
- 关节位置
- 关节速度
- 末端执行器（例如手和脚）的位置（笛卡尔空间）
- 末端执行器（例如手和脚）的位置（笛卡尔空间）
- 末端执行器（例如手和脚）的位置（笛卡尔空间）

对观察数据进行映射，可以提取出通用性运动特征，然后将这些特征输入到判别器中，这样做的好处是：通过观测数据的特征提取，判别器可以适配不同的问题。


***4. Gradient Penalty***

为解决生成器出现的**过度拟合和模式崩溃**的问题，该文提出了**梯度惩罚**的方法，该惩罚项与真实数据流形上**判别器输出的梯度范数的平方成正比**。这样做可以避免判别器过度强大，导致其对生成器生成的样本过于敏感，梯度惩罚阻止判别器对输入的细微变化过于敏感，从而避免生成过于陡峭的决策边界。
$$\begin{aligned}
\arg\operatorname*{min}_{D} & \mathbb{E}_{d^{M}(\mathrm{s,s^{\prime}})}\left[\left(D(\Phi(\mathrm{s}),\Phi(\mathrm{s^{\prime}}))-1\right)^{2}\right] \\
 & +\mathbb{E}_{d^{\pi}(s,s^{\prime})}\left[\left(D\left(\Phi(s),\Phi(s^{\prime})\right)+1\right)^{2}\right] \\
 & +\frac{w^{\mathrm{gp}}}{2}\mathbb{E}_{d^{M}(\mathrm{s,s^{\prime}})}\left[\left\|\nabla_{\phi}D(\phi)\right|_{\phi=(\Phi(\mathrm{s}),\Phi(\mathrm{s^{\prime}}))}\right\|^{2}
\end{aligned}$$
- $\frac{w^{\mathrm{gp}}}{2}$为人为设定的系数


***AMP & GAIL***

**相同点：**

- **两者都使用对抗性判别器**： AMP 和 GAIL 都使用一个对抗性判别器来区分专家演示的行为和策略生成的行为。判别器充当奖励函数，鼓励策略生成与专家演示相似的行为。
- **两者都使用强化学习**： AMP 和 GAIL 都使用强化学习算法（例如 PPO）来训练策略，以最大化从判别器获得的奖励。

**不同点：**

- **输入数据**： GAIL 通常需要状态-动作对作为输入，而 AMP 只需要状态转换 $(s_t, s_{t+1}$) 作为输入。 这使得 AMP 可以直接从运动片段数据中学习，而无需访问专家的动作信息。
- **目标函数**： GAIL 最初的目标是最小化策略生成的数据分布与专家演示的数据分布之间的 Jensen-Shannon 散度。 AMP 则采用最小二乘 GAN (LSGAN) 的损失函数，旨在最小化 Pearson χ2 散度.
- **梯度惩罚**： AMP 使用梯度惩罚来增强训练稳定性，而 GAIL 并没有明确要求使用梯度惩罚。梯度惩罚通过惩罚数据流形上的非零梯度，防止判别器过度拟合，并使策略能够更有效地探索状态空间。


## VMP：潜在运动表示和控制策略分离训练的运动控制框架

***Abstract：***
本文提出一个两阶段运动控制架构： VMP（Versatile Motion Priors），该架构能够从从大规模运动数据中学习并稳健地执行各种复杂的运动。在第一阶段，使用**变分自编码器（VAE）** 从大规模的运动数据集中学习**通用的潜在运动编码**；在第二阶段，结合第一阶段的潜在运动特征表示和实时参考运动进行**强化学习的控制策略训练**。该方法避免了对抗学习的缺点，并通过结合多模态输入和显式模仿奖励，实现了更高的运动跟踪精度和泛化能力。

VMP 是一种基于学习的运动跟踪框架，智能体在按照运动参考轨迹进行运动的同时遵守物理定律。VMP 算法将运动编码和策略训练分离成两个独立的阶段。

![[Pasted image 20241205135745.png]]

***Contributions***
- **两阶段的运动控制架构**：模块化的方法使得训练过程更加高效，并且允许单独优化潜在空间和控制策略。

- **通用的潜在运动特征表示**：利用变分自编码器学习一个通用的潜在运动空间，这一阶段的关键创新在于，**它对运动数据集中的每一个运动参考帧（及其周围的窗口）都生成一个对应的潜在代码**，而不是像以往工作那样为整个运动片段生成单个潜在代码。

- **多模态输入的运动控制策略**：使用强化学习训练一个单一、通用的控制策略，该策略以第一阶段生成的潜在代码和即时的运动参考作为输入，从而生成物理角色的执行器命令。其中运动参考提供了即时反馈，帮助策略精确跟踪目标运动，而潜在代码则包含了运动的过去和未来信息，帮助策略更好地预测和执行复杂的运动。


VMP（Versatile Motion Priors）的算法框架主要由两个阶段构成，其核心思想是将从大量运动数据中提取的运动先验与基于物理的控制策略分离开来，从而实现对复杂动作的鲁棒跟踪和精细控制。下面详细介绍这两个阶段的主要内容：

---

***Stage I：运动先验提取（Kinematic Latent Space Extraction）***

在第一阶段中，VMP 的目标是从海量的运动片段中学习一个紧凑且能表达短时间动作结构的潜在空间。具体步骤如下：

 **1. 运动数据准备**  
   - 输入数据由一系列连续的运动帧组成，每一帧包含关键的运动信息，如基体高度 $h_t$（相对于地面）、基体的朝向（$\theta_t$  6 dim）、基体速度（6 dim）关节角度 $q_t$、角速度 $\dot{q}_t$、以及手足相对于根的位置 $p_t$。  
   - 为了捕捉动作的时空演变，论文从原始运动数据中随机抽取短时间窗口，即对中心帧 $t$ 前后各 $W$ 帧，共计 $2W+1$ 帧，记为：
$$ M_t = \{ m_{t-W}, \dots, m_t, \dots, m_{t+W} \}. $$


**2. 使用变分自编码器（VAE）建模**  
   - 设计一个基于 1 D 卷积层和残差块（resnet blocks）的编码器-解码器结构。编码器将输入的运动窗口映射到一个低维的潜在空间，通常设置潜在向量维度为 $d_z = 64$。  
   - 在编码器的瓶颈层（bottleneck），借鉴 $\beta$-VAE 的思想，通过双倍映射（doubling the encoder output dimension）后采样得到一个多元高斯分布的潜在码。  
   - 解码器则反向映射该潜在码，重构出原始运动窗口。重构损失结合 KL 散度正则项，从而促使学习到的潜在空间能够捕捉到动作的基本构成和局部运动结构。
[[VAE]]

**3. 目标函数与训练细节**  
   - 使用如下形式的目标函数，其中重构误差和 KL 散度被平衡权衡（通过一个较小的 KL 权重，如 0.002，结合循环调度策略来缓解 KL 消失问题）：  
$$ \mathcal{L} = \mathcal{L}_{\text{recon}} + \beta \, \text{KL}(q(z|M_t) \| p(z)). $$
   - 训练过程中，随机从大规模未经过滤的运动片段数据库中采样运动窗口，确保潜在空间泛化到各种运动技能。

---

***Stage II：基于运动先验的控制策略训练（Policy Learning）

在第二阶段中，利用第一阶段中训练好的运动先验（即编码器获得的潜在空间），进行物理控制策略的学习。主要内容包括：

**1. 状态表示与输入构造**  
   - 控制策略（Policy）的输入主要由当前帧的**物理状态**和对应时间窗口内运动先验的**潜在编码**构成。  
   - 状态信息包括角色的局部坐标系内的位置、关节状态等，而潜在码则提供了上下文（过往及未来短时间内动作演化）的信息，使策略可以提前“预知”角色在未来可能的运动趋势。

**2. 目标与奖励设计**  
   - 为了确保在物理仿真中运动能够精确跟踪用户指定的运动轨迹和保持动作平滑，奖励函数主要包括两部分：  
     - **运动重构奖励**：该奖励衡量模拟角色在当前帧的状态与由运动先验（从 VAE 解码得到的状态）产生的目标状态之间的接近程度。  
     - **平滑性奖励**：确保动作输出在连续帧间保持平滑，防止产生不连贯或过于剧烈的运动突变。
     
   - 数学上，可将某一帧的状态误差定义为类似如下公式（这里以姿态误差为例）：
$$ e_{\text{pose}} = \frac{1}{N_{joint}} \sum_{j \in \text{joints}} \Big\| (x^j_t - x^{root}_t) - (\hat{x}^j_t - \hat{x}^{root}_t) \Big\|^2, $$
	 其中 $x^j_t$ 与 $\hat{x}^j_t$ 分别代表模拟角色和目标参考运动中关节 $j$ 的 3 D 位置。

**3. 策略训练过程**  
   - 利用强化学习（RL）方法（例如 PPO 算法）训练控制策略，目标是最大化累积奖励，使得物理仿真中的角色能够精确地跟踪运动先验产生的目标。
   - 在训练过程中，控制策略输出的动作作为 PD 控制器的目标，使得角色关节能够产生合适的扭矩，从而实现精确的物理控制。
   - 训练得到的控制策略与编码器形成一个联合系统，在推理时，将用户提供的目标运动序列先通过编码器提取潜在信息，进而由控制策略输出相应动作，实现精细的全身运动控制。

---

***总结***

VMP 的算法框架通过两阶段处理实现了从大规模、无标注、复杂运动数据中提取通用运动先验，并将之应用于基于物理的实时控制中。关键特点包括：

- **运动先验提取**：利用 VAE 通过短时间运动窗口学习到紧凑的潜在运动空间，既能捕捉局部的运动结构，又具有良好的泛化能力。
- **控制策略训练**：基于 RL 方法训练的策略，输入包含当前物理状态和潜在编码，奖励设计旨在精确跟踪运动先验并保障运动平滑。
- **鲁棒性与通用性**：这种分离先验学习与策略训练的方式，使得系统在面对不同的物理对象（如虚拟角色或机器人）和复杂场景时，都能保持较高的动作精度与稳定性，同时便于扩展到更大规模的运动数据与多样化任务。

这种框架兼顾了数据驱动的运动表达和物理控制的严格性，是实现鲁棒全身运动控制的重要思路。






***运动编码***
通过变分自编码器（VAE）来学习一个潜在的运动空间。

输入：运动切片数据库中的随机采样的运动窗口（连续运动帧）
输出：潜在运动空间$z_t$

Encoder 评价方法：余弦相似度

***策略训练***

- 输入：VAE 重构的潜在运动空间$z_t$、运动窗口的中心帧$m_t$
- 输出：运动窗口中心帧应该采取的动作$a_t$


## ASAP：动作补偿降低仿真和真机的差距
`ASAP: Aligning Simulation and Real-World Physics  for Learning Agile Humanoid Whole-Body Skills` -2025
> Carnegie Mellon University && NVIDIA

***Abstract***
为解决传统方法如系统辨识和域随机化存在参数调优复杂或策略过于保守的问题，该论文提出了名为 ASAP（**A**ligning **S**imulation and Re**a**l **P**hysics）的框架，该框架包含两个训练阶段：第一阶段为预训练阶段，通过人类视频数据重定向训练运动跟踪策略，将预训练策略部署到真机，记录真实世界轨迹。在第二阶段，训练一个**动作补偿模型**（Delta Action Model），补偿仿真和真机的动力学差异，最后在集成动作补偿的仿真中微调预训练策略，最终直接部署到真机中。
![[Pasted image 20250427112907.png]]


***Contribution***
- Delta Action： 提出通过补偿动作去学习动力学不匹配的方法，避免了传统 SysID 对参数空间的依赖和 DR 的保守性，降低 Sim 2 Real Gap。
- 三阶段训练架构：从仿真到真机，再从真机到仿真，最后仿真到真机的训练架构，通过多次仿真和真机的切换，尽可能提升基准策略对真机的适配性。

***1. Pre-Training：Learning Agile Humanoid Skills***


**A. Data Generation: Retargeting Human Video Data** -- 数据生成

1.  **Transforming Human Video to SMPL Motions**：通过 **TRAM**（Trajectory-Aware Motion Represent） 技术对从视频序列中中估计人体的**全局运动轨迹**（全局坐标系）和**基于 SMPL 模型的 3D 姿态和形状**（局部坐标系）。其中 SMPL 是一个参数化的可微分的三维人体模型。
2.  **Simulation-based Data Cleaning:** 通过 MaskedMimic 方法进行动作跟踪模仿，在仿真中对不能被模仿的动作进行清洗。
3.  **Retargeting SMPL Motions to Robot Motions**: 将 SMPL 动作**重定向**到真机机器人上。先对形状参数进行


**B. Phase-based Motion Tracking Policy Training** -- 策略训练

-  **Asymmetric Actor-Critic Training**：采用非对称 AC 架构，Critic 利用特权信息（全局轨迹），Actor 仅依赖本体感知。
- **Termination Curriculum of Tracking Tolerance**： 采用课程学习的方法对回合结束的条件（动作误差容忍度）进行动态调整（1.5 m-0.3 m）
- **Reference State Initialization:** 采用 RSI（Reference State Initialization）架构对随机出实话动作相位，避免出现顺序学习的瓶颈。

***2. Post-Training: Training Delta Action Model and Fine-Tuning Motion Tracking Policy***

- **Data Collection**： 利用第一阶段训练好的模型，在真机中进行动作的采集。
- **Training Delta Action Model**：基于仿真和真机的数据，通过 RL 方法学习一个补偿动作网络 Delta Action，补偿动力学差距。
- **Fine-tuning Motion Tracking Policy under New Dynamics**：对第一阶段的策略模型加入该动作补偿网络重新训练（策略微调），即所有新动作都加上 delta action，在真机部署时不需要部署动作补偿网络。

## StyleLoco: 教师-学生框架与生成对抗框架相结合

***Abstract***
StyleLoco 框架通过将基于**教师-学生框架**的强化学习算法和**生成对抗模型学习**算法（GAIL）相结合，旨在解决机器人在走路稳定性和自然性之间的平衡，该框架共包含两个阶段：第一阶段是单独学习教师策略；第二阶段是固定教师策略，通过多判别器，同时从教师策略和参考数据集对学生策略进行判别指导。

![[Pasted image 20250415154309.png]]


***Contribution***
- **生成对抗蒸馏**（​Generative Adversarial Distillation , GAD）：两阶段训练架构，先通过特权信息训练教师策略，随后通过双判别器架构，结合了训练好的教师策略和参考运动数据集来合成自然和适应性的行为，用于评估学生策略生成的状态转换是否符合教师政策或者运动数据集的参考运。
- **多判别器架构**（​Multi-Discriminator Architecture）：通过双判别器将教师策略和专家数据集的知识蒸馏到学生策略中，使学生策略兼具自然和适应性。


> 教师策略采用课程学习的策略，第一阶段先学习平衡和稳定；第二阶段学习全向运动。


![[Pasted image 20250416110139.png|500]]



-------------

# Getting-Up
## HoST: 不同姿势的起身站立控制
`Learning Humanoid Standing-up Control across  Diverse Postures`  - 2025
> Shanghai Jiao Tong University & Shanghai AI Laboratory  

本文提出了一个机器人从不同初始状态下站立起身的控制器（Humanoid Standing-up Control）， HoST 通过使用多重评论家架构和课程学习，使机器人能够在不同地形上学习适应姿势的动作。

***Contribution***：
* **多评论家 (Multi-Critic) 架构:** 将奖励函数分为多个组，每个组对应一个独立的评论家，从而更有效地平衡不同的优化目标，加速学习过程。
* **课程学习 (Curriculum Learning):** 通过逐步调整训练难度，例如引入垂直拉力来辅助机器人探索，从而克服 RL 探索中的挑战。
* **运动约束 (Motion Constraints):** 使用动作尺度调整（action_scale）和 L2C2 平滑正则化来限制机器人的动作幅度，避免产生剧烈和不稳定的运动。


***Multi-Critic Framework：***

设计了多个奖励函数，并将它们划分为不同的奖励组，例如任务奖励、风格奖励、正则化奖励和任务后奖励。
由于每个奖励组都有自己独立的 critic，因此可以更好地平衡不同奖励目标之间的权重。这种独立性使得每个critic能够更精确地学习与其负责的奖励相关的价值函数。


## HiFAR: 多阶段课程学习跌倒起身
`HiFAR: Multi-Stage Curriculum Learning for High-Dynamics  Humanoid Fall Recovery`   - 2025
> Tsinghua University && Booster Robotics Technology`

***Abstract***
本文提出了一个名为 HiFAR 的多阶段课程学习框架，将复杂高维摔倒恢复任务拆分成简单的低维任务，分阶段去训练机器人，从而实现在动态环境下快速自主恢复。

![[Pasted image 20250329135616.png]]

***Contribution***
- **任务分解和课程学习**：将复杂高维的跌倒恢复任务拆分为两阶段简单低维任务，并结合课程学习的思想，由易到难分阶段训练机器人。
- **优化学习引导机制**：运用*关键状态初始化*（KSI）和*奖励塑造*（Reward Shaping）技术引导学习过程，并通过*维度扩展*（Dimensionality Expansion）策略增加动作空间，提供多维度学习信息。


---
***Multi-Stage Curriculum Learning***
**Training Phase 1:** 基础恢复策略
- **目标**： 在二维跌倒场景中（仰卧和俯卧的初始姿势），学习一个基本的跌倒恢复策略。
- **约束**：除控制仅限于在（x, z）平面内操作的关节外，对关节活动范围、力矩等实行宽松的正则化奖励策略。
- **优势**：
	- *限制策略探索空间*：通过减少控制关节的自由度可以有效限制策略的探索空间，减少训练过程中自我碰撞的风险。
	- *加速学习过程*：实行宽松的关节约束、采用 KSI 和 Reward shaping 等技术促进学习过程的稳定性，智能体逐渐掌握基础的恢复行为，为第二阶段的任务做好准备。

**Training Phase 2:** 可部署的恢复策略
- **目标**：训练场景扩展至三维跌倒空间，注重训练可部署的跌倒恢复策略。
- **约束**：增加了侧向髋部滚转关节，并根据真实机器人的状态设置关节位置、速度和扭矩等现实约束，同时引入了地形和机器人状态的更多域随机化。
- **优势**：
	- *可部署的恢复策略*：考虑了真实机器人的现实约束，并增加各种域随机化参数，模拟了真实环境的多样性。

----
***Guide The Learning Process***
- **KSI（Keference State Initialization）**：对于训练的初始状态，会从预先定义的几个关键初始状态中随机采样，这样做可以避免因初始状态随机性过大导致训练过程不稳定，加快策略收敛速度。
> 在第一阶段仅考虑摔倒起身的两种初始姿势：仰卧式和俯卧式。
   在第二阶段考虑了