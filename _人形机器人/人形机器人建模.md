

# 一、运动学

## 运动学


## 动力学

正向动力学和反向动力学是机器人学中两个核心的概念，分别描述了机器人的运动状态与其控制输入（如关节力矩或位置）的关系：
- **正向动力学**：已知驱动力或力矩，求解关节的加速度、速度和位置。主要用于仿真和运动预测。
- **反向动力学**：已知运动轨迹（位置、速度、加速度），求解所需的力矩或驱动力。主要用于控制和力矩计算。

### 正向动力学（Forward Dynamics）

正向动力学问题是根据机器人的**关节力矩**或**驱动力**来计算**关节的加速度、速度和位置**。它描述了机器人在给定的驱动力（或力矩）作用下会如何运动，是一种“已知输入求输出”的过程。正向动力学的应用场景包括：

- **运动仿真**：通过给定的力或力矩，计算机器人如何运动，以便模拟和分析其运动过程。
- **运动预测**：根据已知的关节驱动力矩，预测机器人下一步的位置、速度和加速度，帮助规划和评估运动轨迹。
  
数学上，正向动力学的基本形式可以表示为：
$$
M (q) \ddot{q} + C (q, \dot{q}) \dot{q} + G (q) = \tau
$$
其中：
- $M (q)$ 是关节位置 $q$ 的惯性矩阵，
- $C (q, \dot{q})$ 是科氏力和离心力矩阵，
- $G (q)$ 是重力向量，
- $\tau$ 是关节力矩。

根据这些力和力矩，正向动力学求解关节的加速度 $\ddot{q}$，再通过积分获得关节速度 $\dot{q}$ 和位置 $q$。

### 反向动力学（Inverse Dynamics）

反向动力学问题是已知机器人的**关节位置、速度和加速度**，求解**需要的关节力矩或驱动力**。它描述了要让机器人按指定的轨迹运动所需的控制输入，是一种“已知输出求输入”的过程。反向动力学的应用场景包括：

- **轨迹控制**：根据预定轨迹，计算需要施加的力或力矩，以确保机器人按期望的路径移动。
- **力矩控制**：在精确控制下，通过反向动力学计算得到的力矩来精确施加控制力，维持运动稳定性。
  
数学上，反向动力学也基于相同的动力学方程：
$$
\tau = M (q) \ddot{q} + C (q, \dot{q}) \dot{q} + G (q)
$$
在此公式中，已知 $q$、$\dot{q}$、$\ddot{q}$ 后，通过反向动力学求出满足期望运动的力矩 $\tau$。


人形机器人的动力学参数主要包括质量、惯性、关节位置、力矩和阻尼等属性。这些参数在描述和控制机器人的运动中扮演重要角色。以下是人形机器人动力学中常用的参数：

### 动力学参数

1. ***质量参数***
   - **关节质量（Mass）**：描述机器人每个连杆或部件的质量，通常用来计算惯性力和重力。
   - **质心位置（Center of Mass, CoM）**：指每个连杆的质心相对于其参考点的位置，用于计算重力作用和动量。

2. ***惯性参数***
   - **惯性张量（Inertia Tensor）**：反映了物体质量在各个方向上的分布情况；描述每个连杆在不同轴上的惯性分布情况，定义了刚体在各方向上的转动惯性。这对于计算旋转运动和力矩很重要。
   - **惯性矩（Moment of Inertia）**：表示刚体绕特定轴的转动惯性，通常是惯性张量的简化形式。


3. ***几何参数***
   - **关节位置（Joint Position）**：描述关节的空间位置，即关节在全局或局部坐标系中的位置。几何位置对于描述机器人姿态和规划路径至关重要。
   - **连杆长度（Link Length）**：描述相邻关节之间的距离，用于定义机器人结构的尺寸。

 4. ***关节参数***
   - **关节角（Joint Angle）**：描述关节的旋转角度，是机器人姿态的主要控制变量之一。
   - **关节速度和加速度（Joint Velocity and Acceleration）**：描述关节的运动状态，是反向动力学和控制计算的输入参数之一。
   - **关节力矩（Joint Torque）**：表示关节所需的驱动力或力矩，通常是反向动力学的输出，用于实现期望的关节运动。

5. ***摩擦和阻尼参数***
   - **摩擦系数（Friction Coefficient）**：关节的摩擦阻力会影响运动平稳性，通常根据实际情况或实验数据给出。
   - **关节阻尼（Damping Coefficient）**：关节阻尼是关节速度与产生的阻力之间的关系系数，用于模拟关节的能量耗散效果。

6. ***动力学方程参数***
   - **惯性矩阵（Inertia Matrix, $M (q)$）**：描述关节质量对运动的影响，依赖于机器人姿态，影响关节的加速度。
   - **科氏力和离心力项（Coriolis and Centrifugal Forces, $C (q, \dot{q})$）**：描述了机器人在加速时所需克服的离心力和科氏力，主要影响到控制力的计算。
   - **重力向量（Gravity Vector, $G (q)$）**：描述重力对各关节的影响，通常是重力分布在关节上的力矩，用于反向动力学计算。

7. ***接触参数***
   - **接触点位置（Contact Point Location）**：描述脚底等部位与地面或物体接触的具体位置。
   - **接触力和摩擦力（Contact Force and Friction Force）**：模拟机器人在接触时产生的支撑力和摩擦力，是步态规划和稳定性分析的重要参数。

8. ***控制参数***
   - **电机功率和扭矩输出范围**：电机的输出功率和最大力矩决定了机器人动作的速度和力量。
   - **传动比（Gear Ratio）**：影响电机的输出特性，将电机的旋转转化为关节运动。

这些动力学参数共同作用，决定了人形机器人的运动性能和控制难度。精确地获取和设置这些参数，对于构建逼真的机器人模型和实现稳定的控制有至关重要的影响。

### 惯性张量

惯性张量和惯性矩都是描述物体旋转惯性的重要物理量，但它们在应用上有一些区别。

#### 1. 惯性张量（Inertia Tensor）

惯性张量是描述物体绕各个轴旋转时的惯性分布的矩阵，是一个 3 x 3 的对称矩阵。它的每个元素都代表物体在不同轴上的惯性属性，**反映了物体质量在各个方向上的分布情况**。惯性张量的定义如下：

$$
I = \begin{bmatrix} I_{xx} & I_{xy} & I_{xz} \\ I_{yx} & I_{yy} & I_{yz} \\ I_{zx} & I_{zy} & I_{zz} \end{bmatrix}
$$

其中：
- 对角线元素 $I_{xx}, I_{yy}, I_{zz}$ 是关于各自坐标轴的惯性矩。
- 非对角线元素 $I_{xy}, I_{xz}, I_{yz}$ 表示各坐标轴之间的惯性耦合，称为惯性积。

惯性张量描述了刚体在三维空间中旋转时的动态行为，广泛用于三维旋转运动的动力学计算。例如在物体绕任意方向旋转时，惯性张量可以帮助计算出角动量和角加速度之间的关系。

#### 2. 惯性矩（Moment of Inertia）

惯性矩是惯性张量的特例，通常表示物体绕某一特定旋转轴的旋转惯性。它是惯性张量的对角元素，即 $I_{xx}, I_{yy}, I_{zz}$，用于描述物体绕单个轴的惯性大小。惯性矩通常通过下列公式计算：
$$
I = \int_V \rho (r) \cdot r^2 \, dV
$$
其中：
- $\rho (r)$ 是物体在位置 $r$ 处的质量密度，
- $r$ 是该位置到旋转轴的距离。

>  在二维平面上旋转时，惯性矩和惯性张量等价，因为只有一个惯性矩就可以描述物体的旋转特性。


#### 3. 惯性积（Product of Inertia）
**惯性积**（Product of Inertia）是惯性张量中的非对角线元素，用于描述物体质量分布在不同轴之间的耦合关系。惯性积反映了物体在旋转时，由于质量分布不均匀，**在一个坐标轴上旋转会引起其他轴的耦合效应**。惯性积广泛应用于三维物体动力学计算和稳定性分析中。

对于一个质量分布已知的刚体，惯性积定义为：
$$
I_{ij} = - \int_V x_i x_j \, dm
$$
其中：

- $x_i$ 和 $x_j$​ 是刚体内质点相对于 i 轴和 j 轴的坐标，
- $I_{ij}$ 是惯性积，它在惯性张量中位于第 i 行第 j 列和第 j 行第 iii 列。

**性质：**
- **对称性**：惯性张量是对称矩阵，因此 $I_{ij} = I_{ji}$ 。
- **坐标依赖性**：惯性积依赖于物体的旋转轴的选择。在某些特定坐标系下（例如主轴系），惯性积可以变为零，使惯性张量对角化。对称物体的惯性积通常为零，意味着对称物体在一个轴上的旋转不会对其他轴产生影响。


### 机器人系统质心
机器人系统的总质心位置$\mathbf{p}_{\text{CoM}}$由机器人所有连杆的质心位置和质量加权平均得到：
$$\mathbf{p}_{\text{CoM}} = \frac{\sum_{i=1}^n m_i \mathbf{p}_{ci}}{\sum_{i=1}^n m_i}$$​
其中：

- $\mathbf{p}$​：第 i 个连杆的质心在全局坐标系中的位置。
- 通过正向运动学计算 $\mathbf{p}_{ci} = T_0^i \cdot \mathbf{r}_{ci}$​。



## 牛顿-欧拉方法
牛顿-欧拉方法将刚体的平动和转动分开处理：
1. 牛顿定律：描述刚体的质心的线性运动。

$$
\mathbf{F}=m \mathbf{a}
$$


其中:
- $\mathbf{F}$ ：合外力。
- $m$ ：刚体的质量。
- A：质心的加速度。
2. 欧拉动力学方程：描述刚体的角运动（转动）。

$$
\tau=\mathbf{I} \alpha+\omega \times(\mathbf{I} \omega)
$$


其中:
- $\tau$ ：合外力矩。
- I：刚体的惯性张量。
- $\alpha$ ：角加速度。
- $\omega$ ：角速度。

通过这两个方程，可以同时描述刚体的线性加速度和角加速度。



# 二、控制方法

无论是 RL 方法还是基于模型（MBC）的方法，控制的本质就是面对**动态系统**去设计**策略**以满足一些**限制**。

## MBC && RL

### 方法对比
***算力运用：*** 
- MBC： MBC 的策略优化是在线，在线计算机器人要执行哪些行为，它没有离线计算的能力，即**在线优化机器人的行为**；
- RL ：RL 的方法是基于轨迹数据去学习策略，有充足的时间进行策略优化（离线更新），从而找到最优策略。

***状态估计***：
- MBC：与控制一样重要，得需要知道机器人的状态，清楚它此刻所在的位置
- RL：再一定程度上可以规避掉这个问题，
	1. 利用仿真器提供的环境信息，它可以同时学习策略和状态估计器（或通过辅助隐式的学习状态估计）；
	2. 利用 Teacher-Student 架构，通过特权信息，教师网络制导学生网络进行学习。

***RL 优势***：
1. **离线优化**：策略的优化可以在离线阶段进行，策略学习好后固定神经网络，可以直接进行推理，避免在线优化。
2.  **绕开状态估计**：利用仿真器的环境信息，通过特权信息、Teacher-Student 架构，可以绕开传统控制中必须要解决的状态估计问题。

***MBC 优势***：
1. **在线优化**： RL 中训出策略后，这个策略网络就固定了，需要根据 Sim 2 Real 的效果重新进行训练调节，如果仿真和真机差距过大，误差就会更大。MBC 可以一边让机器人运动，一边进行计算。
2. **结构性优势：*** 结构性意味着控制方法具有清晰、可解析的数学框架或约束条件，这意味着在给定有限的采样数据或仿真次数的条件下，控制方法能够更快、更准确地收敛到符合期望的行为，**采样效率更高**。 RL 算法大都采用 PPO，因为是 on-policy，因此它的采样效率就可以不用在意，但可能会收敛到我们不想要的状态；


***MBC 结合 RL***
Control 的安全性融入 RL 中，或者借助 Control 良好的结构使强化学习变得更具稳健性。
限制当前 RL 算法的一个点就是仿真器（Sim 2Real Gap），我们很难去创造一个完美的仿真器去解决所有的问题，所以一定要

 **CAJun**：CAJun 由高层次中心化策略和低层次腿控制器组成。在高层，由 RL 策略输出基体速度、步态周期（gait timing），摆动腿的位置（swing foot position）；在低层，MBC 基于步态周期去跟踪摆动腿的位置指令和基体速度指令。 

> CAJun: Continuous Adaptive Jumping using a Learned Centroidal Controller

**Real 2Sim**： 不在固定不变的仿真器里学习，而是到现实中获取数据来改进仿真器，之后再基于改进后的仿真器进行学习。

**世界模型+MPC**：先学习一个世界模型（World model），然后再利用 MPC 去进行控制操作。即在现实环境里把动力学模型学习出来，之后再运用基于采样的模型预测控制（Sampling-based MPC）去开展后续的 Control

>DIAL- MPC: https://github. Com/LeCAR-Lab/dial-mpc. Git


### MBC 方法
在对物理规律进行采样时，其实就是在做基于模型的控制。

MPC：

我们运用 MPC 时，往往会对模型进行简化，也就是处理**简化模型的长时域最优化问题**。在这个过程中，需要进行建模，比如先将其简化成**单刚体模型**，复杂一点的话就变成**机器人模型**。在此基础上，通过预测来保障机器人**状态轨迹的可预测性**，进而确保机器人的稳定性。但做完简化模型那一步后，大家发现存在问题，因为简化后的模型很难真实反映机器人全身动力学的特点。

WBC：

研究进入了第二阶段，也就是处理复杂模型的短时域最优化问题，就是所谓的全身运动控制（WBC）。在这个阶段，要建立机器人的**全身动力学模型**，然后计算出当前的最优控制以保证实时性，用全身运动控制（WBC）来弥补模型预测控制（MPC）因简化模型而产生的问题。


## Sim 2 Real

之所以存在 Reality Gap 是因为我们学习的模型的环境与实际环境是存在一定差距的，那么在一个存在未知信息的环境，如何减少这个差距呢，最通用、最具扩展性的方法就是：模型与真实环境进行交互，尽可能的把未知信息获取回来，然后进行优化。

如果要尽可能提高机器人的可靠性，就需要实现与环境的交互，环境交互不能仅仅通过一种被动的数据学习（收集一些离线数据），那么要实现与环境的交互，就必须得用强化学习。



## Manipulation && Locomotion

### 挑战 

***Locomotion***
对于机器人的 Locomotion，主要的挑战在于**机器人本体的不确定**，这种不确定性相对有限，例如机器人可能踩到石子，或者路面出现打滑等情况。从机器人本体的角度来看，这些不确定性的影响是可以量化的。在控制领域，我们有一整套数学工具来应对这些问题，其中之一就是**鲁棒控制**（Robust control）。鲁棒控制的核心思想是对不确定性进行定量建模，进而确定**最坏情况的界限**（Worst-case bound）。只要实际的不确定性在这个界限范围内，设计出的控制策略就是有效的。


***Manipulation***
Manipulation 的主要挑战来自外部世界的不确定性，而不是机器人本体的不确定性。机器人本体是一个经过精心设计的确定性系统，但**外部世界的复杂性**是无限的（Unbounded complexity）。这些不确定性可能来自视觉、物理交互、触觉感知等多个方面，而对这些因素进行全面、精确的建模几乎是不可能的。尽管我们可以对其中的某些部分进行建模，取得一些成果，但始终无法完全掌控这些外部复杂性。这正是 Manipulation 问题需要强化学习（RL）的原因所在。RL 通过与外界的交互，能够更好地适应外部环境中的不确定性。

从本质上来说，Manipulation 和 Locomotion 是两类截然不同的问题：Locomotion 可以通过控制理论中现有的工具（如鲁棒控制）较好地解决，而 Manipulation 则需要通过 RL 等数据驱动的方式，去应对外部世界的复杂性和不确定性。


### Manipulation

***技术路线***
- 模仿学习 + 真机：以扩散策略（Diffusion policy）或者其他以 Aloha 为代表的模仿学习路线，能产生一系列比较好用的策略（Policy）。就是先收集大量的演示数据（Demonstration），然后采用行为克隆的方式。目前较少采用离线强化学习的路线。
- RL+真机：Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning
- RL + 仿真： 先预训练，在做微调
- 仿真优化：添加一些可微的元素进去，例如通过可微分模拟的方式来构建虚拟环境的世界模型。诸如粒子动力学（Particle Dynamics）之类。