
# 一、环境搭建  & 代码运行
由于本身电脑没有 N 卡，因此在智星云平台租用了一台 GeForce RTX 3090 云主机进行软件安装与环境搭建。
- IsaacGym_Preview_4 
- Python （3.7.12）
- Pytorch （1.12.0）
- Torchvision （0.13.0）
- Torchaudio（0.12.0）
- Cuda (12.2)
- NVIDIA GPU Driver  (535.146.02)

运行 legged_gym 库中 A1 的代码，训练 1500 个 iteration，A 1 机器人可以在多种地形环境中运动，并通过 cv2 将每帧图像转换为视频。

# 二、四足机器人双足行走
依据四足机器人和双足机器人的主要，我主要对三个方面进行修改：
- **机器人本体**：四足机器人，由于接地点更多, 重心分布更好，具有更高的稳定性。双足行走只有两个支撑点, 重心位置相对高, 稳定性较差。为更好的行走，双足机器人需要将前腿固定，并根据行走的姿态将前腿保持一个合适的位置（URDF 中，髋关节前摆调节+ 90 度、膝关节调整为 -140 度）。
- **初始姿态**：每次 reset 后机器人初始姿态会对训练结果有比较大的影响，因此一个好的初始状态对快速训练一个智能体是比较重要的，因此我在设置四足机器人位置的时候会相对友好一些（后腿髋关节前摆 +70 度和膝关节调节+30 度，基体按 y 轴旋转 90 度）
- **奖励函数**：
	- 参照 A 1 、 Cassie 奖励设计以及题目的提示，进行初始奖励函数的设计，并根据测试进行奖励系数的调整，实现跪着走、跳着走、原地蹲起的阶段。
	- 因为我调整了初始基体的姿态，导致基体 x 、z 轴方向改变，基体 x 轴指向上，z 轴指向后。而我们设置的 commands 类中的线速度和角速度是针对原 x、z 轴进行设计的。因此我针对 comput_reward 相关函数进行调整，根据真实基体的方向，对 x、z 轴的奖励进行对调，例如`_reward_tracking_lin_ve` 指令空间中的 x 线速度对应基体 z 轴方向的反方向，修改了大概五个关于速度方向的奖励后基本可以实现正常行走
	- 最后根据步态的状态对相关奖励系数进行一个调整优化


## MDP

**状态空间：（217dim）**

| 状态     | 维度  | 缩放       |
| ------ | --- | -------- |
| 基体的线速度 | 3   | 2        |
| 基体的角速度 | 3   | 0.25     |
| 基体重力向量 | 3   | 1        |
| 基体指令速度 | 3   | 2、2、0.25 |
| 关节位置差值 | 6   | 1        |
| 关节速度   | 6   | 0.05     |
| 上一时刻动作 | 6   | 1        |
| 高度测量   | 187 | 5        |

**动作空间：（6 dim）**

由于将前腿固定，因此动作输出为后腿各个关节的相对目标位置：
$p_d$： target angle = actionscale * action + defaultAngle
通过 PD 控制器，利用目标位置差值转换成力矩信息后在输入仿真器中进力矩控制
$$
\tau = K_p*(p_d- p) - K_d* (v)
$$
**奖励函数：**

| 超参数                | 奖励系数       | 说明                      |
| :----------------- | ---------- | ----------------------- |
| tracking_ang_vel   | 1          | z 轴角速度跟踪奖励              |
| tracking_lin_vel   | 1          | xy 轴线速度跟踪奖励             |
| lin_vel_z          | -1         | z 轴线速度惩罚                |
| ang_vel_xy         | -0.1       | xy 轴角速度惩罚               |
| Joint torques      | -0.00002   | 关节力矩惩罚                  |
| feet_air_time      | 5          | 保持正常行走奖惩                |
| stand_still        | -1         | 无运动惩罚                   |
| dof_acc            | -2.e-7     | 关节加速度惩罚                 |
| termination        | -200       | 提前结束惩罚                  |
| action_rate        | -0.01      | 动作幅度惩罚                  |
| dof_pos_limits     | -1         | 关节位置限制惩罚                |
| reward_orientation | -1         | 方向惩罚（基于机器人本体 X 方向向上的差值） |
| collisions         | -1         | 碰撞小腿、大腿、臀部、关节惩罚         |
| x 线速度指令            | （-1，1）     |                         |
| y 线速度指令            | （-0.5，0.5） |                         |
| z 轴角速度指令           | （-0.5，0.5） |                         |


## 训练过程描述

参照 A 1 、 Cassie 奖励设计以及题目的提示，进行奖励和状态空间的调整。
本次共进行 30 多次的训练，训练过程四个阶段：趴着走、跪着走、双脚跳着走（原地蹲起）、正常走

1 A1出现趴着走的情况
**解决方法：** 修改机器人初始位置，将基体绕 y 轴旋转 90 度，并在终止项中增加前腿的终止条件`terminate_after_contacts_on = ["base","FR_foot","FL_foot"]`

2 A 1双脚跪着走的问题
**解决方法：** 增加关节的接触惩罚，无运动惩罚

3 A 1 跳着走（原地蹲起）的问题
**解决方法：** A 1 初始时将基体绕 y 轴旋转 90 度，导致基体 x 、z 轴方向改变，基体 x 轴指向上，z 轴指向后，因此出现指令速度和基体速度不匹配的情况，接下来就对奖励函数与基体方向进行适配，修改的奖励函数包括：`_reward_lin_vel_z`、`_reward_tracking_lin_vel`、`_reward_tracking_ang_vel`、`_reward_ang_vel_xy`、`def _reward_orientation`

4 正常走后对相关超参数微调，优化行走步态，来回晃动
**解决方法：** 增加 xy 轴角速度惩罚 

# 三、速度估计

参照论文 Concurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion （为动态和稳健的腿部运动同时训练控制策略和状态估计器）。主要思想就是，在原强化学习 AC 框架基础上，增加历史状态信息，通过一个状态估计网络对机器人的线速度、脚的高度和接触概率。并将状态估计信息输入到控制策略产生最后的动作，该状态估计器与控制策略同时进行训练。

**历史状态信息：**
- 历史关节位置差值（前三个时刻）：当前姿态与初始姿态的差值
- 历史关节速度（前三个时刻）   
-  前三个时刻动作（相对目标位置）：期望关节到达的期望位置（相对于初始位置的相对量） 

> 论文中：当前关节位置、历史目标关节位置和历史关节位置差值 

**考虑历史状态的信息好处：**
- 通过融合过去的状态信息, 可以对当前测量进行滤波和修正, 提高状态估计的精确度。
- 保持运动连续性机器人的运动应该是连续和平滑的, 而不是瞬态的跳变。
- 除了利用历史修正当前,历史状态信息还可用于根据运动规律预测未来的状态
- 通过分析较长历史状态序列,可以识别当前所处的运动模式,如正常行走、爬坡、跳跃等,从而触发相应的运动控制策略。


**训练时：**
- 去掉高度检测信息，自定义估计网络结构：75 x 512 x 256 x 128 x 3，Critic 网络利用 privilege 信息获取状态，估计网络更新时我也是让他和 ppo 网络一起进行更新，利用历史 obs_batch 和 critic_obs_batch 的线速度部分，通过均方误差进行更新。
- 整体训练时，先保持原有的奖励不动，先对速度估计网络进行一个调整，使速度估计网络在尽可能短的时间收敛到一个较小的误差值。调整隐藏层层数, 权重初始化、正则化
- 然后再根据表现对相关地形和奖励信息进行调整。这里主要降低金字塔式斜坡地形的坡度`pyramid_sloped_terrain` （0.4->0.2）。。


## 速度估计网络结构

**输入（75dim）：**$o_t$

| 状态              | 维度         | 缩放       |
| --------------- | ---------- | -------- |
| 基体的角速度          | 3          | 0.25     |
| 基体重力向量          | 3          | 1        |
| 基体指令速度          | 3          | 2、2、0.25 |
| 关节位置差值          | 6          | 1        |
| 关节速度            | 6          | 0.05     |
| 历史关节位置差值（前三个时刻） | 18         | 1        |
| 历史关节速度（前三个时刻）   | 18         | 0.05     |
| 前三个时刻动作（相对目标位置） | 18         | 1        |
| x 线速度指令         | （-1，1）     |          |
| y 线速度指令         | （-0.5，0.5） |          |
| z 轴角速度指令        | （-0.5，0.5） |          |

**输出 （3dim）：** $\hat{x_t}$

基体线速度 estimated_line_vel（x、y、z）

**损失函数**
基体线速度估计值和真实值的均方误差
MSE (estimated_line_vel, true_line_vel)

## PPO 网络结构

**Actor**
输入：历史状态信息+基体线速度估计值$o_{t} + \hat{x_{t}}$  78dim
输出：相对目标关节位置 $a_t$   6dim 

**Critic：** 
输入： 历史状态信息 + 基体线速度真实值 $o_{t} + x_{t}$  78dim -- 利用`privileged_obs_buf`实现
输出：状态价值 1dim

## 训练过程描述
经过几轮预训练，发现基体线速度对机器人运动的影响是比较大的，当速度估计网络有较为准确的预测时，机器人才能很好的进行运动控制，因此后序训练调参的话主要分为两个阶段：
- 第一阶段：速度估计网络结构的调参，使速度估计网络在尽可能短的时间收敛到一个较小的误差值
- 第二阶段：在第一阶段基础上，对环境和奖励信息进行微调，由于缺少高度信息的检测，无法提前判断地形环境做出动作的改变，高难度地形对机器人快速适应性也要求比较高，因此整体步态表现略差于题目三。这里主要降低金字塔式斜坡地形的坡度`pyramid_sloped_terrain` （0.4->0.2）。

速度估计网络损失值变化曲线如下图所示：
![[Pasted image 20240329080752.png]]


# 问题

**工作模式和薪资待遇两个方面

- 日后公司的近期和长期的大体规划是怎样的，是以研究为导向还是落地产业化为导向
- 当前主要面临的挑战和工作的难点是什么
- 日后我需要负责的工作内容，工作方式是团队协作为主还是个人研究为主。日常进度汇报是汇报人是谁


- 面试的考核是根据测试题的完成度来判断的，薪资的话是否和测试题相关联
- 工资的组成部分，和平时的考核标准，以及晋升的方式是怎样的
- 15 号前入职是否缴纳社保，五险一金的缴纳比例。
- 合同签几年，试用期时间，考核标准和通过率，

市政府、教育
同质化


安排什么岗位，主要负责的工作内容是什么
提供多少薪资呢，基本工资、绩效工资还有补贴是怎么样的 
试用期的时间，试用期工资
工作的时间，以及加班费

不低于之前的工资，期望在之前基础上有 20%左右的涨幅。