# 一、论文 1

**Han R et al. 提出了一种在动态环境下通过强化学习方法解决多智能体导航问题的方案，将目标位置分配和避免碰撞结合到训练过程中，以学习协作导航策略**

>Han R, Chen S, Hao Q. Cooperative multi-robot navigation in dynamic environment with deep reinforcement learning[C]//2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020: 448-454.

## 概述

###  贡献
- 利用所有机器人的累积经验进行目标位置分配的协作架构
- 通过深度强化学习来训练碰撞躲避
- 开发了从模拟到真实的迁移机制，将训练好的策略更好地应用在真实环境中。

训练策略由所有智能体同时共享和更新
在仿真训练过程中开发了一组随机动力学模型，可以减少仿真和实体之间的不匹配。

### 目标
多个智能体在动态环境以最小代价（时间）无碰撞的完成多目标点的导航。即在动态环境中，所有智能体通过分享和优化信息，最小化完成任务的时间，来得到最优的导航策略。

### 总体过程
- 目标分配策略（组合优化）；首先将目标位置分配给每个机器人（实时变化）； 组合优化问题；计算机器人与目标点之间的分配方案的所有排列，找到总距离最短的排列
- 目标导航策略（RL）；每个智能体通过策略（actor网络）根据获取的状态信息得到相应的动作，智能体执行动作，得到下一时刻状态，根据环境反馈的奖励以及Critic网络预测的价值来评判当前动作的好坏，以此指导actor网络的更新。

## 策略

**状态**：
- 当前智能体状态（笛卡尔坐标系的位置、方向、速度）；
- 其他智能体状态
- 障碍物状态（从所有智能体的观测聚合而成）
- 目标位置
**动作**：
- 平移速度（transitional）和旋转速度（rotational）
**奖励：**
- 接近或到达目标点奖励


# 二、论文2

**Jin Y et al. 提出了一种交错深度强化学习方法(interlaced DRL)，同时学习动态目标选择策略和避碰策略来解决多机器人协作导航问题( multi-agent cooperative navigation problem MCNP)，多智能体在未知环境中不发生碰撞的情况下协同合作到达不同的目标点。**

>Jin Y, Zhang Y, Yuan J, et al. Efficient multi-agent cooperative navigation in unknown environments with interlaced deep reinforcement learning[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019: 2897-2901.


NP 问题


## 概述

首先通过目标选择策略（DQN）为每个智能体分配一个目标点
然后智能体根据与目标位置的角度差值，完成旋转；
如果这时没有障碍物，则直接前往目标点，则该算法就可简化为目标选择策略；若有障碍物，则调用防碰撞策略（DDPG），不断输出智能体朝向，引导智能体前往被分配的目标点


先通过预训练在无障碍物的环境下，先训练目标选择策略，然后增加障碍物，同时训练目标选择策略和障碍物避免策略。

### 贡献
采用 IDRA算法，可以很好的达到收敛
奖励函数不是启发方式，而是直接根据优化目标设定。


### 目标
每个智能体到达不同的目标点，并与智能体和障碍物保持一定的距离
交错 RL 同时学习两个任务，将导航策略建模为动态目标选择和碰撞避免策略。

MCNP：无碰撞的到达目标点，无碰撞

## 策略

**状态**
- 与目标位置、其他智能体的相对位置坐标
- 激光雷达检测信息

**动作**
- 目标位置分配
- 智能体朝向

作者在这里固定了速度的大小，所以作者直接简化为转向问题。

**奖励**

$$
\begin{aligned}
&\max _{\pi_i} \mathbb{E}\left[\sum _ { t = 1 } ^ { T _ { \operatorname { m a x } } } \left(-1+C_1 \delta(I(t)-1)-C_2 \sum_{j=1, j \neq i}^N\right.\right.\\
&\left.\left.u\left(d_r-\left\|\mathbf{o}_{i, a g_j}^t\right\|_2\right)-C_3 \sum_{k=1}^K u\left(d_r-d_{i, k}^t\right)\right) \mid \mathbf{o}_i^0, \pi_i\right]
\end{aligned}
$$

$$
r_i^t=\left\{\begin{array}{l}
-1-C_2, \text { if } \exists j \neq i,\left\|\mathbf{o}_{i, a g_j}^t\right\| \leq d_r \\
-1-C_3, \text { else if } \exists k, d_{i, k}^t \leq d_r \\
-1+C_1, \text { else if } I(t)=1 \\
-1, \text { otherwise }
\end{array}\right.
$$

- 最终任务完成奖励：$C_1$ 应该足够大，以避免协作导航过程以碰撞结束的情况；只有在所有智能体都到达不同的目标点才触发。$\delta$ 判别触发（单位脉冲）
- 与其他智能体的距离；$d_r$ : 距离阈值；$u$同为判别触发，与其他智能体距离越近，惩罚越高
- 与障碍物的距离越近惩罚越大
- 时间步长惩罚

**终止**：
- 每个智能体都到达不同的目标点
- 智能体发生碰撞
- 超时




# 三、论文 3 
Hierarchical and Stable Multiagent Reinforcement Learning for Cooperative Navigation Control 
> Jin Y, Wei S, Yuan J, et al. Hierarchical and Stable Multiagent Reinforcement Learning for Cooperative Navigation Control[J]. IEEE Transactions on Neural Networks and Learning Systems, 2021.

Multiagent Navigation to Unassigned Multiple targets (MNUM)

## 1. 概述

### 任务目标
各个智能体无冲突、无碰撞的到达不同的目标点。
> 无冲突：各个智能体之间前往不同的目标点
> 无碰撞：各个智能体保持安全距离、各个智能体与

### 限定条件
- 每个智能体都到达目标点
- 在运行过程各个智能体间保持安全距离
- 在运行过程中各个智能体与障碍物保持安全距离

### 创新点
- 固定速度大小，只改变移动方向，降低动作空间大小
- 通过扩展 Q 函数解决多智能体非平稳问题
- 分层强化学习框架
- 

## 2. 总体过程

### 分层策略模型

**通过分层强化学习完成整体任务**，上层动态目标选择策略，下层避碰策略：

首先上层策略（SNADQN），依据每个智能体的 *全局*  观测信息选择一个目标点，然后将其视场的中心旋转到与所选目标相同的方向，然后获取 *局部* 观测信息，如果在观测范围内没有观测到障碍物，则直接向目标移动，否则下层策略（SMADDPG）输出一个角度，智能体转向该角度，并向前移动。

### 稳定的 MADRL

**通过扩展Q函数去解决多智能体的非平稳问题**。

由于每个智能体独立做出决策，所以其他智能体的策略会导致非平稳的环境状态转换，即 $p\left(s^{t+1} \mid s^t, a_i^t\right)$ 会由于其他智能体策略的改变而不稳定，此时利用单智能体的 Q 函数计算误差时，就会变得不准确。
$$
p\left(s^{t+1} \mid s^t, a_i^t\right)=\sum_{a^t} p\left(a_{-i}^t \mid s^t\right) p\left(s^{t+1} \mid s^t, a_i^t, a_{-i}^t\right)
$$
通过上述公式看出，主要需要解决的非平稳的分量为： $p\left(a_{-i}^t \mid s^t\right)=\prod_{j \neq i} p\left(a_j^t \mid s^t\right)$ 

本文考虑了其他智能体动作分布，定义了一个扩展 Q 函数，该 Q 函数综合其他智能体的动作，从而替代上述非平稳分量
$$
\begin{aligned}
&Q_i^{\pi_i^*}\left(s^t, a_{-i}^t, a_i^t\right)\\
&=\sum_{s^{t+1}} p\left(s^{t+1} \mid s^t, a_{-i}^t, a_i^t\right)\\
&\times\left[r_i^{t+1}+\gamma \max _{a_i^{t+1}} Q_i^{\pi_i^*}\left(s^{t+1}, a_{-i}^{t+1}, a_i^{t+1}\right)\right] .
\end{aligned}
$$

由于智能体通常独立进行决策，因此其他智能体的动作通常是未知的。作者在这里通过临近时刻状态去估计他们的动作。
$$
\widehat{\kappa_{-i}^t}=\mathbb{E}_{s_{-i}^{t+1} \mid s_{-i}^t, a_{-i}^t} f\left(s_{-i}^t, s_{-i}^{t+1}\right)
$$
通过一定的转换，我们可以去掉非平稳分量，在平稳环境中，通过下面公式学习扩展Q函数：
$$
\begin{aligned}
&\mathbb{E}_{s_{-i}^{t+i} \mid s_{-i}^t, a_{-i}^t} Q_i^{\pi_i^*}\left(s^t, f\left(s_{-i}^t, s_{-i}^{t+1}\right), a_i^t\right)\\
&\left.\approx \mathbb{E}_{s^{t+1}}\right|_{s^t, a_i^t, a_{-i}^t}\left[r_i^{t+1}+\gamma \max _{a_i^{t+1}} Q_i^{\pi_i^*}\left(s^{t+1}, f\left(s_{-i}^t, s_{-i}^{t+1}\right), a_i^{t+1}\right)\right]
\end{aligned}
$$
### HIST-MADRL的交错学习框架

通过交错学习框架来学习高层-低层策略。

高层策略和低层策略是交替执行的，而且他们的累积奖励是相互依赖的，在计算SMADQN和SMADDPG的损失函数时，需要根据在检测范围内是否有障碍物分开讨论。


```
为了解决环境非稳态问题和组合策略问题，作者提出了一种新的分层和稳定的框架，利用分层策略学习的思想，高层策略动态选择目标，低层策略决定转向方向。

没有采用集中式的方法，把所有智能体看作一个整体，每个智能体都会考虑其他智能体行为的估计来评估其策略，这减轻了由其他智能体的未知变化引起的非平稳。 


每个智能体只能在指向其选定目标的方向范围内移动。且当目标方向没有观察到障碍物时，会直接朝目标方向进行直线运动嘛，即智能体不会尝试生成偏离目标方向的轨迹的策略。可以大大减少策略解决空间。


利用相邻时间步长的其他智能体的状态来估计他们的动作？
扩展原来的动作-价值函数为包含估计函数的复合函数
```




## 3. 策略

**状态**
- 全局观测信息：与所有目标点、其他智能体的相对位置坐标、
- 局部观测信息：激光雷达距离信息

**动作**

转向角度

**奖励**
$$
r_i^t=r_{i, \text { step }}^t+r_{i, \text { trans }}^t+r_{i, \mathrm{coll}_1}^t+r_{i, \mathrm{coll}_2}^t
$$
- 时间步长的惩罚：所有智能体到达所有目标点前，每个智能体在每个时间步都会获得负奖励。
$$
r_{i, \text { step }}^t=-C_1
$$
- 核心正奖励：如何智能体接近 *成功状态* 则为正值，否则为负值。在这里利用 *先验知识*（智能体成功状态）来量化当前状态智能体状态与成功状态的距离。

> 通常如果智能体前往不同的目标，他们通常会更接近成功状态，如果越多智能体$m$ 选择相同的目标，则偏离成功状态就越远，惩罚越多，$C_3$ 为正值。
> 
> 例外情况：智能体经常更换目标点，从而很难接近成功状态，因此为了阻止这样情况，$C_2$(正值)小于$C_1$，这样仍会对智能体进行惩罚。

$$
\begin{aligned}
\varphi\left(\mathbf{o}_{1: N}^T, \mathbf{O}_{\text {success }}\right)=& \varphi\left(\mathbf{o}_{1: N}^0, \mathbf{O}_{\text {success }}\right) \\
&+\sum^T\left(\varphi\left(\mathbf{o}_{1: N}^t, \mathbf{O}_{\text {success }}\right)-\varphi\left(\mathbf{o}_{1: N}^{t-1}, \mathbf{O}_{\text {success }}\right)\right)
\end{aligned}
$$

$$
r_{i, \text { trans }}^t=-\left(\varphi\left(\mathbf{o}_{1: N}^t, \mathbf{O}_{\text {success }}\right)-\varphi\left(\mathbf{o}_{1: N}^{t-1}, \mathbf{O}_{\text {success }}\right)\right)=C_{2}
$$
$$
r_{i, \text { trans }}^t=-m \cdot C_3
$$
- 距离惩罚：如何智能体与其他智能体、障碍物的距离小于安全距离，则$u(.)=1$，否则值为0.

$$
\begin{aligned}
&r_{i, \mathrm{coll}_1}^t=-C_4 \sum_{j=1, j \neq i}^N u\left(d_s-\left\|\mathbf{o}_{i, \mathrm{ag}_j}^t\right\|_2\right) \\
&r_{i, \mathrm{coll}_2}^t=-C_5 \sum_{k=1}^K u\left(d_s-d_{i, k}^t\right)
\end{aligned}
$$

## 4. 实验

### 环境设置
N 个智能体、N 个目标
预训练环境（无障碍物）： 15 x 15
随机障碍物环境： 36 x 36； 
智能体运行线速度速度大小：1 m/s
旋转角度：-90 ~ 90
测距：7个均匀分离的测距光束；有效测距 4m


将预训练好的目标选择策略作为未知、随机障碍物环境的初始策略。

### 预训练
在无障碍物的环境下，预训练 SMADQN，并进行评价
在训练过程中并不考虑智能体之间的冲突，目标点的数量2～6

#### 对比实验
1. SMADQN 与 Ind-DQN 和 fingerprint 进行对比
训练实验对比 + 测试实验对比

2. 奖励函数设置的影响

针对 SMADQN 算法仅使用一种奖励进行测试：
- 将核心正奖励改为离散奖励
- 时间步惩罚


#### 评价标准
- 平均回合奖励
- 归一化的平均最大导航时间
-

### 障碍物环境
随机放置目标和智能体的位置，十个障碍物被随机放置在环境中，

#### 对比实验

验证 HIST-MADRL 与单策略学习方法进行对比，

测试过程中，。单策略仅决定转向角度而不选择目标。

其中
- PreTraining + HIST-MADRL 
- low-level-only DDPG：仅学习低层策略，目标点随机分配，且在移动过程中不会进行再分配，不区分在监测范围内是否检测到障碍物
- Ind-DDPG；将 DDPG 算法扩展到 MADRL
- SMADDPG：连续动作空间
- MAAC：加入了注意力机制


此外，将HIST-MADRL 和 low-level-only DDPG额外进行对比。



#分层策略学习 #SG

不足：
- 没有考虑通信受限中断问题，每个智能体都是彼此交流彼此信息，已知所有目标点位置。
- 状态空间扰动的解决
- 算法在其他主流控制问题中的适用性问题





# 补：相关工作：


**论文2:**
预先分配目标点 // 动态分配目标点

有学习的方法和无学习的方法（non-learning methods and learning methods）
无学习的方法：
	- 设计与场景模型相关的可调参数
	- SLAM（通常需要中心规划者）



**论文 3:**


传统的方法
基于规划的两步走方法，依赖于环境地图去分配目标点，并为智能体规划无碰撞路径（全局规划），然后计算智能体沿路径移动的运动值（局部规划）。

集中式规划：目标在路径规划之前或同时 对所有智能体进行分配的，更具全局性，为了避免路径之间冲突或阻塞，会计算每个智能体的路径规划优先级。比较费时，尤其是当智能体数量增加时。

分布式规划：更具效率，根据智能体之间的通信动态调整目标分配。不过由于缺乏全局目标分配和路径规划，


智能方法：


挑战：
- 通过组合策略每个智能体获得更多奖励
	- 分层强化学习
	- 分解累积奖励；将组合策略解决方案空间分解为每个智能体的单独策略解决空间。
- 环境非稳态（因为单个智能体独立决策，其他移动智能体的移动，对该智能体来说就相当于环境在动态变化，不同于单智能体决策引发环境发生相对应变化，当前智能体决策引发的环境的变化是自身无法预料的）
	- 集中式学习框架
	- 通过监督学习或贝叶斯推理来推断其他智能体的策略