
# 1、上海数字大脑研究院《2022上半年度人工智能行业报告》
#已完成调研
[上海数字大脑研究院首次发布《2022上半年度人工智能行业报告》](https://mp.weixin.qq.com/s/3RYVWaw-ENCtsMcK202a1Q)

### 一、学术前沿发展：

**对抗训练、元学习、多任务学习**论文量占比较高；**神经辐射场方法**开始受到广泛关注。

>神经辐射场方法：视图合成技术，能够将场景由多层感知器参数化，呈现出三维场景的逼真图像 https://www.mittrchina.com/news/detail/10782


人工智能最新进展主要集中于**人工智能新技术、人工智能新框架与人工智能新应用**三个方面。

> 从论文发表以及业内信息来看，人工智能最新进展主要集中于**人工智能新技术、人工智能新框架与人工智能新应用**三个方面。在人工智能新技术领域，大模型与多模态学习取得重要突破，标志着通用人工智能的实现又进一步；在人工智能新框架领域，Google与NVIDIA纷纷布局人工智能架构，从软件框架和硬件设计等多角度加速人工智能计算，进而解放算力；在人工智能新应用领域，深度学习与强化学习分别在生命科学领域与能源开发领域大放异彩，助力探索遗传密码，实现人造太阳。


热点：
#决策大模型 #大语言模型 #多模态学习 #神经辐射场 #自监督学习

### 二、产业前沿发展：

**传媒、医疗、金融等领域已拥有较为成熟的解决方案**，信息流广告及内容推荐算法快速崛起。汽车及工业制造，目前发展较快的 AI 应用领域。

元宇宙、自动驾驶、机器人、VRAR、虚拟人、影音娱乐等，新的Al使用场景在全球范围内不断落地。

“Al+国防”备受关注，许多国家已将Al技术纳入国防战略的一部分，
### 三、资本市场动态：

*从全球范围来看，人工智能领域投融资仍然保持较高市场热度，国内市场呈现明显波动，融资阶段整体后移，不过近期投资者信心正在逐步回归。*

从投融资方向上来看，**人工智能应用场景落地**项目为投融资热门方向，其中自动驾驶以及智能医疗作为其中的主要场景受到资本市场的广泛重视。芯片半导体等硬件为代表的人工智能基础支撑方向获得资本市场的青睐

### 四、趋势预测

- 人工智能正在经历从消费互联网到工业互联网的重心下沉趋势；上半年，行业内出现了众多聚焦于汽车、医药、IoT、物流等细分领域的 AI 应用创新，工业互联网已经成为与消费互联网并重的技术与应用发展方向
- **泛化能力更强**、效果更优的的 AI 大模型是现阶段 AI 市场发展的必然趋势；自监督学习 + 预训练模型微调适配方案逐渐成为主流，
- 对应用安全性的要求日益提高，AI 正在向安全关键型产业靠拢

尽管现有人工智能技术已经取得了一系列令人瞩目的成果，但实现通用人工智能的目标任重道远，在技术层面还需要大量的探索。例如，是否有更好的架构替代基于深度神经网络的架构？是否有更快的方式提升计算性能？大模型如何在实际场景很好地落地？在应用层面，如何将算法与场景结合、技术与产业融合，是各企业必须思考的问题。




#已完成调研 
# 2、具身通用智能
#已完成调研   2024/1/02  

#具身智能 #零样本学习
- https://swarma.org/?p=47084

## 具身智能
**具身智能**，通俗来说，是指研究在环境中具有实体的智能体（如现实或仿真环境下的机器人，能够直接与环境进行物理交互）如何通过与环境的交互来取得认知能力，学习并掌握新技能新知识的一个人工智能的子领域。

在语言任务中存在某种结构对于许多其他类别的任务也能通用，即不同数据形式（视觉，蛋白质折叠，数值计算）具备一定的同构性，近期的具身智能大模型 (foundation model) 其实也是沿着这个方向进一步地规模化。

在深度学习范式下，具身智能的研究主要集中在模仿学习(Imitation Learning)和强化学习 (Reinforcement Learning, RL) 两大块。**模仿学习**通过采集特定任务的轨迹数据集并用深度神经网络来拟合状态(state)或观测(observation) 的时间序列到动作 (action) 的映射来实现技能的学习，一般来说*数据采集成本较高*。**强化学习**则是通过让智能体与环境直接交互，在交互的过程中优化预先定义好的与特定任务相关的奖励函数 (reward function) 来学习新技能，一般来说设计奖励函数需要反复迭代，且强化学习的样本效率 (sample-efficiency) 相比于模仿学习来说会低得多。

*目前没有证据表明基于强化学习的方法训练出的智能体能涌现出对其所解决的任务和环境的认知能力*，比如训练机械狗时，训练需要频繁地人为重置机械狗的位置，因为机械狗的奖励函数只鼓励它向前走，即使碰到墙也会反复向前冲撞。同时，*没有任何证据表明目前基于模仿学习的方法能够通过大规模的预训练涌现出训练集中从未出现过的技能。

>在早期具身智能的研究中使智能体学到的策略泛化到与训练数据非常相似的任务是一件非常困难的事情，神经网络并没有数据来学习到这两者在“更加抽象的层面上是相类似的”。因此**一个巨大的挑战是任务数量的组合爆炸**。

### 具身智能研究差距
- 为方便实现，机器人具体形态可能会被人为抽象，使模型不在关注机器人具体形态。
- 目前的**视觉自监督学习** (vision self-supervised learning) 还没能学习到对于世界的结构化表征，不具备足够视觉认知能力。

#视觉自监督学习
>视觉自监督学习是一种机器学习方法，旨在通过利用图像或视频数据的内在结构进行训练，而无需人工标记的监督信号。


### 解决强化学习低效学习方法
强化学习 (尤其是无模型强化学习，model-free RL) 由于无法直接获取环境动力学的梯度信息，在样本效率方面往往比模仿学习低几个量级，对于学习在物理环境中运行的策略，这样低效的学习方式是不现实的。一般来说有两种解决方案：

- 一种是通过构建一个与**真实环境类似的模拟器**，在模拟器中使用大量数据学习到一个策略，然后在真实环境中零样本泛化或在线微调；
- 另一种方案是**学习一个关于环境的模型**，并利用学到的模型来生成学习数据 ，从而极大得减少对真实环境数据的需求，基于这一想法，通过与真实物理世界进行1小时的交互就能让机械狗学会走路并抵抗外界的干扰。


### 基于深度学习范式来实现通用具身智能所面临的一些根本性挑战
- **目前的学习系统本质上仍是一个开环系统，需要人类智能的介入**(如根据学习结果，有针对性地采集更多更好的数据，调整数据的概率分布，反复迭代优化奖励函数等)**来实现闭环**，用Yann Lecun的话来说就是，目前的机器学习系统是Assisted Intelligence，而实现通用具身智能需要的是Autonomous Intelligence [10]; 
- **目前的方法还不具备从自然模态中学习到关于世界的结构化表征与抽象**(或者说世界模型)**的能力**，相对地，人类和动物在婴儿时期就能从自然模态（如视觉，听觉等直接来源于外部世界的信号）中学习并基于直觉理解物理世界的结构和运作规律(intuitive physics )，这种自然习得的认知能力是实现通用具身智能的关键。


### 实现通用具身智能的关键
目前的方法构建学习系统面临一个非常困难的实际问题：**任务指定问题**（其本质也是某种对齐问题），通俗来讲就是说想要训练一个模型来精确地完成一项人类工程师心里希望其完成的任务，是一件极其困难的事。**我们希望机器解决的任务和我们对任务的数学描述之间存在差异性**。

实现通用具身智能的一个关键问题，是如何使机器学习系统从自然模态中（如视觉，听觉）学习到关于世界的层级化抽象（或者说是世界模型，认知地图），即**如何构建一个机器学习系统使其能从自然模态学习到世界模型**。

（1）认知能力的自然涌现
感知和认知是一整体，要完美解决感知问题必然涉及到认知，反之亦然，并且认知能力并不是凭空产生的。因此合理的做法并不是通过将认知模块直接设计进智能系统，而是在同一个框架下通过**对更低层级的感知任务进行优化**，使得认知能力自然地涌现出来，这看起来也是人类和动物学习认知世界的方式。

（2）计算可行性
提出的学习方式应当是实际可行的，其中一个重要指标就是**计算可行性**(computational tractability)，**世界模型让智能体以计算可行的（computationally-tractable）方式对未来进行长期预测和规划。** 这种在**合适粗粒度**上进行规划的能力使得我们可以利用有限的计算资源有效地寻找到一个高度优化的方案，这对时间和空间上的长远期规划是至关重要的。

（3）反事实推理
人类和动物学习技能相比于目前的人工智能系统具有高得多的样本效率，其中一个很重要的原因应该是我们能够通过世界模型构想出行为和结果之间的对应关系（即因果关系）。当我们重复一个任务，我们会不断做**反事实推理**来反思中间的哪些步骤可以做得更好，这个循环往复的过程就是我们学习新技能的过程。

世界模型包含了事件间的因果关系（一种特殊的对时序的抽象），使得智能体能够进行反事实推理（counterfactual reasoning），并据此高效且自动地学习新知识和新技能。

>反事实推理是一种逻辑推理方式，它从一个已知的假设出发，假设该假设不成立，然后推断由此产生的结果。已经发生的事件或情况，但如果某些条件发生了变化，会导致结果的不同。

### 实现通用具身智能的关键
**如何构建一个机器学习系统使其能从自然模态学习到世界模型**

*（1）神经网络架构*
大脑是一个具有记忆和反馈连接的动力系统，且时刻都在预测感官信号并纠正其预测，这种层级化的预测编码架构(predictive-coding)连接了感知与认知。我们第一视角能感知到的关于世界的一切信息都是存在于大脑状态中的信息流。相比之下，当今主流的**人工神经网络架构是前馈的（feedforward），没有状态或信息反馈流**，这种架构似乎不太可能编码表征世界的状态信息。
**在多个尺度上的信息反馈循环**（feedback loop）**可能对autonomous intelligence的涌现至关重要**，而这在当前系统中是不具备的。
>transformer的上下文学习能力与其前向展开（forward pass）过程中的记忆容量（或等效于中间状态的大小）存在因果关系，而不是与模型参数的数量。记忆容量对于通用学习能力的涌现起着至关重要的作用，而如何有效地**带有记忆和反馈链接**的神经网络架构是目前还未解决的问题，其本质是一个极具挑战性的temporal credit assignment问题(即如何衡量当前时刻的某一变化对未来某一时刻结果的影响，在神经网络训练的语境下对应于根据未来某个目标函数的值如何反推以调整神经网络的参数；目前深度学习中最主流的反向传播算法就是一种credit assignment机制)

*（2）学习法则*
大脑的学习机制很有可能是局部（local）的，局部学习机制的好处在于不需要同步全局的信息，并且对噪声（大脑作为一个物理系统）有更好的鲁棒性。
局部学习规则有望解决当前AI系统与自然智能在学习方面的一些显著差别：
1）**训练和推理阶段互相分离**，在训练过程中，权重通过随机梯度下降（SGD）的反向传播进行更新，而在推理阶段通常保持不变；
2）基于统计学习的方法通常假设环境的**分布是静态的**（stationarity）（比如训练是在randomly shuffle过的封闭数据集上进行的），这一假定对于自然数据流几乎不可能得到满足；
3）目前的学习方法对于非静态的环境/任务分布无法实现**持续终身学习**（continual lifelong learning），而会碰到灾难性遗忘（catastrophic forgetting）的问题。而对于通用具身智能来说，具备持续学习能力至关重要，因为世界是不断演化的，通用智能体只有通过不断改变自己内部的状态来适应变化的环境，才能确保其能力的通用性。

*（3）目标函数*
假设世界模型的学习可以归结为优化一个目标函数，那么这个目标函数可能是什么呢，是否存在一个普遍的目标，使得这些行为得以涌现呢：
1）**自由能原理**在主动感知（active perception）的框架下统一了行动和感知，通过最小化变分（variational）自由能，鼓励智能体主动获取信息来学习世界模型，以减少其对未来观测的意外程度（surprise）。与强化学习不同的是，在主动感知的框架下智能体并不需要外部提供的与任务相关的奖励，这就避免了人为设计奖励函数会碰到的困难。
2）**通过对抗训练的方式来学习世界模型**，一个名为世界模型的神经网络和一个名为控制器的神经网络之间进行对抗性博弈，其中控制器的内在奖励可以是世界模型的信息增益、算法压缩进度或在具有可计算答案的更抽象问题上的正确性等等，他们展示了通过神经网络权重编码的自我发明思维实验进行学习的可能。

*（4）数据/环境*
训练智能体的数据和环境也对通用智能的产生起到关键的作用。目前的主流的做法在采集数据和训练模型上是分离的，而我们也看到这对于复杂的实际问题是有局限性的（自动驾驶就需要不断用当前学到的驾驶策略去采集新数据，并用新数据进一步更新驾驶策略，实现反馈闭环）
**一个适合通用智能涌现的环境应当至少具有开放式（opened-endness）非平稳性（non-stationary）的特点，比如说我们所生存的这个世界，不断地会有新的复杂性的出现**。
>目前的绝大多数强化学习的环境都是有限的，类似于单机游戏总有通关的时候，在通关以后没有任何进一步学习适应新环境的动机和必要性了，那么自然不需要涌现出适应性和通用学习能力。

## 零样本学习
**零样本学习**的目标是让模型具备对未见过的数据进行有效推断的能力。为了实现零样本学习，通常会利用**语义嵌入**（semantic embeddings  [[NLP基础#2.2 词嵌入]]，高维的符号型数据（比如单词、短语、图像等）映射到低维连续向量空间）和**元学习**（meta-learning，让模型学会如何学习）等技术，以便让模型在没有标记数据的情况下也能做出准确的预测。

> - 通过语言嵌入，学习机器人多任务策略，从而在测试集中零样本泛化解决这个任务。
> - 为降低具身智能数据采集成本，可以在大规模文本、语音和视觉任务数据集上预训练的模型，在具身任务数据集上微调，从而泛化到具身任务中



# 3、自动驾驶：World Model
#已完成调研   2024/1/5
`CVPR 2023 自动驾驶workshop上Tesla和Wayve都提到了他们在利用生成大模型方面的最新探索方向，即大模型来生成自动驾驶相关的连续视频场景，Wayve将其命名为GAIA-1，并于前段时间发布，而Tesla则将自己的尝试命名为World Model。`

自动驾驶的目标是开发无需人工干预、减少事件和提高交通效率的车辆。自动驾驶车辆需要对环境有着越来越深刻的理解。而**World Model正是打开自动驾驶未来大门的钥匙！**

（1）大规模生成仿真数据
world model凭借自动驾驶车辆采集的大量实景视频数据，利用**生成模型**生成未来场景，并和真实的未来时刻数据对比，从而构建损失，这样就可以不依赖标注信息对模型进行训练，即视觉自监督学习。世界模型可以用作仿真工具来大规模生成仿真数据，尤其是极其罕见的边缘场景（Corner Case）。
> GAIA-1生成的场景虽然很丰富，但是还是偏模糊，另外仔细观察连续帧会发现有些车子或背景会在推演过程发生形状颜色类型的跳边，这可能表明了现阶段GAIA模型还没有很好的理解事物变化的连续性。

（2） 语义信息理解
Tesla World Model不仅能够生成RGB空间图像，还能够生成类似标注的**语义信息**，而这既表明了这项技术未来被利用在标注数据生成的潜力，也说明了模型具备了一定的对于语义的理解推演能力。

（2） 未来展望
World Model更有潜力的应用方向我认为是World Model可能会成为像GPT一样的自动驾驶领域的**基础模型**，而其他自动驾驶具体任务都会围绕这个基础模型进行研发构建。
GPT利用自监督的方式通过海量数据得到一个能力非常强大的模型，这个训练好的模型直接在推理阶段被用来执行后续的任务。那么自动驾驶Foundation Model使用的是图像生成技术，而现在最成功的图像生成技术无疑是Diffusion Model。

#生成模型 #自监督训练 #视觉自监督学习  #扩散模型（Diffusion_Models）

>自监督学习（Self-Supervised Learning）是一种无需人工标签的训练方法，通过使用数据**本身的结构或内在信息**来指导模型的学习过程。传统的监督学习中，需要为每个样本提供人工标签作为训练信号。而自监督学习则通过设计一些任务，使得模型能够自行生成“伪标签”，从而进行学习。这些任务的目标是根据原始数据中的某种特定属性或关系来预测缺失或隐藏的部分。自监督学习的优势在于不依赖人工标签，可以利用大规模未标记数据进行训练，提高数据利用效率，并且可以预训练一个良好的初始模型，再进行有监督任务的微调。这对于数据稀缺或标签困难的场景具有重要意义。

# 4、中国无人机飞控算法的发展 
#已完成调研  2024/1/6

无人机飞控算法的基础是**飞行动力学和控制理论。** 飞行动力学研究飞机在空气中的运动规律，包括姿态控制、稳定性和操纵性等方面。控制理论研究如何设计控制器来实现期望的飞行动作和轨迹。这些基础理论为无人机飞控算法的设计和优化提供了理论基础。

**PID控制器**通过调节比例、积分和微分参数来实现对飞机的控制。这种方法简单易懂，但对于复杂的飞行任务和环境变化较大的情况下效果有限。
PID控制器是一种线性控制器，它主要根据给定值和实际输出值构成控制偏差，然后利用偏差给出合理的控制量。

**模型预测控制（MPC）** 通过建立飞行动力学模型和优化算法来实现对飞机的控制。这种方法可以考虑到飞机的动力学特性和约束条件，提高了飞行控制的精度和稳定性。
MPC的基本思想是**通过对系统未来行为的预测，选择当前最佳的控制策略**。它将系统建模为一个离散时间模型，并使用该模型预测系统在一段时间内的响应。然后，通过求解一个优化问题，选择使得某个性能指标最优化的控制输入。
> 智能体利用环境模型进行推演，从而选择出最优的动作序列，然后选择最有序列的第一个动作。


**自适应控制理论**是一种针对复杂系统的控制方法，主要思想是根据反馈信号来自适应地调整控制器参数，以使系统达到最佳控制效果。常见的自适应控制算法包括以下几种：

1. 参数自适应控制：该方法通过估计系统动态特性的未知参数，自适应地调整控制器参数，从而实现系统控制。常见的参数自适应控制算法包括基于最小二乘法的自适应控制、基于模型参考自适应控制等。
2. 自适应模型预测控制（Adaptive Model Predictive Control，简称AMPC）：该方法在传统MPC的基础上，引入一种自适应机制，通过在线参数估计和模型修正，实现对系统动态特性的自适应建模和控制。
3. 自适应滑模控制：该方法结合了滑模控制和自适应控制的优点，通过在线估计和调整滑模面参数，实现对系统扰动和不确定性的自适应抵消。
4. 自适应神经网络控制：该方法利用神经网络的非线性映射能力和自适应学习能力，实现对系统的自适应建模和控制。
5. 模糊自适应控制：该方法结合了模糊控制和自适应控制的特点，通过在线学习模糊规则和调整模糊参数，实现对系统的自适应建模和控制。

总之，自适应控制理论的控制算法是针对不确定性、变化性和复杂性问题提出的一系列解决方案，其应用领域广泛，并在机器人控制、航空航天、化工等领域取得了很好的应用效果。

#滑膜控制 #模糊控制

# 5、可探索的预训练强化学习Transformer：算法蒸馏

#未完成调研 2024/1/7

`DeepMind提出了算法蒸馏(Algorithm Distillation, AD) ，通过建立因果序列模型将强化学习算法提取到神经网络中。 REF：IN-CONTEXT REINFORCEMENT LEARNING WITH ALGORITHM DISTILLATION`

（1）背景介绍：
使用Transformer将离线强化学习（offline RL）问题转化为一个序列预测问题，训练得到的Transformer模型学到的不是离线数据库中的策略而是产生这个数据库中数据的强化学习算法。所以他们的方法被称为算法蒸馏（Algorithm Distillation），以此与传统的策略蒸馏（policy distillation）相区分。

（2）离线强化学习方法：
- 从不包含学习的数据中学习策略，如通过蒸馏固定的专家策略。
- 从包含学习的数据中学习，如智能体的经验缓存区

（3）研究瓶颈：目前离线强化学习的的内容（context）太小，无法捕捉到策略提升。

#策略蒸馏 #算法蒸馏  #预训练模型


【算法蒸馏 & 策略蒸馏】


Transformer可以通过模仿学习从离线RL数据中学习单任务策略，随后又被扩展为可以在同域和跨域设置中提取多任务策略。这些工作为提取通用的多任务策略提出了一个很有前景的范式：首先收集大量不同的环境互动数据集，然后通过序列建模从数据中提取一个策略。



# 6、多模态LLM做自动驾驶决策器
#已完成调研  2024-1-8
`来自商汤的最新自动驾驶大模型DriveMLM，直接在闭环测试最权威榜单CARLA上取得了SOTA成绩`
#自动驾驶大模型 #DriveMLM

简述：将图像、激光雷达、交通规则和乘客需求输入该模型，就能给出驾驶方案，并具备可解释性（为什么要这样开）

优势：驾驶逻辑可控，过程具备**可解释性**，且更擅长解决**特殊和复杂情况**。

## 研究现状

目前，自动驾驶系统主要有两种方案，**模块化**和**端到端**。
- 模块化方案顾名思义，把自动驾驶任务拆解为**感知**、**定位**和**规控**三个模块，各模块各自完成任务，最后输出车辆控制信号。
- 端到端则是一个**整体的模型**，包含了上述感知、定位等等所有模块的功能， 最后同样输出车辆控制信号。

***模块化 & 端到端对比***
模块化方案的算法依赖**专家知识**，所有规则都需要提前手写、定义。如果在实际驾驶场景中碰到没有提前写入的情况，很可能导致系统失效。比如救护车、消防车这种不会按照交通规则行驶的车辆，让自动驾驶系统自己去处理就很容易出错。

端到端方案则是依赖**数据驱动**，虽然靠大量、真实情况下的驾驶数据，可以不断驱动系统能力进行迭代，但这同样对输入的数据要求很高，需要大量的标注数据，这无异增加系统训练和迭代的成本。端到端方案的神经网络还是一个“黑盒”，决策规划都在系统内部完成，**缺乏可解释性**。万一有问题，很难像模块化方案那样发现到底是哪一部分出了问题。

而对于增强端到端方案的可解释性，近年来也有许多研究将大语言模型（LLM）引入自动驾驶系统中，但缺点是LLM输出主要是语言，无法进一步用于车辆控制。

## DriveMLM
商汤提出了**DriveMLM**模型，它和现有自动驾驶系统行为规划模块中的决策状态对齐，可实现闭环测试中操控车辆，超过之前的端到端和基于规则的自动驾驶系统方法。

![[driveMLM.png]]

***DriveMLM框架***
- **行为规划状态对齐**：将语言输出与对于车辆控制可执行的决策进行多起
- **MLM规划器**：将多模态传感器输入转化为驾驶解释和对齐的决策。包括多模型分词器和基于MLLM的解码器
	- **多模型分词器**：负责将摄像头、激光雷达、用户语言需求、交通规则等各种输入转化为统一的token embedding。
	- **基于MLLM的解码器**：基于生成的token，再生成图片描述、驾驶决策和决策解释等内容。
- **高效的数据收集策略**：以低成本生成丰富的驾驶解释和对齐的决策。数据收集全部收集自CARLA仿真器，也就是目前自动驾驶领域被使用最多的开源仿真工具和闭环测试基准。

*相比现有自动驾驶数据，DriveMLM的数据有两个不同之处：*
1. 决策部分能够**与实际行为决策模块对齐**，方便我们将MLLM规划器的输出转换为控制信号，直接控制闭环驾驶中的车辆；
2. 包含**与人类的交互数据**，可以提高系统理解人类指令并做出反应的能力。

*如何实现语言信号转换为控制信号：*
它将LLM的语言决策输出，和成熟模块化方案中规控部分的**决策状态对齐**，由此LLM输出的语言信号就可转化为车辆控制信号。

*DriveMLM的最大优势和价值主要包含三个方面：*
1. 一致的决策指令设置使得DriveMLM可以直接与现有的**模块化AD系统（如Apollo）进行对接**，无需任何重大更改就能够实现闭环驾驶，让车真的跑起来。
2. 可以**直接输入自然语言指令**传达乘客需求或高级系统消息，交给模型来处理。这样一来，自动驾驶系统便能适应越发多样、高阶的驾驶场景。
3. 基于大模型不光输出结果还能给出**逻辑推理过程**的特性，DriveMLM作出的每一个行为和选择都会跟有详细的说明来解释它为什么要这么做。




# 7、什么是大模型？一文读懂大模型的基本概念
#未完成调研
**2024-1-15**

大模型（Large Model,也称基础模型，即 Foundation Model
大语言模型（Large Language Model，LLM）
GPT（Generative Pre-trained Transformer）


## 定义
大模型是指具有**大规模参数**和**复杂计算结构**的机器学习模型。大模型本质上是一个使用海量数据训练而成的深度神经网络模型，其巨大的数据和参数规模，实现了**智能的涌现**，展现出类似人类的智能。具备**涌现能力**的机器学习模型就被认为是独立意义上的大模型，这也是其和小模型最大意义上的区别

>涌现能力：
>当模型的训练数据和参数不断扩大，直到达到一定的临界规模后，其表现出了一些未能预测的、更复杂的能力和特性，模型能够从原始训练数据中**自动学习并发现新的**、**更高层次**的特征和模式，这种能力被称为“涌现能力”。

## 特点

ChatGPT 的巨大成功,就是在微软Azur强大的算力以及 wiki 等海量数据支持下，在 Transformer 架构基础上，坚持 GPT 模型及人类反馈的强化学习（RLHF）进行精调的策略下取得的。

- 涌现能力指的是当模型的训练数据突破一定规模，模型突然涌现出之前小模型所没有的、意料之外的、能够综合分析和解决更深层次问题的复杂能力和特性，展现出类似人类的思维和智能。**涌现能力也是大模型最显著的特点之一。**
- **迁移学习和预训练**： 大模型可以通过在大规模数据上进行预训练，然后在特定任务上进行微调，从而提高模型在新任务上的性能。
- **多任务学习:** 大模型通常会一起学习多种不同的 NLP 任务,如机器翻译、文本摘要、问答系统等。
- **迁移学习和预训练**： 大模型可以通过在大规模数据上进行预训练，然后在特定任务上进行微调，从而提高模型在新任务上的性能
- **自监督学习**： 大模型可以通过自监督学习在大规模未标记数据上进行训练，从而减少对标记数据的依赖，提高模型的效能。


## 模型微调

常见的模型微调方法：
- Fine-tuning：这是最常用的微调方法。通过在预训练模型的最后一层添加一个新的分类层，然后根据新的数据集进行微调。
- Feature augmentation：这种方法通过向数据中添加一些人工特征来增强模型的性能。这些特征可以是手工设计的，也可以是通过自动特征生成技术生成的。
- Transfer learning：这种方法是使用在一个任务上训练过的模型作为新任务的起点，然后对模型的参数进行微调，以适应新的任务。



# 8、中国人工智能系列白皮书大模型技术（2023 版）

#未完成调研 **2024-1-17**


*“无标注数据预训练标注数据微调”的预训练模型*
对于基于有标注数据微调的预训练模型，其训练过程分为两个阶段：首先，在大规模的无标注数据上进行预训练，然后使用一小部分有标注数据进行微调。这种方法可以有效地利用无标注数据中的信息来提高模型的性能，同时又能够更好地适应具体的任务需求。


*大规模无标注数据预训练+指令微调+人类对齐 的大模型*

# 6、Google三连击：AutoRT、SARA-RT、RT-Trajectory
#未完成调研  [[2024-01-19]]
https://zhuanlan.zhihu.com/p/677628214

**AutoRT、SARA-RT和RT-Trajectory分别展示了更大规模的数据采集、更高效的模型和更强的泛化能力**

## AutoRT
基于大模型的数据自动化采集，整体步骤：

1. 导航到某一场景
2. 使用VLM描述场景
3. 使用LLM生成任务
4. 使用LLM对生成的任务进行过滤
5. 执行最终选中的任务并采集数据
6. 评估采集到的数据
7. 重复以上步骤（监督员偶尔会手动重置场景）


# 10、FinGPT
#已完成调研 

`FinGPT 由 AI4Finance Foundation 开发，是一种以数据为中心的工具，可使大型语言模型(LLM) 的金融数据民主化`
源码：
https://github.com/AI4Finance-Foundation/FinGPT

参考：
- https://developer.aliyun.com/article/1267636
- https://zhuanlan.zhihu.com/p/627955433
- https://blog.csdn.net/weixin_39842528/article/details/131213864

#金融领域的大语言模型   #预训练模型  #LoRA #RLSP

## 1.背景
彭博率先在金融业推出自己的50亿参数大语言模型BloombergGPT，它大约需要130万个GPU小时进行训练，每次训练的成本高达约300万美元。

而FinGPT 提供了一种更易于访问的替代方案，。它优先考虑轻量级适应，利用一些最好的开源 LLM 的优势。然后为这些模型提供财务数据，并针对**财务语言建模进行微调**。适应成本显着下降，估计每次培训不到 300 美元，使 FinGPT 成为具有成本效益的解决方案。



## 2. 实现原理
整体框架主要包含四个层面：数据源、数据处理、模型训练、应用

- **数据源层**：该层确保全面的市场覆盖，通过实时信息捕获来处理金融数据的时间敏感性。它协调从各种在线来源获取广泛的金融数据。通过整合来自新闻网站、社交媒体平台、财务报表、市场趋势等数据，该层确保了全面的市场覆盖。其目标是捕捉市场的每一个细微差别，以应对金融数据固有的时间敏感性。  

- **数据工程层**：为**实时**的自然语言处理数据处理做准备，该层解决了金融数据中高时间敏感性和低信噪比的固有挑战。该层专注于实时处理NLP数据，以解决金融数据中固有的高时间敏感性和低信噪比挑战，过滤噪音并抽取最重要的信息。  

- **语言模型层**：关注一系列的微调方法，该层应对金融数据高度动态的特性，确保模型的相关性和准确性。作为核心层，它包括各种精细调整的方法，优先考虑轻量级适应，以保持模型的更新和相关性。通过保持更新的模型，应对金融数据的高度动态性，确保其响应与当前的金融环境保持一致。  

- **应用层**：也就是面向具体实际金融任务的智能化实现，是金融服务智能化的最终呈现，覆盖各种业务、场景，形成产出，是LLM在金融领域引起变革的具体形态
![[Pasted image 20240122095002.png]]`
### 1.数据源
可保障数据安全的私有化本地自部署的大语言模型，只能寄希望于借力预训练（Pre-trained）的基础大模型进行针对性的、专业化的调优与订制。企业能自主掌握、必须自主掌握的，是数据。等大语言模型被“民主化”后，大家PK的就是**数据质量**了。

证券金融数据，有丰富来源，例如金融新闻、公司报表、公司公股东公告、重大事件通告和社交媒体关于某企业的讨论等等，有其独特的特征

### 2.数据处理

处理金融数据面临的三个主要挑战：  
• **保障时效性**：金融数据具有高度时间敏感的特点。一旦发布市场影响力大的新闻或更新，投资者需要在短暂的时间窗口内最大化其投资收益  
• **适应高动态性**：金融领域不断演变，每天都会涌现大量的新闻、社交媒体帖子和其他与市场相关的信息。频繁重新训练模型以适应这些变化在实际操作上是不切实际且成本高昂的。  
• **海淘有用信息**：金融数据通常呈现出较低的信噪比，即有用信息往往被大量无关或噪音数据所淹没。从这一海量信息中提取有价值的见解需要使用精密的技术手段。

解决方法：
- **数据清洗**：实时数据可能存在噪音和不一致性。因此，实时数据清洗涉及移除不相关的数据、处理缺失值、文本标准化（如转换为小写）和错误修正。
- **分词**：在实时应用中，需要即时进行分词处理。这涉及将文本流分解为更小的单元或标记。
- **特征提取和情感分析**：特征提取涉及将原始数据转换为机器学习模型可以理解的输入。在实时系统中，这通常需要是一个快速高效的过程。情感分析也可以在清洗后的数据上进行，将文本分类为积极、消极或中性。
- **提示工程**：Prompt Engineering是指通过提示（Prompt）的开发和优化，与LLM进行交互，以引导其产生所期望的结果，而无需对模型进行更新。
.... 

## 3.模型训练

模型训练部分包括两个部分：调用预训练好的大模型 API、对预训练好的大模型进行微调。简单的说，RLSP的微调过程，实际上是通过直接利用全市场的智慧来训练模型使其更加有效。

两种微调方式：
- Tensor Layers （LoRA）
> 在模型的Linear层，的旁边，增加一个“旁支”，这个“旁支”的作用，就是代替原有的参数矩阵W进行训练，前向推理时仍然用参数矩阵W进行计算，引入LORA部分的参数，并不会在前向推理阶段加速，网络更新时则用"旁支"进行更新，降低参数量。
> 作用：FinGPT利用低秩适应的手段，通过降低待学习的参数量
- Reinforcement learning on stock prices （RLSP）
> FinGPT还提出一个比较有趣但证券从业者也都能自然想到的精调方法，就是通过**股票价格**的强化学习微调（RLSP - Reinforcement Learning on Stock Prices）。它用股票价格作为对强化学习的反馈机制，替代ChatGPT中的人类反馈的部分。股票价格提供了一个可量化、客观的度量标准，反映了市场对新闻和事件的情绪反应。这使得股票价格成为训练模型的强大、实时的反馈机制。

## 4.应用

- 智能投顾：投资建议
- 量化交易：可以通过大量的历史数据进行数据分析和回测，从而减少人为因素的干扰，提高交易效率和稳定性。此外，量化交易还可以通过程序自动执行交易策略，避免了情绪和主观判断对交易的影响，从而降低了交易风险。
- 低代码开发


# 11、人类反馈强化学习RLHF

#人类反馈强化学习RLHF 
参考：
- https://www.appen.com.cn/blog/the-5-steps-of-reinforcement-learning-with-human-feedback/
- https://www.zhihu.com/tardis/zm/art/615708794?source_id=1003

## 1.背景
基于人类反馈的强化学习（RLHF）是一种机器学习（ML）技术，它利用人类反馈来优化 ML 模型，从而更有效地进行自我学习。

RLHF的一大优势是，它能够使模型向多元化的反馈提供者学习，帮助模型生成更能代表不同观点和用户需求的回复。这点将有助于提高输出的质量和相关性，使模型在各种情况下都更有用。

RLHF的另一个优点是，它可以帮助减少生成式[AI模型中的偏见](https://www.appen.com.cn/blog/ai-hallucinations/)。传统的机器学习方法可能容易产生偏见，因为它们严重依赖于可能偏向具有某些人口特征群体或观点的训练数据。通过使用人类反馈，RLHF可以帮助模型学习生成更平衡、更具代表性的回复，从而降低产生偏见的风险。
## 根据人类反馈进行强化学习的5个步骤

1. **从预先训练的模型开始：** 首先，使用一个经过大量数据训练的预训练模型，为特定任务生成输出。问题生成的过程是一个关键环节，它涉及到根据意图和问题领域设计许多独特的问题。
2. **监督式微调：** 然后，使用经标注数据对预先训练的模型在特定任务或领域上进行进一步训练，使之为特定任务生成更准确、更相关的输出。
3. **奖励模型训练：** 奖励模型被训练用于识别由生成模型生成的期望输出，并基于期望结果的相关性和准确性予以打分。这有助于强化生成模型的学习，并提高生成输出的质量和相关性。
4. **通过近端策略优化（PPO）进行的强化学习**：这项技术使模型能够从经验中学习，并实时适应新的情况。模型与环境互动，并接收奖惩形式的反馈，从而能够了解哪些行动会产生期望结果。
5. **红蓝对抗：** 最后，系统要经过精心安排人员的压力测试，以确保它能够处理现实世界的场景，并做出准确和相关的预测。