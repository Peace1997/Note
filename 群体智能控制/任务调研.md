
强化学习在协同制导的应用


别人是怎么做的，用的什么环境，用的什么算法，做到了怎样的程度。
你怎么去做，用什么环境/算法，为什么用这个环境/算法，你预期可以达到怎样的效果。



Open AI Gym
墨子•未来指挥官系统

基于 MFC 的制导控制仿真软件。

在 MATLAB 和 Simulink 中设计制导系统

Xsim

## 任务要求

协同制导任务大体可分为两类：
- 基于攻击时间可控的独立导引方法
- 基于弹间通信拓扑的协同导引方法




## OpenAI Gym 

[1]何兆一, 刘海颖, 黄魁华等. 面向联合全域作战的海上无人集群协同防御行动策略设计[J]. 指挥与控制学报, 2022 ——南京航空航天大学航天学院
- **背景**：海上无人集群协同防御
- **仿真平台**：Open AI Gym 
- **介绍**：将无人机与无人艇分属的两个平行平面，整合到一个二维平面中。设定防御方为红方, 入侵方为蓝方。如下为初始化战场态势。红方采用基于*MADDPG* 的协同防御策略, 而蓝方采用基于 *DDPG* 的进攻策略。
![[res1.png|300]]

[2] 甄岩, 郝明瑞. 基于深度强化学习的智能 PID 控制方法研究[J]. 战术导弹技术, 2019 (05)
- **背景**：飞行器模型参数变化和外部干扰发生，飞行器的跟踪控制问题
- **仿真平台**：Open AI Gym 
- **介绍**：对飞行器做定点仿真，基于深度强化学习（*DDPG* ) 算法对PID 参数进行调整

## 墨子•未来指挥官系统个人版

[1]施伟, 冯旸赫, 程光权, 黄红蓝, 黄金才, 刘忠, 贺威. 基于深度强化学习的多机协同空战方法研究[J]. 自动化学报, 2021 —— 国防科技大学系统工程学院

- **背景**：3v3多机协同空战
- **仿真平台**：Open AI Gym 
- **介绍**：飞机从基地起飞, 对己方基地进行护卫，对敌方的战斗机和基地进行摧毁。将单智能体*PPO*算法以 CTDE 形式扩展到多机系统，并通过专家经验、优先采样机制、自适应权重、经验共享机制等辅助任务对算法进行优化。
![[res2.png|400]]

MADDPG 深度神经网络的智能学习协同制导





| 仿真平台 |  介绍   | 
| -------- | --- |
|    Gazebo      |     |


[1]王金强, 苏日新, 刘莉等. Q-learning 强化学习协同拦截制导律[J]. 导航定位与授时, 2022 ——江南机电设计研究所
基于Q-learning算法进行协同制导律设计，并在多弹齐射(模式1)和子母弹分离 发射(模式2)两种作战模式下进行仿真验证

[1]蔡远利, 闫明明, 刘佳琪. 基于深度强化学习的时间协同制导方法及仿真[C]//第 22 届中国系统仿真技术及其应用学术年会, 2021 —— 西安交通大学自动化学院
设计了一种基于 MADDPG 时间协同制导方法