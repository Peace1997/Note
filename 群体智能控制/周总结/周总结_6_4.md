
本周的主要任务是对RL端到端运动控制策略进行调参，并进行RL+PID联合动作控制策略的设计和训练工作。相较于之前的训练版本，这两种运动控制策略的任务成功率分别提升到了70%和75%。目前，在相同超参数条件下，联合运动控制策略在成功率和稳定性方面均优于端到端运动控制策略。



# 一、 RL 端到端运动控制

通过不断训练调参，目前基于RL的端到端运动控制策略的任务成功率为 **70%** 左右（与上周相比提升了50%）。其中较为核心的修改为：
- **将之前的离散动作空间转换为连续动作空间**：此时推力的取值不在是一个固定值，而是与连续动作的取值相关；此前固定推力值的设计会额外增加调参的难度，将推力改为连续值会使运动控制更为平滑。
	- 主推力 Fe：运动空间的取值范围为[0, 1]
	- 侧向推力Fs：运动空间的取值范围为[-1,1]
- **修改奖励函数**：此前，当导弹头部与地面发生碰撞时距离目标点的距离超出阈值时，惩罚为定值（-50）；将这个惩罚修改为连续值，即导弹头部与地面碰撞的位置离目标点越远惩罚越大。

### 可视化与总结：
任务完成关键帧截图（任务完成标志：与目标点的位置距离小于一定的阈值）：
![[Pasted image 20230601151725.png|400]]
![[Pasted image 20230601151757.png|400]]
其他任务完成关键帧：
![[Pasted image 20230601152005.png|400]]
![[Pasted image 20230601152157.png|400]]
**总结**：
- 相较于之前的版本，本周训练的版本再任务成功率上有了明显的提升，但仍有 30%左右失败的概率，其中有 20%左右失败场景为导弹命中在目标区域之外，剩下 10%的失败场景为超出 x、y 规定范围。
- 目前设定任务完成的目标区域范围比较大，后期训练会缩小目标区域的范围。


# 二、 PID + RL 联合运动控制

在联合运动控制中，将PID控制器的输出作为主要的控制动作，RL的Actor网络的动作输出作为辅助控制动作：
```python
action =0.8*pid_action+0.2*rl_action
```
在 PID 运动控制中，通过几组预实验，确定三个增益参数（$K_p$、$K_i$、$K_d$）的大致取值：
- Fe：`PID(0.5, 0.01, 0.5)`
- Fs：`PID(0.5, 0.01, 0.01)`

## 可视化与总结
训练后的测试结果如下所示：
![[Pasted image 20230602094807.png|400]]

**总结**：
与端到端的RL运动控制相比，使用相同的奖励函数与超参数取值，PID+RL的运动控制策略的成功率更高（75%左右），整体表现更稳定，其失败的场景大都为导弹命中在目标区域之外：
![[Pasted image 20230602095020.png|400]]



下周任务：
- [ ] 继续 端到端运动控制 & 联合运动控制的调参优化工作
- [ ] 多导弹协同制导任务环境搭建