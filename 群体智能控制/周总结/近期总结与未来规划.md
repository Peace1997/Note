- 学习总结
	- 学习内容总结
	- 问题总结
- 阶段性成果总结
- 未来工作计划

#### 一、学习总结：
![[学习内容总结.png|600]]
**学习内容总结**
在过去两个月里，我主要学习了三个方面的内容。
- 首先是协同制导和编队控制的相关论文，我了解了整个任务的要求和目标，并总结了关键技术和方法，同时学习了导弹力学的基础知识，包括四种坐标系、坐标系角度定义和转换关系以及导弹动力学方程的推导。
- 其次，我调研了导弹/飞行器的物理仿真平台，并通过学习强化学习在导弹中的应用的论文和开源项目，对相应项目进行了复现。我选用了 Box2D 作为当前的仿真环境，并以 Lunarlander 项目为基准，修改了导弹构型、太空和地面环境以及碰撞检测等规则。为了方便日后的环境迁移，我基于 Docker 制作了相应的 Image 环境。
- 最后，我安装了 Ray 和 RLlib 框架，设计了 MDP 模型，并基于 Gymnasium 的编码范式对导弹环境进行了标准化。最终，我完成了 RLlib 和环境之间的交互测试。

**问题总结：**
1. 由于之前没有系统学习过飞行器的力学和控制系统，因此在学习《导弹飞行力学》这本书时，对于一些专业概念不太熟悉，需要在学习的过程中逐步积累。
2. 目前对导弹环境并非时分熟悉，为降低入门难度，先在2D仿真环境中进行学习，尽快熟悉导弹环境，并逐步理解其动力学和控制问题。之后，逐步迁移到3D环境中进行学习，并尝试更复杂的任务和控制方法。
3. 之前我在做强化学习任务时，通常只使用其中一种强化学习算法来与环境交互。但由于每个人的编码习惯不同，每次对于新任务或新算法，都需要进行算法与环境的适配，这是一件耗时的事情。现在随着RLlib、NNI、TianShou、PyMARL等强化学习库的快速发展，调用不同的算法或修改网络结构变得更加方便。这些库由顶尖研究实验室或公司开发，具有广泛的应用和社区支持，且算法库也会不断更新。只需要设定好环境，就可以与不同的算法进行交互。相较于单个RL算法，这些框架具有较好的可扩展性和灵活性，能够适应不同的环境。由于我是第一次接触Ray RLlib，目前仍处于探索阶段，尚未完全掌握其运行机制。不过我会继续学习并增加使用它的机会，逐渐熟悉并掌握它的各种功能和用法。

#### 二、阶段性成果总结：

我使用 Box2D 和 Gymnasium 编码范式设计了一个标准化的单导弹仿真环境，并成功地将其与 Ray RLlib 框架进行了交互。这个仿真环境要求导弹从左侧红旗出发，到达右侧的目标点。
下图为单导弹环境：

![[box2d16.png|450]]

RLlib与环境交互过程日志文件如下图所示（result.json 、progress.csv）。后面会对这些数据进行一个可视化展示。
![[log2.png|475]]

![[log3.png|475]]

#### 三、未来工作计划：
我的未来工作研究计划主要分为以下三个部分。然而，由于在学校里还需要完成两项任务，分别是基于 MARL 的协同探索小论文和毕业论文，因此下面的时间安排可能会略有变动。
![[未来工作计划.png|500]]
