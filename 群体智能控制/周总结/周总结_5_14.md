**本周主要内容**：
本周主要工作是利用 Ray 进行训练和调参，同时结合 Tune 对超参数进行优化。

**当前问题**：
由于实验室服务器使用人数较多，训练速度较慢，而且进行并行训练时经常出现崩溃的情况，导致训练中断，从而可能增加训练周期。
因此，接下来将在训练调参的同时开始准备多导弹协同制导问题的研究。



## 训练调参记录

### 1. RLlib训练

在RLlib中，`num_workers`参数用于指定使用多少个Worker来并行运行强化学习算法。Worker是一个独立的进程或线程，它们可以并行地运行RLlib中的算法，从而提高训练的速度和效率。

```
config["num_gpus"] = 0
config["num_workers"] = 5
config["framework"] = "torch"
config["lr"] =0.001
config["train_batch_size"] = 512
```

![[train1.png]]

![[train1_1.png|400]]
在该版本的训练过程中，导弹平均运行步数大概在 200 步左右，且最终每回合平均奖励为-105，导弹很快便失败了，要么是下落腿部着地，要么为出左边界。说明智能体并未学会任何智能行为。



**课程学习**：为降低训练难度，使导弹先学会以头部与地面发生碰撞；因此，基于课程学习的思想，初始时设定较低难度的任务目标：
- 减少目标点与初始点的距离
- 增加目标区域范围（任务完成奖励：当导弹头部与目标区域的地面发生碰撞时会获得一个较大的正奖励）

![[群体智能控制/img/box2d21.png|400]]



### 2.  Tune 超参数调优
指定了三个不同的学习率（0.01、0.001、0.0001）作为超参数进行调优。Tune会自动训练和评估每个学习率对应的模型，并返回表现最好的学习率和对应的模型。
```python
tune.run(
    "PPO",
    stop={"episode_reward_mean": 200},
    config={
        "env": "One_Missile_Env-v0",
        "num_gpus": 0,
        "num_workers": 1,
        "lr": tune.grid_search([0.01, 0.001, 0.0001]),
    },
    num_samples=2,
)
```
其中，通过平均回报来看，当学习率设置为 0.0001 时，模型效果最佳。因此接下来将采取 0.0001 作为训练学习率继续进行训练。

### 3. 训练进度
本周共进行了10几次训练：
![[Pasted image 20230514145740.png]]


目前表现比较好的训练结果如下所示，相较于第一次训练结果已经有了明显提升，但智能体仍未能完全满足任务要求。
![[Pasted image 20230514150911.png]]



**下周任务安排：**
- [ ] 继续进行单导弹攻击训练调参
- [ ] 进行协同制导相关内容的调研，并在单导弹环境的基础上搭建多导弹协同攻击环境