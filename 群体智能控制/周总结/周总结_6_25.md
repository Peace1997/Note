
本周主要任务是单导弹运动控制训练调参和多导弹环境完善和交互过程的编码。在单导弹制导任务中，对于随机任务环境，PID+RL联合运动控制制导成功率目前在65%左右，在一些特殊场景的表现比较差；在多导弹制导任务中，虽然完成了交互过程的代码，但是实时性较差，还需要结合Rllib相关案例进行代码优化。



# 一、单智能体环境训练

## 1. PID+RL联合运动控制
本周重点对 PID+RL 联合控制进行训练，其中对 PID 算法进行简化，并降低了联合动作中 PID 所占比例，在目前版本中整体成功率大概在 65%左右，经测试分析，当起始目标点与终止目标点距离相对较近时（如测试图 3），就经常会出现提前越过目标点的问题。
产生这样问题的原因之一：由于初始位置和目标位置是随机设置的，这种距离相对较近的场景被训练的次数是较少的，所以在该场景下智能体表现不佳，接下来将适当增加该场景出现的频次。
![[Pasted image 20230625161730.png|400]]
![[Pasted image 20230625162057.png|400]]
![[Pasted image 20230625162631.png|400]]

![[Pasted image 20230625155724.png]]

对于PID+RL 联合控制，大概在10w步左右逐步达到收敛，与RL端到端运动控制相比收敛速度提升明显，但整体的平均回报目前还不是很理想。目前是用PPO算法进行训练的，接下来也会尝试一下其他算法，看是否训练效果能得到提升。

## 2. RL端到端运动控制
受限于笔记本算力，仅进行了几组 RL 端到端运动控制，在随机任务环境中的，尝试了修改状态空间，增加循环神经网络等手段， RL 端到端运动控制的成功率目前仅有 10%左右。随着任务难度的增加，RL 端到端运动控制任务失败的一个重要原因之一是很难收集到有效的经验。对于这样的问题，可尝试的解决方案包括：
- 选用之前固定任务中能达到 70%任务成功率的网络模型作为初始网络进行训练。
- 参照模仿学习的思路，换用 off-policy 算法，将收集到的好的样本数据在训练开始前输入到经验缓存区中。
- 增加可并行任务数量，尽可能多的去收集经验。


与PID+RL 联合控制相比，端到端的RL训练难度较大，接下来的重点还是暂时放在PID+RL联合控制上。

# 二、多智能化环境搭建
本周主要参照 Rllib 有关多智能体的环境的案例和 Gymnasium 的编码规范，对多导弹环境的交互过程 `reset`、`step` 等函数进行重构。目前， `step` 函数中是通过for循环来依次完成每个导弹的运动控制，在 PID 测试过程中发现，随着导弹数量增多，导弹与地面发生碰撞的时间越来越早，即无法有效完成实时控制。
![[Pasted image 20230625173722.png|400]]
![[Pasted image 20230625173540.png|400]]
因此，接下来将参考 Rllib 中提供的有关多体控制的环境（https://github.com/ray-project/ray/tree/303df6ab80a2d0599cc4d96b1cef4fe04ea50156/rllib/examples/env ）.
重新设计运动控制的交互代码。


**下周任务：**
- 重点对单导弹环境种PID+RL联合运动控制进行训练
- 完成多导弹环境的交互过程 & 预训练过程