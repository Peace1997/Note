
# 周总结
- [ ] Ray 环境安装 & Conda虚拟环境适配
- [ ] 导弹环境的标准化（gymnasium + rllib）
- [ ] RLlib 与 自建导弹环境的交互

## 1. Ray 环境安装及适配

安装Ray：
```shell
pip install -U "ray[default]" # Install Ray with support for the dashboard + cluster launcher
pip install -U "ray[tune]"  # installs Ray + dependencies for Ray Tune
pip install -U "ray[rllib]"  # installs Ray + dependencies for Ray RLlib
```

### 问题描述与解决方法

**问题描述1**：虽然 Ray（2.3.1） 可以支持 Python3.6~Python3.10 ，但在运行测试样例时，发现有些库函数仅支持 Python3.8 及以上版本，并在 Python3.9 中标准化。

**解决方法**：我之前打包制作的 Image 的 Python 环境为 3.7，为此，我在 Anaconda 下新建了一个*python=3.9* 的虚拟环境，并更新了 Gym、Pytorch、Tensorflow 的版本。

**问题描述 2**：更新Gym版本后，新版本的 Gym（0.26）与旧版 Gym（0.21） 软件包的不适配。新版本 Gym 取消了 `from gym.envs.classic_control import rendering`，而是将其封装在每个 Gym 环境的 `render()` 函数中，并通过 `render_mode` 指定环境的类型。
**解决方法**：参照 gym0.26 中 `LunarLander.render()` 重新对我之前的可视化函数 `One_Missile_Guidance.render()` 进行了修改，并增加了 `render_mode` 这个接口。

>ref：
>- https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py
>- https://github.com/mgbellemare/Arcade-Learning-Environment/blob/master/docs/gym-interface.md


## 2. 自建导弹环境的标准化

我上周调研 Ray 与环境交互时，发现 Gym 的环境注册方式与 Ray 存在部分差异。传统的Gym自建环境与任意RL算法交互时的限制不是很严格（上周也通过DDPG与自建导弹环境进行了交互测试），但是自建环境与Rllib交互时，限制是比较高的。我按照 rllib 官方案例（../rllib/examples/custom_env.py）的格式修改了几次导弹环境后，在进行交互时还是发生了下面的问题：
```shell
ValueError: Your environment (<OrderEnforcing<PassiveEnvChecker<One_Missile_Guidance<One_Missile_Guidance-v0>>>>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
```

这是因为RLlib 对自建的环境要求是比较高的，特别是 `__init__`、 `reset`、`step` 、`render` 函数，对输入输出的限制比较高。
因此我按照**Gymnasium 的编码范式**对原导弹环境重新编写，从而去适配 RLlib 的要求。修改后的环境与原环境如下图所示，仅对发射地面进行了部分修改。

> ref：
> - https://gymnasium.farama.org/
> - https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/lunar_lander.py
> - https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py



### 3. RLlib 与 自建导弹环境的交互



重新对 导弹基体、环境可视化render 进行了封装，这部分代码要进行重写。







![[box2d11.png]]