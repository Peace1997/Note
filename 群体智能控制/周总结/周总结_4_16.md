
# 周总结
- [ ] Ray 环境安装 & Conda虚拟环境适配
- [ ] 导弹环境的标准化（gymnasium）
- [ ] RLlib 与 自建导弹环境的交互

## 1. Ray 环境安装及适配

安装Ray：
```shell
pip install -U "ray[default]" # Install Ray with support for the dashboard + cluster launcher
pip install -U "ray[tune]"  # installs Ray + dependencies for Ray Tune
pip install -U "ray[rllib]"  # installs Ray + dependencies for Ray RLlib
```

### 问题描述与解决方法

**问题描述1**：虽然 Ray（2.3.1） 可以支持 Python3.6~Python3.10 ，但在运行测试样例时，发现有些库函数仅支持 Python3.8 及以上版本，并在 Python3.9 中标准化。

**解决方法**：我之前打包制作的 Image 的 Python 环境为 3.7，为此，我在 Anaconda 下新建了一个*python=3.9* 的虚拟环境，并更新了 Gym、Pytorch、Tensorflow 的版本。

**问题描述 2**：更新Gym版本后，新版本的 Gym（0.26）与旧版 Gym（0.21） 软件包的不适配。新版本 Gym 取消了 `from gym.envs.classic_control import rendering`，而是将其封装在每个 Gym 环境的 `render()` 函数中，并通过 `render_mode` 指定环境的类型。
**解决方法**：参照 gym0.26 中 `LunarLander.render()` 重新对我之前的可视化函数 `One_Missile_Guidance.render()` 进行了修改，并增加了 `render_mode` 这个接口。

>ref：
>- https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py
>- https://github.com/mgbellemare/Arcade-Learning-Environment/blob/master/docs/gym-interface.md


## 2. 自建导弹环境的标准化

我上周调研 Ray 与环境交互时，发现 Gym 的环境注册方式与 Ray 存在部分差异。传统的Gym自建环境与任意RL算法交互时的限制不是很严格（上周也通过DDPG与自建导弹环境进行了交互测试），但是自建环境与Rllib交互时，限制是比较高的。我按照 rllib 官方案例（../rllib/examples/custom_env.py）的格式修改了几次导弹环境后，在进行交互时还是发生了下面的问题：
```shell
ValueError: Your environment (<OrderEnforcing<PassiveEnvChecker<One_Missile_Guidance<One_Missile_Guidance-v0>>>>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
```

这是因为RLlib 对自建的环境要求是比较高的，特别是 `__init__`、 `reset`、`step` 、`render` 函数，对输入输出的限制比较高。
因此我按照**Gymnasium 的编码范式**对原导弹环境重新编写，从而去适配 RLlib 的要求。修改后的环境与原环境如下图所示，仅对发射地面进行了部分修改。通过对环境进行分块，随机设置了不同高度的地形。

![[box2d14.png]]


基于Gymnasium的部分核心代码：
```python
class One_Missile_Guidance(gym.Env, EzPickle):
	def __init__(self,
				render_mode: Optional[str] = None,
				continuous: bool = False, 
				gravity: float = -9.8,
				enable_wind: bool = False,)
		self.observation_space = spaces.Box(-np.inf, +np.inf,(8,),dtype=np.float32)
        if self.continuous:
            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)
        else:
            self.action_space = spaces.Discrete(4)
        ....
     def reset(self,*,seed: Optional[int] = None,options: Optional[dict] = None):
	    ...
	    self._create_terrain()
        self._create_missile()
        if self.render_mode == "human":
            self.render()
        return self.step(np.array([0, 0]) if self.continuous else 0)[0], {}
	     
	 def step(self,action):
		 ...
	     return np.array(state, dtype=np.float32), reward, terminated, False, {}
```
> ref：
> - https://gymnasium.farama.org/
> - https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/box2d/lunar_lander.py
> - https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py



### 3. RLlib 与 自建导弹环境的交互测试

搭建好标准化的导弹环境后，设定随机动作，完成导弹与环境的交互测试：
![[box2d15.png]]



环境测试没问题后，将通过 RLlib 提供的算法与自建导弹环境进行交互测试，部分测试代码：
```python
# __rllib-custom-gym-env-begin__
import gymnasium as gym
import ray
from ray.rllib.algorithms.ppo import PPOConfig
from env.lundar_lander import LunarLander
from env.missile import One_Missile_Guidance
ray.init()
config = PPOConfig().environment(One_Missile_Guidance,env_config={"render_mode":"human"})
algo = config.build()
for _ in range(3):
    print(algo.train())
algo.stop()
# __rllib-custom-gym-env-end__
```

运行过程中可视化数据如下，其中更详细的交互数据会被保存在`ray_result`文件夹下的`result.json`、`progress.csv`文件中。
![[log1.png]]






**下周任务**：
- 根据 Ray RLlib API 学习更多交互函数
- 根据实验结果进行调参，设定更合理的状态、动作、奖励函数。