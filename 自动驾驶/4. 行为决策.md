==典型的决策规划模块分为三个模块：==
- **全局路径规划**：在接收到一个给定的行驶目的地之后，结合地图信息，生成一条全局的路径，作为为后续具体路径规划的参考；（一般生成一次） 
- **行为决策**：在接收到全局路径后，结合从 感知模块得到的环境信息（包括其他车辆与行人，障碍物，以及道路上的交通规则信息），作出具体的行为决策（例如选择变道超车还是跟随）；
- **运动规划**：根据具体的行为决策， 规划生成一条满足特定约束条件（例如车辆本身的动力学约束、避免碰撞、乘客舒适性等）的轨迹，该轨迹作为控制模块的输入决定车辆最终行驶路径。


# 一、 简述

**行为决策**： 权衡所有的输入信息，做出有效和安全的决策；无人车通过*车辆传感器感知到交通环境信息，考虑行驶区域、动静态障碍物以及车辆汇入和让行规则*，并结合*无人驾驶车辆自身的属性、无人驾驶车辆历史信息*，与无人驾驶行为库中的各种决策知识、经验**相匹配**，进而选择合适当前道路交通环境的驾驶行为。行为决策模块在**宏观**上决定了无人车如何行驶。行为决策包括行驶、 跟车、转弯、换道和停车等。

>驾驶行为决策后，其结果交给运动规划模块，运动规划模块与行为决策功能模块紧密协调配合，进行纵向的轨迹规划和横向的速度规划。

**行为决策方式**：基于规则的行为决策方法 和 基于强化学习的行为决策方法。

**任务规划**：描述了道路、车道和行驶三级任务分工，在道路级进行全局的任务规划，在车道级根据周边交通状况规划运动轨迹，行驶时根据前后车进行运动智能控制。

***行为决策的难点**

1. 首先，真实的驾驶场景千变万化，如何覆盖？

2. 其次，真实的驾驶场景是一个多智能体决策环境，包括主车在内的每一个参与者所做出的行为，都会对环境中的其他参与者带来影响，因此我们需要对环境中其他参与者的行为进行预测；如果更加准确的预测其他参与者的行为或行驶轨迹。

3. 自动驾驶车辆对于环境信息不可能做到100%的 感知，例如存在许多被障碍物遮挡的可能危险情形。这时如何决策。

综合以上几点，在自动驾驶行为决策层，我们需要解决的是在**多智能体决策的复杂环境中，存在感知不确定性情况的规划问题**。可以说这一难题是真正实现L4、L5级别 自动驾驶技术的核心瓶颈之一，近年来随着 深度强化学习等领域的快速发展，为解决这一问题带来了新的思路和曙光。


## 1. 决策规划体系结构

***1. 分层递阶式体系结构***
**串联**系统结构，各模块之间次序分明，对任务进行了自上而下的分解，使得每个模块的工作范围逐层缩小，提高求解精度。
缺点：
- 对全局环境模型的要求比较高，对传感器要求比较高
- 从环境感知到执行模块存在延迟，缺乏实时性
- 可靠性不高，其中某个模块出问题，整个系统会受到影响

***2. 反应式体系结构***
**并联**结构，每个控制层可以直接基于传感器的输入进行决策，易于适应完全陌生的环境，主要的特点是存在着多个**并行的控制回路**，针对各个局部目标设计对应的基本行为，这些行为通过协调配合后作用于驱动装置。虽然高层次对低层次有影响，但是低层次本身具有独立控制系统运动的功能，而不必等高层次处理完毕，提升了鲁棒性。
缺点：
- 需要特定的**协调机制**解决各个控制回路对同一执行机构争夺控制的冲突
- 随着任务复杂程度以及各种行为之间交互作用的增加，预测一个体系整体行为的难度将会增大，缺乏较高等级的智能。

***3. 混合式体系结构***
将分层递阶式体系结构和反应体系结构相结合。

在全局决策规划层次上，生成面向目标定义的分层递阶式体系结构。
在局部轨迹规划层次上，生成面向目标搜索的反应体系的行为分解。
![[decision.png]]


# 二、 解决方法

## 1. 基于规则

**简述**：基于规则的行为决策方法主要按场景划分，其核心思想是利用**分治原则**，将车辆周边环境分解成有层次的场景并单独解决。在单个场景中，根据本车道的车辆数据、道路基础设施和道路目标物数据，以及交通运行数据和用户出行要求等信息，运用对应的规则对汽车的驾驶行为及其参数进行决策。再将划分的单个场景的驾驶行为决策进行综合，得出最后综合的行为决策。

**评价**：基于规则的行为决策方法易于搭建和调整、实时性好、应用简单，但是该模型忽略了环境的动态性和不确定性，此外，当驾驶场景较多时，状态的划分和管理比较繁琐，多适用于简单的场景下。

### 1.1 有限状态机模型

车辆根据当前环境选择合适的驾驶行为，如停车、换道、超车、避让、缓慢行驶等模式，状态机模型通过构建**有限的有向连通图**来描述不同的驾驶状态以及状态之间的转移关系，从而**根据驾驶状态的迁移反应式地生成驾驶动作**。

该类模型忽略了环境的动态性和不确定性，此外，当驾驶场景特征较多时，状态的划分和管理比较繁琐，多适用于简单场景下，很难胜任具有丰富结构化特征的城区道路环境下的行为决策任务。

### 1.2 决策树模型

决策/行为树模型和状态机模型类似，也是通过当前驾驶状态的属性值反应式地选择不同的驾驶动作，但不同的是该类模型将**驾驶状态和控制 逻辑固化到了树形结构中**，通过**自顶向下**的“轮询”机制进行**驾驶策略搜索**。这类决策模型具备可视化的控制逻辑，并且控制节点可复用，但需要针对每个驾驶场景离线定义决策网路，当状态空间、行为空间较大时，控制逻辑将比较复杂。另外，该类模型同样无法考虑交通环境中存在的不确定性因素。


## 2. 基于知识的推理决策模型

基于知识的推理决策模型由“场景特征－驾驶动作”的映射关系来**模仿人类驾驶员**的行为决策过程，该类模型将驾驶知识存储**知识库或者神经网络**中，这里的驾驶知识主要表现为规则、案例或场景特征到驾驶动作的 映射关系。进而，通过“ **查询**”机制从 知识库或者训练过的网络结构中推理出驾驶动作。

该类模型主要包括：*基于规则的推理系统*、*基于案例的推理系统*和*基于神经网络的映射模型*。

该类模型对先验驾驶知识、训练数据的依赖性较大，需要对驾驶知识进行精心整理、管理和更新，虽然基于 神经网络的 映射模型可以省去数据标注和知识整合的过程，但是仍然存在以下缺点：

-   其“数据”驱动机制使得其对训练数据的依赖性较大，训练数据需要足够充分
-   将 映射关系固化到网络结构中，其解释性较差；
-   存在“黑箱”问题，透明性差，对于实际系统中出现的问题可追溯性较差，很难发现问题的根本原因。

## 3. 基于价值的决策模型

根据最大效用理论，基于效用／价值的决策模型的基本思想是依据选择准则在**多个备选方案**中选择出最优的驾驶策略／动作。

为了评估每个驾驶动作的好坏程度，该类模型定义了效用（utility）或价值（value）函数，根据某些准则属性定量地**评估**驾驶策略符合驾驶任务目标的程度，对于无人驾驶任务而言，这些准则属性可以是安全性、舒适度、行车效率等，效用和价值可以是由其中单个属性决定也可以是由多个属性决定。


## 4. 基于强化学习的行为决策

**简述**：用于描述和解决智能体与环境的交互过程中，通过学习策略迭代，经验回放，损失函数设计，以达成回报最大化或实现特定目标的问题。

**马尔可夫决策**：强化学习的研究都是建立在马尔可夫决策过程基础上。马尔可夫决策模型提供了一个最底层的数学架构，用于面对部分条件随机情况，或者部分条件可由决策者控制的情况下进行决策。


**评价**:传统的基于规则的行为决策系统，往往只能采取非常保守的驾驶策略，需要认为设计精细的规则用来应对复杂情况，且传统的方法忽视了车辆之间、车辆与行人之间的互动性。而强化学习可以从人类的驾驶样本中学习相应的的策略决策，并能将决策泛化到类似的驾驶情景中。不过深度强化学习难以解释，一旦系统发生故障无法进行针对性的改进，而人为构建选项图之后，每个决策细分为对应动作，再由神经网络控制，决策的整个推理过程的可解释就增强了。

# 二、 异常处理

**简述**：作为预留的智能驾驶系统安全保障机制，

- 一方面是在遇到不平、复杂路面，易造成车辆机械部件松动、传感部件失效等问题时，通过预警和容错控制维持车辆安全运行；
- 另一方面是在决策过程中，由于某些参数设置不合理，推理规则不完备等因素导致智能车在行为动作中重复出现某些错误并陷入死循环时，能够建立错误修复机制使智能汽车自主地跳出错误死循环，朝着完成既定任务的方向继续前进，以减少人工干预来解决问题。

**技术方法**：

- 建立**专家系统**，就智能汽车交叉路口通行时出现的错误状态的表现与成因进行分析、定义与规则描述，制定判断动作失败的标准；
- 研究**自适应错误修复算法**，对各错误状态的成因进行分类，并相应的制定调整策略，以产生新的动作序列。

