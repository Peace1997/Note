# 常用概念

# 伯努力分布 & 二项式分布 & 多项式分布

**_伯努利分布：_**
伯努利分布（the Bernoulli distribution，又名两点分布或者 0-1 分布，是一个离散型概率分布），若伯努利试验成功，则伯努利随机变量取值为 1。若伯努利试验失败，则伯努利随机变量取值为 0。记其成功概率为 p ($0 \leq p \leq 1$)，则失败概率为 q = 1 − p。
其概率质量函数为:
$$f_{X}(x)=p^{x}(1-p)^{1-x}= \begin{cases}p & \text { if } x=1, \\ q & \text { if } x=0 .\end{cases}$$
伯努力分布的模型参数就是其中**一个类别的发生概率**。

 **_二项式分布：_**
就是将贝努利实验重复n次（各次实验之间是相互独立的）

**_多项式分布：_**

将二项式分布推广到多个类别，所以单词观测下的多项式分布就是伯努利分布的多类别推广，多项式分布的模型参数就是**各个类别的发生概率**。
$$
f_{\text {multi }}(x ; p)=\prod_{i=1}^{C} p_{i}^{x i}
$$
> $C$表示类别数；$p$ 表示向量形式的模型参数，即各个类别的发生概率，如：$p=[0.1,0.1,0.7,0.1]$。$x$表示one-hot类型的观测值，如 $x=$ 类别3 则表示为 $x=[0,0,1,0]$ 
> 扩展到机器学习的分类问题中，一个分类模型对某个样本的输出，就代表着各个类别发生的概率，但是**对于一个样本（观测）来说，只有一个类别** ，这次观测的结果要服从上述各个类别的发生概率，也就是要服从多项式分布，所以各个类别的发生的概率 predict 自然就是这个多项式分布的参数。

# 熵 & 交叉熵
**_熵_**
熵是服从某一特定概率分布事件的理论最小平均编码长度。
$$
H(P)=\text { Entropy }=\mathbb{E}_{x \sim P}[-\log P(x)]
$$

已知离散变量 $i$ 的概率分布$P(i)$，熵的公式为：$Entropy =-\sum_{i} P(i) \log _{2} P(i)$
已知连续变量 $x$ 的概率分布 $P(x)$，熵的公式为：$Entropy =-\int_{x} P(x) \log _{2} P(x)$

只要我们知道了任何事件的概率分布，我们就可以计算它的熵；那如果我们不知道事件的概率分布，又想计算熵，就需要交叉熵来完成。

**_熵 & 方差_**

熵和方差都描述了随机变量的离散程度, 但分别侧重于**不确定性**和**离散程度**。
方差(Variance)描述了随机变量值与其均值之间的偏离程度。方差越大,说明观测值离均值越远,分布更加分散。
熵则刻画了随机变量值的不确定性或混乱程度。熵越大,说明随机变量的值越不确定,难以预测。

**_交叉熵_**

**熵的估计**
我们用预估的概率分布Q，可以计算估计的熵
$$
H(Q)=\text { Estimated\_Entropy }=\mathbb{E}_{x \sim Q}[-\log Q(x)]
$$
-   计算期望的概率分布是Q，与真实的概率分布P不同。
-   计算最小编码长度的概率是 -logQ，与真实的最小编码长度 -logP 不同。

**交叉熵**
交叉熵使用H(P,Q)表示，意味着使用P计算期望，使用Q计算编码长度；这样实际编码长度和理论最小编码长度就有了对比的意义。
$$
 \text { CrossEntropy }=H(P,Q) =\mathbb{E}_{x \sim P}[-\log Q(x)]
$$
对于期望，我们使用真实概率分布P来计算；对于编码长度，我们使用假设的概率分布Q来计算，因为它是预估用于编码信息的。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。
>H(P,Q)并不一定等于H(Q,P)，除了在P=Q的情况下，H(P,Q) = H(Q,P) = H(P)。

**交叉熵作为损失函数**
交叉熵通常用作分类问题的损失函数。交叉熵的值由正确解标签所对应的输出结果决定的
离散型交叉熵:
$$
 \text { CrossEntropy }=\mathbb{E}_{x \sim P}[-\log Q(x)] = \sum\limits_{x} - P(x)logQ(x)
$$
 P 为真实每个标签概率 \[ 1, 0, 0 \] ；因为通常最后分类结果只有一个）
Q 为模型预测的的每个标签概率（\[0.6, 0.1, 0.3\]；对各个类别的预测）
此时就可以通过交叉熵来对比模型预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小。如果预测完全正确，交叉熵的值就为0


 
 https://zhuanlan.zhihu.com/p/149186719


# 似然函数 & 极大似然估计

**_似然函数（likelihood）_**


对于多类分类问题，似然函数就是衡量当前这个以predict为参数的单次观测下的多项式分布模型与样本值label之间的似然度。

**_极大似然估计_**
多数情况下，我们是根据已知条件来推算结果，而极大似然估计是已知结果，寻求使该结果出现的可能性最大的条件，以此作为估计值。

利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值。
>极大似然的核心思想是如果现有样本可以代表总体，那么**极大似然估计就是找到一组参数使得出现现有样本的可能性最大。**
>简单来说就是**根据数据推测模型**。

极大似然估计（Maximum Likelihood Estimation，简称 MLE）是统计学中常用的一种**参数估计方法**。它是一种通过观察到的样本数据来估计模型参数的方法，其核心思想是要找到**使得观察到的样本数据出现的概率最大的那组参数值作为估计值**。

 我们已知的条件有两个：样本服从的分布模型、随机抽取的样本。我们需要求解模型的参数。



**模型**：==模型主要指的就是这些**特定的算法下的特定的参数的组合**==

极大似然估计中采样需满足一个重要的假设，就是所有的采样都是**独立同分布**的。

**应用场景**：样本太多，无法得出分布的参数值，可以采样小样本后，利用极大似然估计获取假设中分布的参数值。

**极大似然估计思想在强化学习中的应用**：
在策略梯度神经网络更新时，首先从经验回放池中采样出一批数据，这批数据有好有坏（好坏有该状态下对应的价值评定），我们希望好的数据出现的概率大一些，因此就需要调整模型的参数，即根据采样出来的样本集，反推出能使好数据出现概率更高的模型参数。




在基于策略的强化学习中，我们期望优化策略，使得**生成的轨迹能够最大化累计奖励**。其中，通过策略会生成很多的轨迹，我们期望策略能够强化累计奖励较高的那条轨迹，即**使累计奖励较高的那条轨迹出现的概率也更高一点**，因此可以将最大化轨迹的累计奖励这一目标与轨迹的概率关联，即可以将问题转换最大似然估计问题，已知条件就是我们已经采集到这个累计奖励较大的轨迹，我们期望优化策略，使得该策略能够生成该轨迹的概率提高，那么这个问题就转换成了最大化轨迹的似然估计问题。



#极大似然估计 


# 极大似然估计 & 最大化边际似然

**最大化边际似然**和**极大似然估计**在本质上都是概率模型的优化目标，但它们适用的场景和处理方式有所不同。

## 极大似然估计
适用于**没有隐变量**的简单模型，优化目标是直接拟合数据分布。
最大化参数化模型对观测数据的条件概率或概率密度：

公式： 
$$
\hat{\theta} = \arg\max_\theta \prod_{i=1}^N p_\theta (x^{(i)})
$$

或对数形式：
$$\hat{\theta} = \arg\max_\theta \sum_{i=1}^N \log p_\theta(x^{(i)})
$$
- 假设：观测数据 $x$ 是直接从模型分布 $p_\theta(x)$ 中生成的，没有引入隐变量。






## 最大化边际似然

主要用于**包含隐变量**的生成模型，通过边际化隐变量优化模型参数，通常需要近似推断或迭代算法。
在含有隐变量的模型中，通过最大化边际概率$p_\theta(x)$ （对隐变量 $z$ 积分后得到的概率），找到最优参数。
公式：
$$p_\theta(x) = \int p_\theta(x, z)dz = \int p_\theta(x|z) p_\theta(z)dz$$
如果 $z$ 是连续变量，积分通常无法解析，需要使用近似方法（如 VAE 中的变分推断或 EM 算法）来求解。
$$\hat{\theta} = \arg\max_\theta \sum_{i=1}^N \log \int p_\theta(x^{(i)}, z) \, dz
$$

为了优化边际似然，通常引入近似方法，如：
- **变分推断**：通过优化证据下界（ELBO）来近似优化。
- **EM 算法**：在隐马尔可夫模型（HMM）等场景中，逐步优化隐变量的期望和模型参数。

**边际化的目的是消除不感兴趣的变量影响**，从而聚焦于我们关心的变量的概率分布。例如：
假设我们有一组联合概率分布 $p(x, z)$，其中 x 和 z 是两个随机变量，x 是观测变量，z 是潜在变量或不关心的变量。**边际化**的目的是从联合分布中得到 x 的边际分布 $p(x)$
$$p(x) = \int p(x, z) \, dz \quad \text{（连续变量）}$$
$$
p(x)=\sum_z p(x,z)（离散变量）
$$

# 概率函数 & 概率分布函数 & 概率密度函数

## 概率函数
通过函数的形式表示概率，随机变量 $x_i$ 与概率 P (X) 的映射 $P(X = x_i)$
**概率分布**：把所有可能的出现的概率情况进行列举。

## 概率分布函数
概率分布函数F(X)是概率函数P(X)累加结果：$F(X)=P(X<x)=\sum\limits_{x_k \leq x} p_k$
![500](Normal_DIstribution_1.png)
> 当 x 的值大于3时，F(X) 接近于1

## 概率密度函数
连续型随机变量的概率函数，也叫做“概率密度函数”，概率密度函数是概率分布函数的导函数：$P(a \leq X \leq b)=F(b)-F(a)=\int_a^b f(x) d x$
![500](Normal_DIstribution_2.png)



https://zhuanlan.zhihu.com/p/138401151

# 全概率公式 & 贝叶斯公式
根据时间顺序，先发生的是A，后发生的是B；求B就用全概率公式；求A就用贝叶斯公式（路径概率）
## 全概率公式: 
$$\quad P(B)=P\left(A_{1}\right) P\left(B \mid A_{1}\right)+P\left(A_{2}\right) P\left(B \mid A_{2}\right)+\ldots+P\left(A_{n}\right) P\left(B \mid A_{n}\right)$$
## 贝叶斯公式:
$$
P\left(A_{i} \mid B\right)=\frac{P\left(A_{i}\right) P\left(B \mid A_{i}\right)}{\sum_{j=1}^{n} P\left(A_{j}\right) P\left(B \mid A_{j}\right)}=\frac{P\left(A_{i}\right) P\left(B \mid A_{i}\right)}{P\left(A_{1}\right) P\left(B \mid A_{1}\right)+\ldots+P\left(A_{n}\right) P\left(B \mid A_{n}\right)}
$$



# 先验概率 & 后验概率

## 先验概率
根据统计或自身依据经验给出的一个**概率**，即先验概率是对某一件事情发生可能性的预先客观评估。

## 后验概率
在考虑和给出相关证据或数据后所得到的**条件概率**。而后验概率是对事情发生是由某一个原因导致的概率。

> [先验概率](https://zh.m.wikipedia.org/zh-hans/%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87)通常是主观的猜测，为了使计算后验概率方便。在使用[贝叶斯定理](https://zh.m.wikipedia.org/wiki/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86 "贝叶斯定理")时，我们通过将先验概率与[似然函数](https://zh.m.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0 "似然函数")相乘，随后标准化，来得到[后验概率](https://zh.m.wikipedia.org/wiki/%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87 "后验概率")分布
> 概率和条件概率简单的区别是有无限制条件：如 P(A) 为概率，P(A|B)为条件概率



先验概率是 以全事件为背景下,A事件发生的概率，P(A|Ω)
后验概率是 以新事件B为背景下,A事件发生的概率， P(A|B)

全事件一般是统计获得的，所以称为先验概率，没有实验前的概率
新事件一般是实验，如实验B，此时的事件背景从全事件变成了B，该事件B可能对A的概率有影响，那么需要对A现在的概率进行一个修正，从P(A|Ω)变成 P(A|B)，

所以称 P(A|B)为后验概率，也就是试验(事件B发生)后的概率

**举例**：
https://zhuanlan.zhihu.com/p/26464206
已知：
	- P ($A_1$) = 0.6  、 P ($A_2$) =0.4   —— 先验概率
	- P ($B_1$ | $A_1$) = 0.8  、P ($B_2$ | $A_1$) = 0.2  、 P ($B_2$ | $A_2$) = 0.8 、 P ($B_2$ | $A_2$) = 0.2  —— 条件概率
求：已知在事件 $B_1$ 的基础上，事件 $A_1$ 发生的概率
解：利用贝叶斯公式
$$
P(A_1 | B_1) = \frac{P(A_1)P(B_1|A_1)}{P(A_1)P(B_1|A_1) + P(A_2)P(B_1|A_2)}
$$
我认为整个计算过程可以分为两块，
- 先计算$B_1$发生的概率：所以需要先计算$B_1$发生的概率：$$\frac{P(B_1|A_1)}{P(A_1)P(B_1|A_1) + P(A_2)P(B_1|A_2)}$$
- 然后在计算在 $B_1$ 发生的基础上 $A_1$ 发生的概率
> 如果没有说在$B_1$基础上$A_1$发生的概率的话，就可以直接得出$A_1$发生的概率：$P(A_1)$

因此有无计算B事件基础上发生A事件的概率，是先验概率和后验概率的区别。在B事件基础上发生A则为

#后验概率 #贝叶斯公式

# 源空间 & 目标空间

## 源空间（source domain
表示与测试样本不同的领域，但是有丰富的监督信息；

## 目标空间（target domain）：
表示测试样本所在的领域，无标签或者只有少量标签。源域和目标域往往属于同一类任务，但是分布不同。

在经典的机器学习问题中，一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”，即我们往往假设**训练集和测试集**分布一致，在训练集上训练模型，在测试集上测试，如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。

以人脸识别为例，如果用东方人人脸数据训练，用于识别西方人，相比东方人识别性能会明显下降。当训练集和测试集分布不一致的情况下，通过在训练数据上按经验误差最小准则训练的模型在测试上性能不好，因此出现了迁移学习等技术。

https://www.cnblogs.com/ying-chease/p/13846413.html


# P、NP、NPC、NP-Hard
- P 问题就是能在多项式时间内解决的问题；即算法的时间复杂度是多项式级的。比如n个数中间找到最大值，或者n个数排序之类的。
- NP 问题就是能在多项式时间验证答案正确与否的问题；即求问图中起点到终点是否有一条小于100个单位长度的路线。

# 凸性 & 非凸性



***凸函数***
一个函数$f(x)$在定义域$D$上是凸的，当且仅当对于任意$x_1,x_2\in D$和$\lambda\in[0,1]$, 满足以下不等式：
$$f(\lambda x_1+(1-\lambda)x_2)\leq\lambda f(x_1)+(1-\lambda)f(x_2)$$
- 函数图像在任意两点之间的连线位于函数曲线的上方或重合。
- 即函数的"碗口朝上”。

**在凸优化问题中，局部极小值必定是全局极小值，因此可以用梯度下降法等简单方法高效求解。**

***非凸函数***
如果函数$f(x)$不满足凸函数的定义，它就是非凸函数
- 函数图像可能有多个局部极小值。
- 函数的"碗口可能朝下"或者曲线存在多个凹陷和突起。


***Jensen 不等式***
设 f 是定义域为实数的函数，如果对所有的实数 x，f(x)的二阶导数都大于0，那么 f 是凸函数。

如果 f 是凸函数，X 是随机变量，那么：$E[f(x)]>f(E(x))$ 。当且仅当 X 是常量时，该式取等号。其中，E(X)表示 X 的数学期望。
![[Pasted image 20240326232435.png|500]]

凸性方便找最小值： https://zh.d2l.ai/chapter_optimization/convexity.html

# KL 散度
KL divergence 也称为**相对熵**（relative entropy），用来衡量两个分布的相似程度。KL散度的值越小，表示两个概率分布差异越小。

对于离散随机变量：
$$
D_{KL}(P || Q) = \sum\limits_i  P(i)log \frac{P(i)}{Q(i)} = -\sum\limits_{i} P(i) log \frac{Q(i)}{P(i)}
$$
对于连续随机变量：
$$
D_{KL}(P || Q) = \int p(x)log \frac{p(x)}{q(x)} dx= -\int p(x)log \frac{q(x)}{p(x)}dx
$$
KL 散度是**非负**且**非对称**的，即 $D_{KL}(P || Q)\neq D_{KL}(Q || P))$ 

 KL 散度可以被分解为两部分：

$$D_{\mathrm{KL}}(P\|Q)=\mathbb{E}_{P(x)}\left[\log P(x)-\log Q(x)\right]$$

$\bullet$ $\log P( x)$: 表示真实分布$P$的对数概率。
$\bullet$ $\log Q( x)$: 表示模型分布$Q$ 的对数概率。
$\bullet$ $\mathbb{E} _{P( x) }:$期望是根据真实分布$P(x)$计算的。

直观上：
- 当$Q(x)$ 与 $P(x)$ 很接近时，$\frac P(x){Q(x)}\approx1$, 此时$\log\frac P(x){Q(x)}\approx0$, KL 散度很小。
- 当$Q(x)$偏离$P(x)$时，$\frac{P(x)}{Q(x)}$偏离 1, KL 散度增大。


**举例：**
假设 $P(x)$ 和 $Q(x)$ 为两个离散分布：
- $P(x)=[0.4,0.6]$
- $Q(x)=[0.5,0.5]$
计算：

$$
D_{\mathrm{KL}}(P \| Q)=0.4 \cdot \log \frac{0.4}{0.5}+0.6 \cdot \log \frac{0.6}{0.5}
$$


分别计算每一项：
- $\log \frac{0.4}{0.5} \approx-0.223, \log \frac{0.6}{0.5} \approx 0.182$
- $D_{\mathrm{KL}} \approx 0.4 \cdot(-0.223)+0.6 \cdot 0.182=0.007$
KL 散度值较小，说明 $P(x)$ 和 $Q(x)$ 的分布相似。


# JS 散度

Jensen-Shannon 散度（Jensen-Shannon Divergence, JS 散度） 是信息论中的一种度量方法，用于衡量**两个概率分布之间的相似性**。它是对 KL 散度的**对称化和归一化改进**。

***定义***
对于两个概率分布$P$和$Q$ (定义在同一个样本空间上), JS 散度的定义为：
$$D_{\mathrm{JS}}(P\|Q)=\frac{1}{2}D_{\mathrm{KL}}(P\|M)+\frac{1}{2}D_{\mathrm{KL}}(Q\|M)$$

其中：
 - $M= \frac 12( P+ Q) :$是$P$和$Q$的平均分布。
- $D_{\mathrm{KL}}( P\| Q)$: 是 KL 散度，表示$P$相对于$Q$的信息增益。

***性质***
- **非负性**：$D_{\mathrm{JS}}(P\|Q)\geq0$。当且仅当$P=Q$时，$D_\mathrm{JS}(P\|Q)=0$。
- **对称性**：$D_{\mathrm{JS}}(P\|Q)=D_{\mathrm{JS}}(Q\|P)$。 这是 JS 散度相较于 KL 散度的一大改进。
- **归一化**：$D_{\mathrm{JS}}(P\|Q)$ 的取值范围为 $[0,\log2]$, 即最大值为 $\log2$  (当$P$和$Q$完全不重叠
时)。

 KL 散度公式为：

$$D_{\mathrm{KL}}(P\|Q)=\sum_xP(x)\log\dfrac{P(x)}{Q(x)}$$


# Jensen-Shannon Divergence（JSD）
是一种用于衡量两个概率分布之间相似度的对称指标。它是基于 Kullback-Leibler Divergence（KL 散度）的一种改进，解决了 KL 散度**不对称**的问题。

给定两个概率分布 P 和 Q，Jensen-Shannon 散度定义如下：
$$
\operatorname{JSD}(P \| Q)=\frac{1}{2} \mathrm{KL}(P \| M)+\frac{1}{2} \mathrm{KL}(Q \| M)
$$

其中， $M$ 是 $P$ 和 $Q$ 的平均分布:
$$
M=\frac{1}{2}(P+Q)
$$

而 $\mathrm{KL}(P \| M)$ 和 $\mathrm{KL}(Q \| M)$ 分别是 $P$ 和 $Q$ 相对于 $M$ 的 Kullback-Leibler 散度:
$$
\begin{aligned}
& \mathrm{KL}(P \| M)=\sum_x P(x) \log \frac{P(x)}{M(x)} \\
& \mathrm{KL}(Q \| M)=\sum_x Q(x) \log \frac{Q(x)}{M(x)}
\end{aligned}
$$


# 独立同分布

定义：是指一组随机变量中每个变量的概率分布都相同，且这些随机变量互相独立。

并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而Logistic Regression 、神经网络则在非独立的特征数据上依然可以训练出很好的模型，比如使用LR拟合用户收入，会使用很多相关联的特征，这里就不要求特征之间是独立同分布），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识




# L2 损失和高斯分布

假设已有数据集 $\mathcal{X}=\left\{\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \cdots, \mathbf{x}^{(m)}\right\}$，这些数据集来自于未知分布 $p_{\text {data }}(\mathbf{x})$ ，现在构建一个模型 $p_{\text {model }}(\mathbf{x} ; \theta)$ 来对未知分布 $p_{\text {data }}(\mathbf{x})$ 进行建模，利用**极大似然估计**的思想去估计模型参数 $\theta$ ：
$$
\theta_{M L}=p_{\text {model }}(\mathcal{X} ; \theta)=\prod_{i=1}^m p_{\text {model }}\left(\mathbf{x}^{(i)} ; \theta\right)
$$
由于累乘不易计算而且可能会影响计算精度，所以利用 `log` 操作将其转换为累加运算：
$$
\theta_{M L}=\sum_{i=1}^m \log p_{\text {model }}\left(\mathbf{x}^{(i)} ; \theta\right)
$$
我们假设模型遵从高斯分布 $p_{\text {model }}(\mathbf{x} ; \theta) \sim \mathcal{N}(\mu, 1)$ ,待学参数 $\theta := \mu$
$$
\begin{aligned}
\theta_{M L} &=\arg \max _\theta \sum_{i=1}^m \log p_{\text {model }\left(\mathbf{x}^{(i)} ; \theta\right)} \\
&=\arg \max _\theta \sum_{i=1}^m \log \frac{1}{\sqrt{2 \pi}} \exp \left\{-(x-\theta)^2\right\} \\
&=\arg \max _\theta \sum_{i=1}^m\left(\log \frac{1}{\sqrt{2 \pi}}+\left(-(x-\theta)^2\right)\right) \\
&=\arg \max _\theta-m \log \sqrt{2 \pi}-\sum_{i=1}^m(x-\theta)^2 \\
&=\arg \max _\theta-\sum_{i=1}^m(x-\theta)^2 \\
&=\arg \min _\theta \sum_{i=1}^m(x-\theta)^2
\end{aligned}
$$
根据最后结果可知，L2 损失函数等价于高斯分布的最大似然估计。即想要得到我们期望的某个分布，就需要缩小当前已有数据和分布之间的差异。

其中高斯分布概率密度函数公式为：$u$为均值，$\sigma^2$为方差
$$
f(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
$$




[L2 Loss And Gaussian Distribution](https://chuanting.github.io/blogs/post/mmse/)


# 均值和期望

平均数是一个统计学概念，期望是一个概率论概念。

**平均数是实验后根据实际结果统计得到的样本的平均值，期望是实验前根据概率分布“预测”的样本的平均值。**

> 之所以说“预测”是因为在实验前能得到的期望与实际实验得到的样本的平均数总会不可避免地存在偏差，毕竟随机实验的结果永远充满着不确定性。如果我们能进行无穷次随机实验并计算出其样本的平均数的话，那么这个平均数其实就是期望。当然实际上根本不可能进行无穷次实验，但是实验样本的平均数会随着实验样本的增多越来越接近期望，就像频率随着实验样本的增多会越来越接近概率一样。

如果说概率是频率随样本趋于无穷的极限，那么期望就是平均数随样本趋于无穷的极限。


# 对数概率

为什么要计算概率后取对数？
- 首先log是单调函数（不改变极值的位置）
- 将难以计算的累乘转换为累加，同时避免了累乘造成的精度不准确
- 打散同一轨迹上样本之间的关联性（RL）
- 扩大数据范围，加快策略更新速率

# 协变量偏移 & 内部协变量偏移
**协变量偏移**：通常用于训练集和测试集，指[[#源空间 目标空间]]分布不一致。
**内部协变量偏移**：通常用于神经网络内部，指由于反向传播，导致上层隐藏层与下层隐藏层更新速率（参数大小）差距较大。


# 正则化方法



# 协方差（Covariance）
协方差是统计学中用来**衡量两个随机变量之间的关系**的指标。它反映了两个变量如何共同变化（正向变化、反向变化）的趋势，表示这两个变量的线性关系的强度和方向。

***定义***
设有两个随机变量 $X$ 和 $Y$，它们的协方差定义为：
$$
\text{Cov}(X, Y) = E[(X - E[X])(Y - E[Y])]
$$
其中：
- $E[X]$ 表示 $X$ 的期望（平均值），$E[Y]$ 表示 $Y$ 的期望。
- $E[(X - E[X])(Y - E[Y])]$ 是 $X$ 和 $Y$ 偏离其平均值的乘积的期望。

在样本数据中，协方差可以通过以下公式计算：

$$
\text{Cov}(X, Y) = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})
$$

其中：
- $X_i$ 和 $Y_i$ 分别是第 $i$ 个样本的 $X$ 和 $Y$ 值。
- $\bar{X}$ 和 $\bar{Y}$ 分别是 $X$ 和 $Y$ 的样本均值。
- $n$ 是样本数量。

***协方差的含义***

- **正协方差**：如果协方差为正，说明 $X$ 和 $Y$ 倾向于同时增大或同时减小，表示两者存在正向关系。
- **负协方差**：如果协方差为负，说明 $X$ 和 $Y$ 倾向于一个增大时另一个减小，表示两者存在负向关系。
- **零协方差**：如果协方差接近零，说明 $X$ 和 $Y$ 之间没有明显的线性关系，彼此之间的变化是相对独立的。

***协方差矩阵***

在多维数据中，我们常用**协方差矩阵**来表示不同变量两两之间的协方差关系。对于一个 $n$ 维向量 $X = (X_1, X_2, \dots, X_n)$，其协方差矩阵定义为：

$$
\Sigma = \begin{bmatrix}
\text{Cov}(X_1, X_1) & \text{Cov}(X_1, X_2) & \cdots & \text{Cov}(X_1, X_n) \\
\text{Cov}(X_2, X_1) & \text{Cov}(X_2, X_2) & \cdots & \text{Cov}(X_2, X_n) \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}(X_n, X_1) & \text{Cov}(X_n, X_2) & \cdots & \text{Cov}(X_n, X_n)
\end{bmatrix}
$$

**协方差矩阵是对称的**，其对角线元素是各个变量的方差，非对角线元素是变量之间的协方差。

***协方差的应用***
- **特征选择**：在数据分析中，协方差可以帮助识别哪些特征具有较高的线性关系，可以用于降维方法，比如主成分分析（PCA）。
- **资产组合管理**：协方差广泛应用于金融领域，帮助衡量不同资产的关系，以优化投资组合的风险。
- **误差分析**：在工程中，协方差用于分析误差之间的相关性，以提高测量和预测的精度。

***总结***

协方差用于衡量两个变量之间的线性关系，是统计分析、特征提取及降维等领域的重要工具。虽然协方差和相关性都描述变量关系，但协方差不受限于 -1 和 1 范围，也会因变量的量纲而改变，因此在比较不同变量关系时，更常用标准化后的相关系数。

# 协方差 & 相关性

协方差和**相关性**（Correlation）都反映了两个变量之间的关系，但**协方差的值会受量纲的影响**（即变量的单位影响协方差的大小），因此不适合用于比较不同量纲的数据。为了克服这一缺点，通常会使用相关系数来衡量变量之间的线性关系：

$$
\text{Corr}(X, Y) = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y}
$$

其中 $\sigma_X$ 和 $\sigma_Y$ 分别是 $X$ 和 $Y$ 的标准差。这样得到的相关系数是一个无量纲的值，介于 -1 和 1 之间。


#  Pareto 最优法

在多目标优化中，**Pareto 最优法**（Pareto Optimality）是一种常用的方法，特别适用于多目标强化学习（MORL），因为在多目标环境中，不同的目标之间往往存在冲突。Pareto 最优法可以帮助我们找到一个最优解的集合，使得在满足所有目标的情况下，没有任何一个目标可以单独优化而不损害其他目标。

在机器人控制中常需平衡速度、能量消耗、稳定性等多个目标，通过 Pareto 最优法可以获得多个优化方案，以应对不同场景需求。

***Pareto 最优的概念***

假设有一个多目标优化问题，目标函数可以表示为 $f(x) = [f_1(x), f_2(x), \dots, f_n(x)]$，其中每个 $f_i(x)$是一个需要优化的目标。对于两个解 x 和 y，如果满足以下条件：

1. 对于所有目标 $i$，都有 $f_i(x) \le f_i(y)$.
2. 至少存在一个目标 $j$ 使得 $f_j(x) < f_j(y)$。

则称解 x **支配**（dominate）解 y。在这种情况下，如果一个解不能被任何其他解所支配，我们称这个解为 **Pareto 最优解**。

 ***Pareto 最优集（Pareto Optimal Set）：***
 在决策空间中，所有 Pareto 最优解的集合称为 Pareto 最优集。换句话说，这个集合中的解无法通过对任一目标的优化而不损害其他目标的情况下进一步改善。
    
***Pareto 前沿（Pareto Front）：***
在目标空间中，将 Pareto 最优集对应的目标值投影出来，形成一个曲线或面，称为 Pareto 前沿。这条曲线展示了各个目标间的最优折中关系。


# 拉格朗日函数

**拉格朗日函数**（Lagrangian Function）是数学优化和变分法中用于求解约束优化问题的一种工具。它将目标函数和约束条件结合在一个表达式中，通过引入**拉格朗日乘子**（Lagrange multipliers），**将带约束的优化问题转化为无约束问题**。

***定义***
设优化问题如下:
最小化 $f(x)$ （目标函数）
约束条件: 
$$g_i(x)=0, i=1, \ldots, m  (等式约束)$$
$$
h_j(x) \leq 0, j=1, \ldots, n \quad \text { (不等式约束) }
$$
则其对应的拉格朗日函数为：
$$
\mathcal{L}(x, \lambda, \mu)=f(x)+\sum_{i=1}^m \lambda_i g_i(x)+\sum_{j=1}^n \mu_j h_j(x)
$$
- $x$ ：优化变量。
- $\lambda_i$ ：等式约束的拉格朗日乘子。
- $\mu_j$ ：不等式约束的拉格朗日乘子，要求 $\mu_j \geq 0$ 。
- $\mathcal{L}(x, \lambda, \mu)$ ：拉格朗日函数。


***拉格朗日函数的优化过程***

1. 构造拉格朗日函数：将目标函数和约束条件写入拉格朗日函数中。
2. 寻找驻点 (Stationary Points): 对拉格朗日函数的所有变量求导数，得到一组方程：
$$\dfrac{\partial\mathcal{L}}{\partial x}=0,\quad\dfrac{\partial\mathcal{L}}{\partial\lambda}=0,\quad\dfrac{\partial\mathcal{L}}{\partial\mu}=0$$
3. 结合 KKT 条件求解：根据具体问题的约束类型，附加 KKT 条件解出最优解。


***示例：***

最小化 $f(x,y)=x^2+y^2$ ,  约束条件为 $g(x,y)=x+y-1=0$。
解法：
1. 构造拉格朗日函数：
$$\mathcal{L}(x,y,\lambda)=x^2+y^2+\lambda(x+y-1)$$
2. 求偏导数：

$$\dfrac{\partial\mathcal{L}}{\partial x}=2x+\lambda=0,\quad\dfrac{\partial\mathcal{L}}{\partial y}=2y+\lambda=0,\quad\dfrac{\partial\mathcal{L}}{\partial\lambda}=x+y-1=0$$

3. 解方程组：从前两个方程得$x=y$, 代入$x+y-1=0$, 得到$x=y=\frac{1}{2}$。
4. 最优解 : $x^{* }= \frac 12, y^{* }= \frac 12, f( x^{* }, y^{* }) = \frac 12.$