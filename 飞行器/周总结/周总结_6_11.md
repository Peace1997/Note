
本周的主要任务包括单导弹运动控制训练调参和多导弹环境搭建。在单导弹制导任务中，引入随机的初始位置和目标位置，以提高导弹在不同场景下的适应性，并为多导弹协同制导任务提供指导。在多导弹环境搭建方面，在单导弹环境基础上进行环境修改，并对 `reset`、`step` 等函数进行适配。

- [ ] 单导弹运动控制调参
- [ ] 多导弹环境搭建 & RL函数修改

# 一、 PID + RL 联合运动控制
在之前的设置中，每次迭代的导弹初始位置和目标位置都是固定的，经测试，训练后的模型在不同位置的泛化性较低。因此，在训练过程中，随机设置导弹的初始位置和目标位置，并调整相应超参数。

### 可视化与总结：
不同起始位置和不同目标点位置 任务完成关键帧：
![[Pasted image 20230611105824.png|400]]

![[Pasted image 20230611105844.png|400]]

![[Pasted image 20230611110200.png|400]]

![[Pasted image 20230611110011.png|400]]

**总结**：
每次训练迭代过程中随机初始化起始位置和目标位置增加了整体训练难度，因此在这样的场景中，整体任务成功率目前只有 60%左右。
经过分析，由于实验并未过多对PID本身的增益参数进行优化，而PID运动控制在整体运动控制所占比例比较高（80%），主导了运动方向，因此对不同场景的适配度较低。接下来，将对PID进行适度调参优化，并适量调整RL、PID运动控制所占比例。

# 二、 RL 端到端运动控制
对于连续运动控制任务，历史动作对当前时刻动作的选择是有影响的，因此尝试加入**循环神经网络**和**注意力机制**，来提升网络模型的综合决策能力。

因此，分别加入这两种机制，在相同超参数以及 MDP 模型上进行训练，训练结果表明，这两种机制并没有对网络模型起到很好的优化效果，反而增加了整体训练时间。

产生这样的原因可能是：在当前的状态空间中，主要包含当前时刻导弹状态信息，并没有包含上一时刻的导弹输出的动作等时序信息。因此，接下来将对状态空间进行适当修改并重新训练。

# 三、 多导弹环境搭建
在原单导弹环境基础上修改为多导弹环境，并修改相应的交互函数（`reset`、`step`、`render`等），通过事先指定动作，对每个导弹都可以完成相应的运动控制。
![[Pasted image 20230611112430.png|500]]
**总结**
- 本预想将单导弹的训练好的单体模型迁移到多导弹环境中进行测试，但是 RL 交互过程中出现了错误，目前排查到问题是：自己的多体交互范式可能与 Gymnasium 不适配，接下来将参考 Gymnasium 提供的 MARL 交互代码进行修改。
- 目前导弹间还没有加入碰撞检测，接下来也会对环境进行完善。


**下周任务**：
- 继续单导弹运动控制调参
- 多导弹任务的环境完善 & 多体决策模型设计 & 交互代码适配 
