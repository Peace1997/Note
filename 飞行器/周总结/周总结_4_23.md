
- [ ] 终止条件设置
- [ ] MDP 模型优化
- [ ] Ray、RLlib、Tune 相关内容学习


## 1. 终止条件

**终止条件**：
当导弹的头部、主体、腿部、推进器与地面发生碰撞时，则判为达到终止状态。

**任务目标：**
当导弹的头部击中目标点（右侧红旗）附近区域时，算作完成一次有效的任务目标。在此时会获得较大的正奖励。

![[box2d17.png|500]]

**问题描述**：
在之前的设置中，初始状态时腿部是与地面直接接触的，通过回调函数会直接触发终止条件，对环境进行重置。为解决该问题设计了如下两种方案。
**解决方法**：
1. 导弹的初始位置要略高于地面，如果导弹开始时没有点火，则导弹由于重力作用也会自由落体，从而与地面发生碰撞。
2. 额外给 `step` 函数传入一个记录当前步数的形参 `step`，如果 `step<100`，即使发生腿部接触碰撞的情况，也不会重置环境。

## 2. MDP模型

通过观察几组预训练的结果，对状态、动作、奖励空间进行如下调整，。为方便训练，将状态空间和动作空间归一化

### 2.1 状态空间：
- 归一化的导弹位置坐标
- 归一化的导弹线速度
- 与目标点的相对距离
- 导弹当前时刻角度
- 导弹当前时刻角速度
- 导弹头部是是否与地面发生接触
- 导弹本体、腿部、推进器是否与地面发生接触
- 上一时刻的动作
> 为方便训练，对导弹在地面坐标系的坐标系进行归一化处理

如下图所示，采用第一种导弹初始化方式，即导弹的初始位置要略高于地面
![[box2d18.png|500]]


### 2.2 动作空间：

连续空间 ` spaces.Box(-1, +1, (2,))`：
- 主推力大小：$action[0]\in[0,1]$
- 侧向推力大小：$action[1]\in[-1,1]$

离散空间 `spaces.Discrete(4)`：
- 执行无任何推力：$action=0$
- 执行主推力：$action=1$
- 执行左侧向推力：$action=2$
- 执行右侧向推力：$action=3$


#### 连续、离散动作空间 
在某些控制任务中，将连续动作空间离散化，对训练是有帮助的。与连续动作相比，离散动作会降低整个搜索空间的大小，在相同时间内，可能会比连续动作更容易学习一个轨迹。但是，在控制任务中，如何设置每个离散动作的值的大小是比较关键的，过大会造成整个运动轨迹不够平滑；过小会增大搜索空间。

经过几组测试后，当动作空间为离散空间时，当主推力和侧向推力时，主、侧向推力的大小为0.5。

#### 动力学设置
在该需要模拟的力包括：重力、空气阻力、推力。在Box2D中，`ApplyLinearImpulse`和`ApplyForce`方法都是可以应用于物体的力学函数，它们的作用和应用场景有所不同。ApplyLinearImpulse是施加一个瞬时的线性冲量，它的作用是改变物体的速度和方向；ApplyForce方法则是施加一个持续的力，它的作用是改变物体的速度、角速度和方向。因此，在本环境中：
- 通过 `ApplyForce` 这个函数来模拟导弹所受到的持续的重力、空气阻力等力。
- 通过`ApplyLinearImpulse`这个函数来模拟导弹受到的主推力和侧向推力。

为增加随机性，初始时会在机器人上施加一个随机作用力。


### 2.3 奖励空间

奖励空间的设置与状态空间的设置是相对应的，主要由两个稀疏型奖励和两个密集型奖励组成：
- 任务完成奖励：当导弹头部与目标区域的地面发生碰撞时会获得一个较大的正奖励
- 失败惩罚：
	- 导弹运行过程中，如果导弹腿部或主体与地面发生碰撞会获得一个较大的负奖励
	- 导弹运行过程中，如果导弹头部与地面碰撞，与目标点距离越远惩罚越大（设置此奖励的目的是想让智能体先学会以头部完成地面碰撞）
- 距离差值的目标奖励；t时刻导弹距离目标点的距离与t-1时刻导弹距离目标点的距离差值
- 能量消耗惩罚；期望消耗更少的能量去到达目标点。




## 3. Ray 相关内容学习

由于官方Ray  下的内容十分繁杂，通过下面的视频简要了解Ray、RLlib、Tune可以完成哪些内容。
1. [How to Use Deep RL Algorithms to Solve Reinforcement Learning Problems](https://www.youtube.com/watch?v=HteW2lfwLXM&pp=ygUFcmxsaWI%3D)
2. [Jonathan Mugan: RLlib, a Python Library for Deep Hierarchical Multi-Agent Reinforcement Learning](https://www.youtube.com/watch?v=nF02NWK1Rug)
3. [What's new in RLlib](https://www.youtube.com/watch?v=DZUiD9SIYUk&t=253s)


**下周安排：**
- 将本周学习的Ray、 RLlib、Tune学习的内容应用于自建导弹环境中，并将相应结果进行可视化展示。
- 基于 RLlib、Tune进行训练并根据实验效果调参