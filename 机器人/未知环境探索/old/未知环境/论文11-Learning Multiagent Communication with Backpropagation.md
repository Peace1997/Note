# Learning Multiagent Communication with Backpropagation



## 一、 简述



**此前研究不足**：目前多智能体的通信协议通常是预先定义好的，而非自动化地学习智能体之间的通信协议。比如在训练前机器人每个时间步通信他们的位置。

**整体流程**：提出了针对完全合作任务的CommNet网络架构，实现了通过训练的方式控制智能体间的通信过程。

**适用场景**：同构、合作型多智能体系统

**创新点**：

- 网络架构的大小是可以动态改变的，即在通信过程中支持智能体数量的动态变换，
- 通信信息在网络中间层传递，并且在每一时刻智能体可以进行多步通信。

**疑问点：**

- 对沟通交换的信息没有进行足够的解释，仅交换每个智能体的隐藏状态。
- 虽然引入了“局部沟通”，但是输入的信息还是所有智能体的，并没有实现“输入动态变化”，相比于经典的“集中训练分布执行”（CTDE）架构，模型本身更像是一个“集中训练集中执行”的架构。



## 二、 整体流程

![Paper11_1](Paper11_1.png)

### 1. 单智能体模块

单智能体模块$f^i$由左图所示，每个智能体$j$的输入向量由两个向量组成（对应于通信过程中$f^{i-1}$ 到 $f^i$）：状态向量$h_j^i$和交互向量$c_j^j$ ；输出下一个交互阶段的状态向量$h_j^{i+1}$ 。对与输入具体的计算公式如下所示：
$$
\begin{aligned}
h_{j}^{i+1} &=f^{i}\left(h_{j}^{i}, c_{j}^{i}\right) \\ 
c_{j}^{i+1} &=\frac{1}{J-1} \sum_{j^{\prime} \neq j} h_{j^{\prime}}^{i+1}
\end{aligned}
$$

其中对于$f^i$来说，是单层非线性网络层$\sigma$（tanh）。其计算公式为$h_j^{i+1} = \sigma(H^ih_j^i + C^ic_j^i)$，可以将其转换为$\mathbf{h}^{i+1}=\sigma\left(T^{i} \mathbf{h}^{i}\right)$，其中$\mathbf{h}^i$是对所有智能体$h_j^i$的拼接 ，其中  $T^i$表示为：
$$
T^{i}=\left(\begin{array}{ccccc}
H^{i} & \bar{C}^{i} & \bar{C}^{i} & \ldots & \bar{C}^{i} \\
\bar{C}^{i} & H^{i} & \bar{C}^{i} & \ldots & \bar{C}^{i} \\
\bar{C}^{i} & \bar{C}^{i} & H^{i} & \ldots & \bar{C}^{i} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\overline{\bar{C}}^{i} & \overline{\bar{C}}^{i} & \overline{\bar{C}}^{i} & \ldots & H^{i}
\end{array}\right)
$$

> 其中$\bar{C}^{i}=C^{i} /(J-1)$，$T^i$的大小可以随着交流的智能体进行动态变换。一般来说对于所有智能体$c^0_j =0$

### 2. 通信过程

一次通信过程如中间图所示，在这里包括了2个阶段的通信，即每一时刻智能体可以进行2步通信。一个灰色方格表示一个智能体(4个)，其中从$f^{i-1}$ 到$f^i$共有两部分组成

- 第一部分（蓝色）：自己上一个通信步输出的隐藏状态，即 $f_j^{i-1} \rightarrow f_j^{i}$

- 第二部分（红色）：其他智能体的上一个通信步输出的隐藏状态的平均值。

###  3. 整体架构

整体网络模块由最右边图所示，该模型$\Phi$由所有智能体模块组成一个大型集中式网络架构来实现。一次前向传播，就是一次执行策略的过程，每个时刻输入所有智能体的观察信息，输出所有智能体的动作。

>  其中$J$表示智能体的个数。在这里表示了有四个智能体。

**输入编码层**：$\mathbf{h}^0 = \mathbf{r}(s)$

输入所有智能体的状态信息 $\mathbf{s}$，输出为隐藏状态$\mathbf{h}^0$，其设计与具体问题相关，通常采用单层神经网络实现。

**中间通信层：**$\mathbf{h}^{i+1}=\sigma\left(T^{i} \mathbf{h}^{i}\right)$

利用单智能体的 $h_{j}^{i+1}、c_{j}^{i+1} $ 的计算公式，通过$K$轮迭代，整合为$h^K$。

> 若K为2，则通信层共有两层，即$\mathbf{h}^{1}、 \mathbf{h}^{2}$

**输出解码层**：$\mathbf{a} \sim q(\mathbf{h}^K)$

输入智能体$j$的隐藏状态$h_j^K$，从动作分布中进行采样$a_j \sim q(h_j^K)$。通常采用单层神经网络与 softmax 实现。



## 三、 模型扩展

### 1. Local Connectivity

相比于智能体可以在所有智能体之间进行通信，我们可以限制其只在某个范围内进行通信，所以随着智能体的不断移动，$N(j)$也会发生动态变换，所以这种设计与动态图类似
$$
c_{j}^{i+1}=\frac{1}{|N(j)|} \sum_{j^{\prime} \in N(j)} h_{j^{\prime}}^{i+1}
$$

### 2. Skip Connections

对于某些任务，可以将首层编码层的输出作为每一个通信层的智能体模块的输入。
$$
h_{j}^{i+1} =f^{i}(h_{j}^{i}, c_{j}^{i},h^0_j)
$$

### 3. Temporal Recurrence

使用RNN网络，主要的方法是使用时间步$t$替换通信步$i$，即将$f^i$换为$f^t$，在每一个时刻我们从$q(h^t_j)$中采样获得当前的动作。即使用循环神经网络替换原全连接层神经网络。

‘









通信通道传递了连续变量

可通过梯度下降的方式进行训练，

平均完全可以被attention替代，训练过程也可以用REINFORCE算法进行替
