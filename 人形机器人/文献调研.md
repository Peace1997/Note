
# 基于强化学习的人形机器人方法


Intelligent proximal-policy-optimization-based decision-making system for humanoid robots (Advanced Engineering Informatics CCF B)

发表时间：2023

主要内容：

针对仿人机器人自主运动决策和控制问题，提出了一种基于 PPO 的分层决策系统架构，上层负责高层次运动任务和步态规划，下层负责详细的关节级运动控制。
- 在上层，首先由**任务规划模块**，根据任务目标 (如移动目的地)规划出大致的运动序列；然后再由 **PPO 算法**基于上层状态空间（如目标位置、步态等），输出合适的**运动原语**（预先定义了一组基本的运动原语 (motion primitives), 如向前迈步、转弯、俯身等基本动作。）序列作为下层输入。
- 在下层，首先由 **运动原语执行模块**，根据上层选择的原语, 生成对应的关节目标轨迹；再由**约束解算器**，针对机器人的动力学约束和关节极限, 对轨迹进行优化调整, 输出满足约束的最终关节轨迹；然后利用**PPO 策略网络**，基于下层状态空间 (当前关节角度、速度等), 直接输出每个关节的力矩控制命令；最后通过**随机采样模块**， 在执行过程中, 通过蒙特卡罗随机采样预测未来可能的状态, 提高决策的鲁棒性。

> 蒙特卡罗随机采样在本文的应用：
> - 对于价值估计：再每一步决策时,系统并不是只考虑当前状态的确定性预测，而是通过蒙特卡洛采样 N 种可能的下一状态，即对于当前状态 s 和动作 a,下一状态 s'遵循某个条件概率分布 P(s'|s,a)，然后将这 N 个采样状态的价值进行平均,作为当前动作 a 在状态 s 下的近似价值估计值。
> - 对于动作输出： 不是单纯地选择当前动作价值最大的动作, 而是从可选动作集合中, 选择具有最优预期价值的动作, 其中预期价值就是通过蒙特卡罗采样估计得到
