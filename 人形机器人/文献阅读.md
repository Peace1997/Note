
# 人形机器人零样本迁移
#zero-shot #Nvidia-Isaac-Gym  
Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim 2 Real Transfer - 基于强化学习的人形机器人零样本迁移

2024年3月5日，星动纪元联合清华大学、上海期智研究院开源了人形机器人强化学习训练框架Humanoid-Gym。

***主要思想：***


***论文亮点：***
- 开源了人形机器人端到端强化学习训练框架：Humanoid-Gym
- 在 Nvidia Isaac Gym 环境设计不同地形并增加动态随机域
- 对 Mujoco 环境进行精细化校准使其接近真实环境




# 同时训练控制策略和状态估计器
**Concurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion —— 为动态和稳健的腿部运动同时训练控制策略和状态估计器**

#sim2real #ppo


***主要思想：***
该框架包含两个网络结构：策略网络结构和状态估计网络。其中，策略网络结构输出关节期望位置；状态估计网络用于输出机器人状态信息。
该在强化学习 AC 框架基础上，增加**历史状态信息**，通过一个状态估计网络对机器人的线速度、脚的高度和接触概率进行估计。并将状态估计信息输入到控制策略产生最后的动作，该状态估计器与控制策略同时进行训练。

![[Pasted image 20240419171922.png]]

***论文亮点***
由于本体的状态信息对机器人运动控制是计较重要了，为实现 sim2real，增加历史状态信息和状态估计网络，训练完成后保存状态估计网络模型和策略网络模型。经验证，该方法可以有效实现 sim2real

***其他技巧***
- 动态随机化：动态随机化对于 sim 2 real 也是有很大帮助的，其中包括
	- Observation noise：观测噪声是在训练过程中添加的。例如，在真实机器人上，关节速度的测量来自于关节位置的数值微分，会导致关节速度观测到误差
	- Motor frictions: 为不同部位的执行器增加不同的噪声
	- PD controller gains: 我们分别为位置增益 P 和速度增益 D 添加均匀噪声，以减轻电机摩擦和阻尼的影响。
	- Foot positions and collision geometry：足部位置观测和碰撞几何形状随机化，可以减少测量误差和橡胶脚变形的影响。
	- Ground friction：地面摩擦力随机化，可以使机器人在低摩擦力的湿滑路面和高摩擦力的沥青路面进行运行。
	- 


# RL无模型的控制与有模型安全性导航
**Bridging Model-based Safety and Model-free  Reinforcement Learning through System  Identification of Low Dimensional Linear Models -- 通过低维线性模型的系统辨识将基于模型的安全性和无模型强化学习相结合**


 #System-Identification  #model-base #model-free

***主要思想：***
这篇论文通过系统辨识的方法对强化学习控制的动态非线性的闭环系统建模为低维线性模型，并在该线性模型基础上应用基于模型的方法（NMPC-DCBF）进行局部规划，确保无模型强化学习运动控制系统的安全性。

![[Pasted image 20240814113656.png|425]]
![[Pasted image 20240814143226.png|450]]

***论文亮点：***
- 提供了一种有模型算法和无模型强化学习算法相结合的方式
- 将整个无模型强化学习算法闭环系统通过系统辨识建模为一个线性模型，有助于上层基于模型的算法进行安全性规划

***评价：***
- 这个线性模型的是需要有条件的，
	1. 需要有一个较好的 RL 策略
	2. 步态频率（输入频率）不能超过某个阈值（文中为：0.6hz），当步态频率超过0.6 Hz 时,Cassie 机器人的系统就会表现出明显的非线性特性。


# AMP：加入参考运动步态进行对抗性训练


# VMP：通过自监督技术学习潜在运动特征
#VAE

VAE：生成式模型



# WoCoCo: 序列化接触全身运动控制

***主要思想***
论文介绍了一种名为 WoCoCo (Whole-Body Control with Sequential Contacts)的全新框架，旨在利用**强化学习**方法对具有**序列式接触的人形机器人进行全身运动控制**
![[Pasted image 20241011154144.png]]


***创新点：***
- **基于接触阶段的任务分解：**
    - 定义了接触目标和任务目标：接触目标规定了机器人哪些部位需要与环境发生接触，而任务目标则规定了在完成接触目标的前提下，机器人还需要满足哪些额外条件。
    - 优势：将原本复杂的长视野任务分解成多个相对简单的短视野子任务，从而降低学习难度并提高学习效率。
- **好奇心奖励**：
    - WoCoCo 提出一种基于哈希的随机神经网络 (random neural network based hash) 的方法，通过在状态空间上进行**计数**来衡量状态的新颖性。

> 通过固定神经网络，可以将输入数据映射到一个固定空间，类似于哈希表的查找操作。



***任务建模：***
![[Pasted image 20241011155401.png]]
- 任务无关的奖励：在不同的任务场景中可以共用，第一项奖励 $r_{WoCoCo}$ 主要包括三项：接触奖励、阶段奖励、好奇心奖励。
![[Pasted image 20241011155621.png|500]]
- 任务相关的奖励：不同的任务场景（跑酷、搬运、舞蹈、攀岩），需要针对性的调整几项任务奖励。


![[Pasted image 20241011154144.png]]



***Sim2Real***
- **课程学习**：
    - 训练的第一阶段：不采用域随机化的方法，并加入好奇心奖励
    - 训练的第二阶段：加入域随机化直至模型收敛
    - 训练的第三阶段：每 2000 次迭代将正则化奖励项的权重增加 20%，直到权重翻倍，从而引发更保守的行为。
- **正则化奖励**：用于对机器人的某些行为进行惩罚，来引导机器人学习更安全、更自然的动作。

![[Pasted image 20241011155321.png]]



**Intelligent proximal-policy-optimization-based decision-making system for humanoid robots —— 基于近端策略优化的仿人机器人智能决策系统**
(Advanced Engineering Informatics CCF B)

#ppo

发表时间：2023

主要内容：

针对仿人机器人自主运动决策和控制问题，提出了一种基于 PPO 的分层决策系统架构，上层负责高层次运动任务和步态规划，下层负责详细的关节级运动控制。
- 在上层，首先由**任务规划模块**，根据任务目标 (如移动目的地)规划出大致的运动序列；然后再由 **PPO 算法**基于上层状态空间（如目标位置、步态等），输出合适的**运动原语**（预先定义了一组基本的运动原语 (motion primitives), 如向前迈步、转弯、俯身等基本动作。）序列作为下层输入。
- 在下层，首先由 **运动原语执行模块**，根据上层选择的原语, 生成对应的关节目标轨迹；再由**约束解算器**，针对机器人的动力学约束和关节极限, 对轨迹进行优化调整, 输出满足约束的最终关节轨迹；然后利用**PPO 策略网络**，基于下层状态空间 (当前关节角度、速度等), 直接输出每个关节的力矩控制命令；最后通过**随机采样模块**， 在执行过程中, 通过蒙特卡罗随机采样预测未来可能的状态, 提高决策的鲁棒性。

> 蒙特卡罗随机采样在本文的应用：
> - 对于价值估计：再每一步决策时,系统并不是只考虑当前状态的确定性预测，而是通过蒙特卡洛采样 N 种可能的下一状态，即对于当前状态 s 和动作 a,下一状态 s'遵循某个条件概率分布 P(s'|s,a)，然后将这 N 个采样状态的价值进行平均,作为当前动作 a 在状态 s 下的近似价值估计值。
> - 对于动作输出： 不是单纯地选择当前动作价值最大的动作, 而是从可选动作集合中, 选择具有最优预期价值的动作, 其中预期价值就是通过蒙特卡罗采样估计得到



#sim2real  