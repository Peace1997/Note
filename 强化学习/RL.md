---
aka description: 深度Q网络(Deep Q-network）
---

## 一、简述

DQN 是基于深度学习的 **Q 学习**算法，主要结合了价值函数近似与神经网络技术，并采用目标网络和经验回放的方法进行网络的训练。

#### Critci 学习对象
DQN 是基于价值的算法，在基于价值的算法里，我们**学习的不是策略， 而是 Critic**（评论员）， Critic 用于评价现在 Actor 输出动作的好坏,即策略评估。

#### Critic 类型： 
- **状态价值函数**：输入是一个状态，它根据状态计算出这个状态以后的期望的累积奖励
- **动作状态价值函数**：输入是一个状态-动作对，其指在某一个状态采取某一个动作，假设我们都使用策略 $\pi$ ，得到的累积奖励的期望值有多大。

>Critic 评估的不是当前状态下采取动作的即时奖励，而是期望累积奖励，即预期到该回合结束时，可以获得多大的奖励。

#### 衡量Critic的方法：
- 基于蒙特卡洛的方法
- 基于时序差分的方法



Critic 通常需要绑定一个 Actor，它是衡量 Actor 的好坏，而不是衡量一个状态的好坏，Critic 的输出与 Actor 有关，状态的价值则取决于 Actor，当 Actor 改变的，状态价值函数的输出也会跟着改变。
> 这是因为 Critic 使用状态动作值函数 Q (s, a) 进行评价，而 PPO 中的则是使用状态价值函数 V (s) 进行评价。


