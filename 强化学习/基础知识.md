# 基础知识

## 一、 强化学习概述

**定义**：强化学习（reinforcement learning）是智能系统(智能体)在与环境的连续**互动**中学习最优行为策略的机器学习问题。

**核心**：强化学习的核心就是 策略和奖励，策略就是给定状态下采取的动作，**根据奖励信号去改进策略**。

**目标**：智能系统的目标不是短期奖励的最大化，而是长期**累积奖励的最大化**。

**本质**：强化学习的本质是智能体在环境中通过不断**试错** (trial and error)并利用环境的延迟回报进行学习。

> 强化学习的过程中，没有监督数据进行指导，智能系统不断地试错，根据奖励信号（不一定是实时），以达到学习最优策略的目的。

智能体与环境交互获得观测信息，智能体根据观测信息决定要对环境实施一个行为，环境会根据该行为做出相应改变并给予一定的反馈信息（奖励），智能体根据接收的反馈信息，建立“自身状态“”所施行为”以及“所得反馈”之间的联系，作为**自身记忆**的一部分给后续的决策提供参考。

#### 监督学习 & 无监督学习

* 监督学习（supervised learning）：从标注数据中学习预测模型的机器学习方法，其本质是学习**输入到输出的映射**的统计规律。
* 无监督学习（unsupervised learning）：从无标注数据中学习预测模型的机器学习问题，其本质是学习数据中的统计规律或**潜在结构**。

#### 数学描述

强化学习问题是从策略集寻找最优策略，因此强化学习是一个**优化问题**。

强化学习也是一类决策问题。完成任务过程中，智能体要作出一系列决策。而在决策时，智能体不仅要考虑到当前回报，还要兼顾长远利益，因此强化学习的问题是**序列决策问题**。其数学模型是马尔可夫决策过程。

强化学习理论的一个突出特点是利用数学模型的性质（Markov性），针对性地引入有效的采样-估计（MC、TD）。

> 估计通常也称为自举，自举的思想就是根据其他估算值来更新估算值。

## 二、 马尔可夫决策过程

### 2.1 定义

如果序列决策过程所有状态都具有**Markov性**，且环境状态是**完全可观测的**，则称该过程为**Markov决策过程**。如果环境状态是**部分可观测**的，则称其为**部分可观测Markov决策过程**。

==**马尔可夫性质（Markov property）是指一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态**==

$$
P[S_{t+1} \mid S_t] = P[S_{t+1} \mid S_1,S_2,...,S_t]
$$

其中 $P$ 是条件概率
由${S_1,S_2,....S_t,....}$组成的序列称为Markov链（离散时间的马尔可夫过程）。

相对于马尔可夫奖励过程，马尔可夫决策过程多了决策（决策是指动作），其他的定义与马尔可夫奖励过程的是类似的。

### 2.2 基础公式

#### 1. 状态转移概率

与MRP相比，其中状态转移也多了一个条件 ，未来的状态不仅依赖于当前的状态也依赖于当前状态智能体采取的动作。

$$
p(s_{t+1} = s' \mid s_t = s ,a_t =a)
$$

#### 2. 状态-动作期望奖励：

$$
R(s_t = s, a_t = a) = \mathbb{E} [r_t \mid s_t =s , a_t= a]
$$

> 对于非确定性策略，在同一状态下可能有多种动作选择，因此需要求解期望

#### 3. 策略函数

策略定义了在某一个状态应该采取什么样的动作。知道当前状态后，我们可以把当前状态代入策略函数来得到一个概率，即

$$
\pi(a \mid s) = p(a_t=a \mid s_t=a)
$$

概率代表在所有可能的动作里怎么采取行动。策略分为两种确定性策略和不确定性策略。对于确定性策略它有可能直接输出一个值，或者直接告诉我们当前应该采取什么样的动作，而不是一个动作的概率。假设概率函数是平稳的（stationary），不同时间点，我们采取的动作其实都是在对策略函数进行**采样**。

已知马尔可夫决策过程和决策$\pi$，可以把马尔可夫决策过程转换成马尔可夫奖励过程，**状态转移概率和奖励函数**可以表示为：

$$
\begin{align*}
p_{\pi}(s' \mid s) = \sum_{a\in A} \pi(a \mid s) p (s' \mid s,a) \\ r_{\pi}(s) = \sum_{a\in A} \pi(a \mid s) R(s,a)
\end{align*}

$$

> 通常 $\mid$ 表示**不确定**，例如在这里 $\pi(a \mid s)$ , 表示在状态 $s$ 下可选择的 $a$ 有很多，不一定选择哪一个 $a$, 而 $R(s,a)$, 已经确定好采取那个动作 $a$ , 是**确定的**。


#### 4. 价值函数

MDP价值函数可以定义为在某一状态下，它可能得到的回报的期望。

$$
\begin{aligned} V_\pi(s) &=\mathbb{E}\left[G_{t} \mid s_{t}=s\right] \\ \end{aligned}
$$

其中，期望基于我们采取的策略，当策略决定后，我们通过对策略进行**采样**得到一个期望，计算出它的价值函数，

动作价值函数（Q函数）可以定义为在某一状态下采取某一动作，它可能得到的回报的期望。

$$
Q_\pi(s,a) =\mathbb{E}\left[G_{t} \mid s_{t}=s,a_t=a\right]
$$

这里的期望其实也是基于策略函数的。所以我们需要对策略函数进行一个加和，然后得到它的价值。对 Q函数中的动作进行加和，就可以得到价值函数:

$$
V_\pi(s) = \sum_{a \in A} \pi(a \mid s)Q_\pi(s,a)
$$

#### 5. MRP & MDP

马尔可夫决策过程（MDP）它的中间多了一层动作$a$，即智能体在当前状态时，首先决定采取某一种动作，这样我们会到达某一黑色节点，到达这个黑色节点后，因为存在一定的不确定性，所以当智能体当前状态以及智能体当前采取的动作决定过后，智能体进入未来的状态其实也是一个概率分布。

![MRP & MDP](img/MRP&MDP.png)


### 2.3 贝尔曼方程

贝尔曼方程就是**当前状态与未来状态的迭代关系**，表示当前状态的价值函数可以通过下个状态的价值函数来计算。

> **另一种计算计算某状态价值的方法就是 MC**, 在某一状态下，多次进行采样，获得每条轨迹对应的回报值，然后求平均值来计算该状态下的价值。

从状态价值函数里，我们可以推导出状态价值函数的贝尔曼方程：

$$
V_\pi(s) =R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V_\pi \left(s^{\prime}\right)
$$

主要包含两部分：**即时奖励和未来奖励折扣的总和**。

动作价值函数（Q函数）贝尔曼方程：(其推导过程与贝尔曼方程近似)

$$
Q_\pi(s,a) = R(s,a) + \gamma \sum_{s' \in S}p(s' \mid s,a)V_\pi (s')
$$

利用公式$$V_\pi(s) = \sum_{a \in A} \pi(a \mid s)Q_\pi(s,a)$$ 我们可以推导出状态价值函数与动作价值函数的关联：

> 贝尔曼方程推导：(从价值函数中进行推导)
> $$
> \begin{aligned} V(s) &=\mathbb{E}\left[G_{t} \mid s_{t}=s\right] \\ &=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots \mid s_{t}=s\right] \\ &=\mathbb{E}\left[r_{t+1} \mid s_{t}=s\right]+\gamma \mathbb{E}\left[r_{t+2}+\gamma r_{t+3}+\gamma^{2} r_{t+4}+\ldots \mid s_{t}=s\right] \\ &=R(s)+\gamma \mathbb{E}\left[G_{t+1} \mid s_{t}=s\right] \\ &=R(s)+\gamma \mathbb{E}\left[V\left(s_{t+1}\right) \mid s_{t}=s\right] \\ &=R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right) \end{aligned}
> $$
> 倒数第二步由**全期望公式**推导而来。注意区分r 、R

### 2.4 贝尔曼期望方程

定义了基于策略$\pi$的状态价值函数和动作价值函数后，根据贝尔曼方程，我们可以得到相应的**贝尔曼期望方程**。

$$
\begin{align*}
& V_\pi(s) = \mathbb{E}_\pi[r_{t+1} +\gamma V_\pi(s_{t+1}) \mid s_t =s ]\\\\
& Q_\pi(s,a) = \mathbb{E}_\pi[r_{t+1} +\gamma Q_\pi(s_{t+1},a_{t+1}) \mid s_t =s , a_t =a]
\end{align*}
$$

将 Q 函数贝尔曼方程 ：
$$Q_\pi(s,a) = R(s) + \gamma \sum_{s' \in S}p(s' \mid s,a)V_\pi (s')$$ 带入价值函数与 Q 函数关联公式：
$$V_\pi(s) = \sum_{a \in A} \pi(a \mid s)Q_\pi(s,a)$$ 我们可以得到**当前状态的价值与未来状态价值的关联**（贝尔曼期望方程的另一种形式）

$$
V_{\pi}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{\pi}\left(s^{\prime}\right)\right)
$$

同理，将价值函数与Q函数关联公式带入Q函数贝尔曼方程，我们可以得到**当前时刻的Q函数与未来时刻的Q函数的关联**。

$$
Q_{\pi}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) Q_{\pi}\left(s^{\prime}, a^{\prime}\right)
$$

### 2.5 贝尔曼最优方程

解决强化学习问题，意味着我们要寻找一个最优的策略（让个体在与环境交互过程中获得始终比其他策略都要多的收获）。通过不断执行**贪心操作**（arg max 操作），我们就可以得到一个最佳的策略，而不会使价值函数变差。当改进停止后，我们就会得到一个最佳策略，即当改进停止后，我们取让 Q 函数值最大化的动作，Q 函数就会直接变成价值函数：

$$
Q_{\pi}\left(s, \pi^{\prime}(s)\right)=\max _{a \in A} Q_{\pi}(s, a)=Q_{\pi}(s, \pi(s))=V_{\pi}(s)
$$

我们也就可以得到**贝尔曼最优方程(Bellman optimality equation)：**

$$
V_\pi(s) = \max_{a \in A} Q_\pi(s,a)
$$

贝尔曼最优方程表明：**最佳策略下的一个状态的价值必须等于在这个状态下采取最好动作得到的回报的期望。** 当马尔可夫决策过程满足贝尔曼最优方程的时候，整个马尔可夫决策过程已经达到最佳的状态。

只有当整个状态已经**收敛后**，我们得到最佳价值函数后，贝尔曼最优方程才会满足。满足贝尔曼最优方程后， 我们可以采用**最大化操作**（max 操作），即

$$
V^*(s) = \max_a Q^*(s,a)
$$

我们取让Q函数最大化的动作对应的值就是当前状态的最佳的价值函数的值。

Q函数贝尔曼方程：

$$
Q^{*}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^{*}\left(s^{\prime}\right)
$$

通过将$$V^*(s) = \max_a Q^*(s,a)$$带入上式，可得Q函数之间的转移，从而看出 **Q-learning 是基于贝尔曼最优方程来进行的。**

$$
Q^{*}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)
$$
**Q-Leaning:**
$$
Q\left(s_t, a_t\right) \leftarrow Q\left(s_t, a_t\right)+\alpha\left[r_{t+1}+\gamma \max _a Q\left(s_{t+1}, a\right)-Q\left(s_t, a_t\right)\right]
$$

同理，基于Q函数的贝尔曼方程推理，我们可以得出状态价值函数的转移：

$$
\begin{aligned}
V^{*}(s) &=\max _{a} Q^{*}(s, a) \\
&=\max _{a} \mathbb{E}\left[G_{t} \mid s_{t}=s, a_{t}=a\right] \\
&=\max _{a} \mathbb{E}\left[r_{t+1}+\gamma G_{t+1} \mid s_{t}=s, a_{t}=a\right] \\
&=\max _{a} \mathbb{E}\left[r_{t+1}+\gamma V^{*}\left(s_{t+1}\right) \mid s_{t}=s, a_{t}=a\right] \\
&=\max _{a} \mathbb{E}\left[r_{t+1}\right]+\max _{a} \mathbb{E}\left[\gamma V^{*}\left(s_{t+1}\right) \mid s_{t}=s, a_{t}=a\right] \\
&=\max _{a} R(s, a)+\max _{a} \gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^{*}\left(s^{\prime}\right) \\
&=\max _{a}\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^{*}\left(s^{\prime}\right)\right)
\end{aligned}
$$


### 2.6 动态规划

动态规划（dynamic programming，DP）适合解决满足**最优子结构**（optimal substructure）和 **重叠子问题**（overlapping subproblem）两个性质的问题。

> 最优子结构：原问题可以分解为一系列的子问题，原问题的最优解可以利用子问题的最优解获得。
>
> 子问题重叠：一些子问题在求解过程中反复出现，可利用表格记录子问题求解结果。

马尔可夫决策过程是满足动态规划要求的，不过动态规划是用于解决马尔可夫决策过程的[规划](常问问题#1 学习（Learning） 规划（Planning）：) 问题而不是学习问题。

> 贝尔曼方程可以分解为递归结构，如果子问题的子状态是可计算的，根据马尔可夫性质，其未来状态因为与子状态是直接相关的，所以也可以将其推理出来。

通过动态规划可以用来计算价值函数的值，通过一直迭代对应的贝尔曼方程，最后使其收敛，即当最后更新的状态与上一个状态差距不大的时候，动态规划算法的更新就可以停止。

我们必须对环境是**完全已知**的，才能做动态规划，也就是要知道状态转移概率和对应的奖励。使用动态规划完成预测问题和控制问题的求解，是解决马尔可夫决策过程**预测问题和控制问题**的非常有效的方式。

### 2.7 策略评估(预测)

策略评估就是给定马尔可夫决策过程和策略$\pi$，计算价值函数$V_\pi(s)$的过程，即对于当前策略，我们可以得到多大的价值。

#### 1. 策略评估计算

我们可以直接把**贝尔曼期望备份**（Bellman expectation backup），变成迭代的过程，反复迭代直到收敛。这个迭代过程可以看作**同步备份**（synchronous backup）的过程。

> 同步备份：每一次的迭代都会完全更新所有的状态。
>
> 异步备份： （asynchronous backup）：每一次迭代不需要更新所有的状态。

策略评估的核心是将贝尔曼期望备份反复迭代，然后得到一个收敛的价值函数的值。因为给定了策略，可以将其简化为马尔可夫奖励过程，即去掉a。

$$
V^{t+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^{t}\left(s^{\prime}\right)\right)
$$

> 注：贝尔曼期望备份可以体现出下一时刻的状态价值与当前时刻价值有关系，而强化思想描述的是下一个状态的价值来更新当前状态的价值。两者针对对象不同，前者建立起前后不同**迭代**间的关联，后者针对于某次迭代中**前后状态**间的关联。

### 2.8 控制

策略评估是指给定马尔可夫决策过程和策略，我们可以估算出价值函数的值。对于只给定马尔可夫决策过程，我们希望寻找最佳策略，从而得到**最佳价值函数**（optimal value function）。最佳价值函数是指，我们搜索一种策略 $\pi$ 让**每个状态**的价值最大。在这种最大化情况中，我们得到的策略就是最佳策略。

$$
V^*(s) = \max_{\pi}V_{\pi}(s)
$$

寻找最佳策略的过程就是马尔可夫决策过程的控制过程，即寻找一个最佳策略使我们得到一个最大的价值函数值。

$$
\pi{(s)}  = \arg\max_{\pi} V_{\pi}(s)
$$

我们可以通过**策略迭代和价值迭代**来解决马尔可夫决策过程的控制问题。

#### 1.  策略迭代

策略迭代由两个步骤组成：**策略评估和策略改进**（policy improvement）。

* 首先进行策略评估，对当前的策略函数估计状态价值函数

$$
V^{t+1}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^{t}\left(s^{\prime}\right)\right)
$$

* 然后进行策略改进（**贪婪策略**），得到状态价值函数后进一步推算出它的Q函数，得到Q函数后直接对Q函数进行最大化，通过在Q函数做一个贪心的搜索 $$\pi = greedy（V）$$来进一步改进策略

$$
\begin{align*}
&Q_{\pi_i}(s,a) = R(s,a) + \gamma\sum_{s' \in S} p(s' \mid s,a)V_{\pi_i}(s) \\ \\
& \pi_{i+1}(s)= \arg\max_a Q_{\pi_i}(s,a)
\end{align*}

$$

* 这两个步骤一直迭代进行，通过不断改进初始状态$V$和策略$\pi$，得到最优价值函数$V^*$和最优策略$\pi^*$

> max 返回值为 y ； argmax 返回值为 x


#### 2. 价值迭代

动态规划将优化问题分为两个部分，第一步执行的是最优的动作，之后后继的状态的每一步都按照最优的策略去做，最后的结果就是最优的。因此一个最优的策略可以分为两个部分：**最优动作和最优后续状态**。

> **最优性原理**：
>
> 一个策略 $\pi(s \mid a)$ 在状态 $s$ 达到了最优价值，也就是 $V_\pi(s) = V^*(s)$ 成立，当且仅当对于所有从 $s$ 到达的 $s'$, 策略 $\pi$ 在状态 $s$ 达到最优值，也就是说对于所有的 $s'$，$V_\pi(s') = V^*(s')$ 恒成立。


**2.1 确定性价值迭代**：

基于最优性原理，如果此时我们知道子问题的最优解$V^*(s')$，就可以通过价值迭代来得到最优的$V^*(s)$的解。

$$
V(s) \leftarrow \max _{a \in A}\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V\left(s^{\prime}\right)\right)
$$

由于只有当马尔可夫决策过程已经达到最佳状态时，上述公式才满足。不过我们可以通过不断迭代贝尔曼最优方程，价值函数就能逐渐趋向于最佳的价值函数。因此，为了得到最佳的$V^*$，对于每个状态的$V$,我们直接通过**贝尔曼最优方程进行迭代**，迭代多次之后，价值函数就会收敛。

**2.2 价值迭代算法**

* 初始化: 令 $k=1$, 对于所有状态 $s,   V_{0}(s)=0$ 
* 对于 $k=1: H$ ($H$ 是迭代次数) ；对于所有状态 $s$

$$
\begin{align*}
&Q_{k+1}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{k}\left(s^{\prime}\right) \\ 
&V_{k+1}(s)=\max _{a} Q_{k+1}(s, a) \\
&k \leftarrow k+1
\end{align*}

$$

* 上一步迭代**收敛后**，就可以得到$V^*$,然后通过 **argmax** 来提取最优策略:

$$
\pi(s)=\underset{a}{\arg \max } R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{k+1}\left(s^{\prime}\right)
$$

### 2.9 总结

#### 计算贝尔曼方程常用的方法

* 蒙特卡洛：从某一状态开始，多次采样，获得多条轨迹，得到对应折扣奖励，将积累的奖励除以轨迹的数量，得到的平均值就是其价值函数的值。
* 动态规划：迭代对应的贝尔曼方程，使其收敛，收敛后的值就是其价值函数的值。
* 时序差分：动态规划方法和蒙特卡洛方法的结合。

#### 折扣因子作用

* 某些任务是一个持续的没有尽头的任务，即**持续任务**（continuing task），此时把未来所有的奖励作为当前的状态价值就很不合理，所以我们需要通过折扣因子避免这样无穷的奖励。
* 同时，如果一个奖励是有实际价值的，我们希望立刻就能获得该奖励，而不是在未来的某个时刻得到奖励。相比于未来奖励我们会更关注当前时刻奖励。

#### 预测 & 控制

* 对于预测问题，即策略评估问题，我们不停执行**贝尔曼期望方程**，这样就可以估算出给定的策略，然后得到价值函数。
* 对于控制问题，如果我们采取的算法是策略迭代，使用的就是**贝尔曼期望方程**，如果我们采取的算法是价值迭代，使用的就是**贝尔曼最优方程**。

#### 策略迭代 & 价值迭代

* 策略迭代分为两步，对当前已有的策略函数进行估值，得到估值后，通过策略改进，计算Q函数，改进策略。
* 价值迭代直接使用贝尔曼最优方程对价值函数进行迭代，从而寻找最优的价值函数。找到最佳函数之后，我们再去提取最佳策略。

> 策略迭代边迭代价值函数边改进策略，价值迭代价值函数收敛后直接提取策略。















## 三、 环境动力学特征

### MC（Markov Chain）

MC又称离散时间马尔可夫链（Discrete-Time Markov chain，DTMC）；在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态，也可以保持当前状态。**状态的改变叫做转移**，与不同的状态改变相关的概率叫做转移概率

> 马尔可夫性：t+1时刻的状态仅取决于t时刻的状态$S_t$与 t 时刻前的历史状态无关

### MP（Markov Process）

原始模型为马尔可夫链（Markov chain），具备了**马尔可夫性的随机过程**称为马尔可夫过程；通常用$$<S,\mathcal{P}>$$表示；

> S是状态集合
> P是状态转移概率矩阵：$$\mathcal{P}_{s s^{\prime}}=\mathbb{P}\left[S_{t+1}=s^{\prime} \mid S_{t}=s\right]$$
### HMM（Hidden Markov Model）

它用来描述一个含有隐含未知参数的马尔可夫过程，其难点是从可观察的参数中确定该过程的隐含参数，然后利用这些参数做进一步的分析。通常由五个参数表示$$<S,O,\pi,A,B>$$，一般可由$<A,B,\pi>$进行表示。

> 在正常的马尔可夫模型（马尔可夫链）中，状态S对于观察者来说是可以直接可见的，因此状态转移概率P是唯一的参数。在隐马尔可夫模型中，状态是不直接可见的，但输出依赖于该状态下，是可见的。**每个状态通过可能的输出记号有了可能的概率分布**。因此，通过一个HMM产生标记序列提供了有关状态的一些序列的信息
>
> $S$；隐含状态S；这些状态通常无法通过直接观测而得到
> $O$：可观测状态；可通过直接观测而得到，与隐含状态相关联，不过其数目不一定和隐含状态相同。
> $\pi$：隐含状态在初始时刻的概率矩阵。
> $A$：隐含状态转移概率矩阵；描述各个状态间转移概率
> $B$：观测状态转移概率矩阵；

### MRP（Markov Reward Process）

通常用$$<S,\mathcal{P},R,\gamma>$$表示；其中在MP基础上增加了一个奖励函数（期望），以及衰减因子。

> $R_s = \mathbb{E}[R_{t+1} \mid S_t =s]$
>$\gamma \in [0,1]$:折扣因子决定了如何在最近的奖励和未来的奖励之间进行折中（考虑未来奖励对自己的影响程度）：$\gamma = 0$ 只考虑眼前利益，$\gamma = 1$未来和当前一样重要

### MDP（Markov Decision Process）

通常用$$<S,A,P,R,\gamma>$$表示；因为MRP不涉及个体行为的选择，因此增加了有限动作集$A$；马尔科夫决策过程是完全可观测（Fully Observable），即环境会把所有信息开放给个体。

> $S$：状态集合
> $A$：动作集合
> $P$：集合中基于行为的状态转移概率矩阵：$$P^a{ss'}=\mathbb{E}\left(S{t+1} \mid S{t}=s, A{t}=a\right)$$$R$：奖励函数；基于状态和行为的奖励函数；$$R^a_s=\mathbb{E}\left(R{t+1} \mid S{t}=s, A_{t}=a\right)$$$\gamma \in [0,1)$：衰减因子
> 马尔可夫决策过程具有马尔科夫性(Markov Property)，下一状态只依赖于前一状态与动作，由状态转移概率函数表示。下一奖励依赖于前一状态与动作，有奖励函数表示。

马尔可夫决策过程中，它的环境是\*\*全部可观测的，\*\*当环境的状态并不是完全观测时，此时个体可以结合自身对环境的历史观测数据构建一个近似的完全可观测环境的描述。因此从这个角度，几乎所有的强化学习问题都可以被认为获可以转化为马尔科夫决策过程。

### POMDP（Partially Observable MDP）

通常用$$<S, A, T, R, \Omega, O, \gamma>$$表示。POMDP是部分可观测的（Partial Observable），POMDP = MDP + sensor model（$O(o|s)、O(o|s,a)$）

> $S$ : 一组状态
> $A$：一组动作
> $T$：状态之间一组条件转移概率；
> $R$：奖励函数
> $\Omega$：一组观测；
> $O$：一组条件观测概率；
> agent 采取动作$a \in A$，会导致转换到状态$s'$的环境概率为$T(s'|s,a)$ ,agent接收观测值 $o \in \Omega$，取决于环境的新状态，概率为：$O(o|s',a)$，最后agent接收奖励$r$等于$R(s,a)$。

### Dec-POMDP（Decentralized- POMDP）

是多智能体之间协调和决策的模型，它可以考虑结果、传感器、通信的不确定性。可以适用于多个去中心化的智能体。共有七个参数进行表达$$<S,{A_i},T,{\Omega},O,\gamma>$$

> $S$：一组状态
> $A_i$：智能体$i$的一组动作，$\{A_i\}$是所有智能体动作集合
> $T$：一组状态之间的条件转移矩阵。$T(s,a,s') = P(s'|s,a)$
> $R$：奖励函数
> $\Omega_i$：智能体$i$的一组观测值，${\Omega_i}$是所有智能体的观测值集合
> $O$：一组条件观测概率$$O(s',a,o) = P(o|s',a)$$

#### 参考

《强化学习入门-从原理到实践》 —— 叶强

《统计学习方法》 —— 李航

{% embed url="https://github.com/datawhalechina/easy-rl" %}
