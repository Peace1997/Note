
## 1. 背景

- 确定性策略 & 随机策略

  - 确定性策略，在相同的状态下，其输出的动作是确定的；
  - 随机策略，对于相同的状态，其输出的状态并不唯一，而是满足一定的概率分布，从而导致即使是处在相同的状态，也可能输出不同的动作。

- 策略梯度 （Policy Gradient）

  - **随机策略**，通过一个概率分布函数$\pi \theta$来表示每一步的最优策略，在每一步根据该概率分布进行action采样，获得当前的最佳action取值。

  - 缺点：通过PG学习到随机策略后，在每一步行为时，我们还需要对得到的最优策略概率分布进行采样，才能获得action的具体值，在高维的action空间的频繁采样，消耗计算能力。

- 确定性策略梯度（DPG ，Deterministic Policy Gradient） + AC

  - **确定性策略**，每一步的行为通过函数u（最优行为函数）直接获得确定的值。
  - **Actor采样随机策略，Critic采用确定性策略**
  - 特点：样本数据小，算法效率高

  > DPG算法融合进actor-critic框架，结合Q-learning或者Gradient Q-learning这些传统的Q函数学习方法，经过训练得到一个确定性的最优行为策略函数

- 深度确定性策略梯度（DDPG，Deep）

  - 深度是指利用深度神经网络逼近行为价值函数和确定性策略
  - 两个技巧：经验回放和独立的目标网络

- 单智能体算法处理多智能体问题时遇到的问题

  - Q-Learning局限
    - 在训练过程中，每个智能体都在变化，而且从每个智能体的角度来看，环境都会变得不稳定。无法直接利用先前的经验重放
  - 策略梯度局限
    - 当多个智能体协作时就会产生较大的方差
  - 状态和动作空间会指数增长

- ==MADDPG算法改进==

  算法是 DDPG 算法在多智能体系统下的自然扩展，属于中心化训练，去中心化执行的算法框架。

  改进之处： 在 Q值函数的建模过程中，通过在**输入端引入从其他智能体当前策略采样出的动作作为额外信息**，来解决多智能体场景下的环境非平稳问题

## 2. MADDPG算法简述

- 基本假设
  - 每个智能体的策略仅取决于自身观察到的状态，与其他智能体观察的状态无关
  - 环境是未知和无模型的，每个智能体的奖励以及采取动作后的下一个状态是不可预料的，奖励来源于环境的反馈，自身动作仅取决于策略
  - 在训练时，各智能体之间的通信不作设定，即相互之间不通信，或通信的内容作为其观察的一个分量
- 算法特点：
  - ==**集中训练，分散执行**==
  - 每个智能体只能使用本地信息（即智能体自身的动作和状态）。通过学习得到的最优策略，在应用时只利用局部信息就能给出最优动作。
  - 算法只不需要知道环境动力学模型，也不需要知道智能体之间的通讯方法
  - 该算法不仅能用于合作环境，也能用于竞争环境。
- 基本思想：**集中训练，分散执行**（extension of actor-critic policy gradient methods ）
  - 每个智能体的critic网络都会收集所有智能体的状态和动作信息
  - 每个智能体的actor网络根据本地信息做出决策

- MADDPG神经网络数量

  假设智能体的数量为N：

  - Actor网络数量为2N。每个智能体都对应一个Actor的训练网络和目标网络
  - Critic网络数量为2N。每个智能体都对应一个Critic的训练网络和目标网络

- 模型训练框架

  训练过程采取集中训练，分散执行的方式，每个智能体根据自身策略得到当前状态执行的动作，并与环境交互得到经验存入自身的经验缓存池，**待所有的智能体与环境交互后，每个智能体从经验池中随机抽取经验训练神经网络**。

  为加速智能体的学习过程，Critic网络需要输入包括其他智能体的观察状态和采取的动作，通过**最小化损失**以更新Critic网络参数，然后通过**梯度下降**的方法更新动作网络参数。

- 

## 3. 集中训练、分散执行

MADDPG算法框架是集中训练，分散执行的

- 集中训练

  - 训练时：
    - 首先，Actor根据当前的state选择一个action。
    - 然后，Critic可以根据state-action计算一个Q值，作为对Actor动作的反馈
    - Critic根据估计的Q值和实际的Q值进行训练，Actor根据Critic的反馈更新策略
  - 测试时：
    - 只需要Actor就可以完成，不需要Critic的反馈。因此在训练时，我们可以在Critic阶段加上一些额外的信息来得到更准确的Q值，比如其他智能体的状态和动作等，这就是集中训练的意思，即每个智能体不仅根据自身的情况，还根据其他智能体的行为来评估当前动作的价值

- 分散执行

  当每个Agent都训练充分后，每个Actor就可以自己根据状态采取合适的动作，此时是不需要其他智能体的状态或动作的。

## 3. 两种其他改进

####  推断其他智能体的策略



#### 策略集成

- 问题提出：针对MARL中由于各个智能体改变策略而导致的环境不平稳的问题，在竞争环境中，智能体的策略可能对他们的竞争者的行为过拟合，从而导致，当竞争者策略变化时，智能体的策略就可能失效了。
- 目的：提升多智能体策略的鲁棒性
- 方法：训练K个不同子策略的集合，在每个回合，随机选择一个策略让每个智能体去执行。由于不同的子策略在不同的博弈回合中执行，我们为每个智能体的子策略维护一个回放缓冲区。



### 4. 伪代码

![img](/Users/mapeixin/Documents/Typora/Algorithm_MDRL/img/MADDPG.png)



## REF

- 论文出处：Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments
- 论文翻译：https://blog.csdn.net/qiusuoxiaozi/article/details/79066612

- 参考笔记：
  - https://zhuanlan.zhihu.com/p/100817603
  - https://blog.csdn.net/qq_41871826/article/details/110678626
  - https://www.geek-share.com/detail/2721122205.html

