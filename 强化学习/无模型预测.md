---
description: 蒙特卡洛策略评估  &  时序差分
---

# 无模型预测

在无法获取马尔可夫决策过程的模型情况下，我们可以通过蒙特卡洛方法和时序差分方法来估计某个给定策略的价值。

## 一、蒙特卡洛策略评估

### 1.1 定义

蒙特卡洛Monte-Carlo(MC) 基于**采样**的方法。个体在不清楚MDP状态转移概率和奖励函数的情况下，对于给定策略$\pi$, 直接从所经历过的**完整状态序列**来估计状态的真实价值，每条轨迹都有对应的回报。

$$
s_{1}, a_{1}, r_{2}, \ldots, s_{k} \sim \pi \\

G_{t}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{T-1} r_{T}
$$

给定策略$\pi$,智能体与环境进行交互，可以得到很多轨迹。并认为某状态的价值等于在多个状态序列（轨迹）中状态所有收获值的平均值。

$$
v_{\pi}(s)=\mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s\right]
$$

#### 特点

* 蒙特卡洛仿真是指我们可以采样大量的轨迹，计算所有轨迹的真实回报，然后计算平均值。蒙特卡洛 方法使用**经验平均回报**(empirical mean return)的方法来估计，它不需要马尔可夫决策过程的状态转移 函数和奖励函数，并且不需要像动态规划那样用自举的方法。
* 起点可以不是某一特定的状态，但是个体最终要进入环境认可的某一个终止状态。因此MC的局限是只能用于有终止的MDP。
* 理论上完整的状态序列越多，结果越准确。

### 1.2 MC评估策略

为了评估$V(s)$,采用如下的步骤：

在每个回合中, 如果在时间步 $t$ 状态 $s$ 被访问了, 那么

* 状态 $s$ 的访问数 $N(s)$ 增加 $1, N(s) \leftarrow N(s)+1$ 。
* 状态 $s$ 的总的回报 $S(s)$增加 $G_{t}, S(s) \leftarrow S(s)+G_{t}$ 。&#x20;

状态 $s$ 的价值可以通过回报的平均来估计, 即 $V(s)=S(s) / N(s)$ 。

要针对多个**包含同一状态**的完整状态求得收获值，继而取收获值的平均值。如果一个完整的状态序列中某一需要计算的状态出现在序列的多个位置，也就是说个体在与环境交互的过程中从某个状态出发后又一次或多次返回到该状态，那么根据收获的定义，在一个状态序列下不同时刻的同一状态计算得到的收获值是不一样的。为了解决此问题，有两种解决方案：

**首次访问(First Visit)**

仅把状态序列中第一次出现该状态时收获值纳入收获平均值的计算中。

**每次访问(Every Visit)**

针对一个状态序列中每次出现该状态时都计算对应的收获值并纳入收获平均值的计算中。

### **1.3 增量均值**

通过增量（累积更新）均值（incremental mean）替代之前的经验均值的方法，不需要存储所有历史收获值；**利用前一次的平均值和当前数据以及数据总个数来计算新的平均值.**

$$
\begin{aligned}
\mu_{k} &=\frac{1}{k} \sum_{j=1}^{k} x_{j} \\
&=\frac{1}{k}\left(x_{k}+\sum_{j=1}^{k-1} x_{j}\right) \\
&=\frac{1}{k}\left(x_{k}+(k-1) \mu_{k-1}\right) \\
&=\mu_{k-1}+\frac{1}{k}\left(x_{k}-\mu_{k-1}\right)
\end{aligned}
$$

其中$x_k - \mu_{k-1}$为残差，$\frac{1}{k}$类似于学习率，当我们得到$x_k$时，就可以用上一时刻的值来更新现在的值。

### 1.4 增量式蒙特卡洛

采集数据，获得一个新的轨迹：$S{1}, A{1}, R{2}, \ldots, S{T}$,对于这个轨迹，采用增量的方法评估$V(S)$

$$
\begin{array}{l}
N\left(s_{t}\right) \leftarrow N\left(s_{t}\right)+1 \\
V\left(s_{t}\right) \leftarrow V\left(s_{t}\right)+\frac{1}{N\left(s_{t}\right)}\left(G_{t}-V\left(s_{t}\right)\right)
\end{array}
$$

可以将$\frac{1}{N(s_t)}$变成学习率 $\alpha$，即：

$$
V\left(s_{t}\right) \leftarrow V\left(s_{t}\right)+\alpha\left(G_{t}-V\left(s_{t}\right)\right)
$$

### 1.5 MC 和 动态规划

MC 可以在不知道环境的情况下工作（model-free），而 DP 是 model-based。

MC 只需要更新一条轨迹的状态，而 DP 则是需要更新所有的状态。状态数量很多的时候（比如一百万个，两百万个），DP 这样去迭代的话，速度是非常慢的。这也是 sample-based 的方法 MC 相对于 DP 的优势。

在动态规划方法里面，使用了自举(bootstrapping)的思想。自举就是我们基于之前估计的量来估计一个量。此外，动态规划方法使用贝尔曼期望备份(Bellman expectation backup)，通过上一时刻的值$V_{i-1}(s')$来更新当前时刻的值$V_i(s)$，即

$$
V_{i}(s) \leftarrow \sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) V_{i-1}\left(s^{\prime}\right)\right)
$$

蒙特卡洛方法通过一个回合的经验平均回报(实际得到的奖励)来进行更新，即

$$
V\left(s_{t}\right) \leftarrow V\left(s_{t}\right)+\alpha\left(G_{i, t}-V\left(s_{t}\right)\right)
$$

## 二、时序差分

### 2.1 定义

时序差分法（Temporal-Difference）是介于MC和DP之间的方法，它是**无模型**的，不需要马尔可夫决策过程的状态转移矩阵和奖励函数。此外，时序差分方法可以从**不完整**的状态序列中学习，并结合了**自举**（Bootstrapping，引导法）的思想。

> 先**估计**某状态在该状态序列完成后可能得到的**收获**，并在此基础上利用前文所述的**增量均值**的方法得到该状态的价值，再通过不断的采样来持续更新这个价值。
>
> **Bootstrapping：用一个估算去更新同类的估算**；在这里是指 用TD目标值代替收获$G_t$的过程。
>
> **强化**：自举的概念之一，用下一个状态的价值来更新当前状态的价值。
>
>

对于某个给定的策略$\pi$, 在线（online）地算出它的价值函数$V_\pi$，即一步步地（step-by-step）算，最简单的方法就是**一步时序差分**（one-step TD，TD(0) ）。每走一步就做一步自举，用得到的估计回报$r_{t+1}+\gamma V(s_{t+1})$ 来更新上一刻的值$V(s_t)$：

$$
V\left(s_{t}\right) \leftarrow V\left(s_{t}\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)\right)
$$

$r_{t+1}+\gamma V(s_{t+1})$被称为**时序差分目标（TD target）**,该公式体现了强化的特点，由两部分组成：

* 走了某一步之后得到的实际奖励
* 利用自举法，通过之前的估计（上次迭代）来估计$V(s_{t+1})$,然后增加一个折扣系数$\gamma$：

$$
\begin{aligned}
\mathrm{v}(\mathrm{s}) &=\mathbb{E}\left[\mathrm{G}_{\mathrm{t}} \mid \mathrm{s}_{\mathrm{t}}=\mathrm{s}\right] \\
&=\mathbb{E}\left[\mathrm{r}_{\mathrm{t}+1}+\gamma \mathrm{r}_{\mathrm{t}+2}+\gamma^{2} \mathrm{r}_{\mathrm{t}+3}+\ldots \mid \mathrm{s}_{\mathrm{t}}=\mathrm{s}\right] \\
&=\mathbb{E}\left[\mathrm{r}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}=\mathrm{s}\right]+\gamma \mathbb{E}\left[\mathrm{r}_{\mathrm{t}+2}+\gamma \mathrm{r}_{\mathrm{t}+3}+\gamma^{2} \mathrm{r}_{\mathrm{t}+4}+\ldots \mid \mathrm{s}_{\mathrm{t}}=\mathrm{s}\right] \\
&=\mathrm{r}(\mathrm{s})+\gamma \mathbb{E}\left[\mathrm{G}_{\mathrm{t}+1} \mid \mathrm{s}_{\mathrm{t}}=\mathrm{s}\right] \\
&=\mathrm{r}(\mathrm{s})+\gamma \mathbb{E}\left[\mathrm{v}\left(\mathrm{s}_{\mathrm{t}+1}\right) \mid \mathrm{s}_{\mathrm{t}}=\mathrm{s}\right]
\end{aligned}
$$

$r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)$被称为**时序差分误差（TD error），**类比于增量式蒙特卡洛方法，给定一个回合 $i$ ,我们可以更新$V(s_t)$ 来逼近真实的回报$G_t$ , 具体更新公式为：

### 2.2 TD(0) 和 MC

时序差分方法可以**在线学习**(online learning)，每走一步就可以更新，效率高，因此TD可以从不完整的序列上进行学习。蒙特卡洛方法仅在到达终止状态后，才更新各个状态的价值，因此MC只能从完整的序列上进行学习。

时序差分方法可以在连续的环境下(**没有终止**)进行学习。蒙特卡洛方法只能在有终止的情况下 学习。

时序差分方法利用了马尔可夫性质，在马尔可夫环境下有更高的学习效率。蒙特卡洛方法没有假 设环境具有马尔可夫性质，利用采样的价值来估计某个状态的价值，在不是马尔可夫的环境下更加有效。

TD 学习在更新状态价值时使用的是 TD 目标值，即基于即时奖励和**下一状态的预估价值**来替代当前状态在状态序列结束时可能得到的收获，它是当前状态价值的**有偏估计**，而 MC 学习则 使用实际的收获来更新状态价值，是某一策略下状态价值的**无偏估计**。

TD 学习得到的价值是有偏估计的，但是其**方差** (Variance) 却较 MC 学习得到的方差要低，且对初始值敏感，通常比 MC 学习更加高效，这 也主要得益于 TD 学习价值更新灵活，对初始状态价值的依赖较大。

### 2.3 n步时序差分

相较于TD(0)只往前走一步，我们可以调整向前走的步数(step) n ，变成 **n 步时序差分**(n-step TD)，通过步数来调整算法需要的实际奖励和自举。

$$
G_{t}^{n}=r_{t+1}+\gamma r_{t+2}+\ldots+\gamma^{n-1} r_{t+n}+\gamma^{n} V\left(s_{t+n}\right)
$$

得到时序差分目标之后，我们用增量式学习(incremental learning)的方法来更新状态的价值：

$$
V\left(s_{t}\right) \leftarrow V\left(s_{t}\right)+\alpha\left(G_{t}^{n}-V\left(s_{t}\right)\right)
$$

如果n为无穷大，即整个游戏结束后再更新，时序差分法就变成了蒙特卡洛方法。

### 2.3 DP & MC & TD 

自举是指更新时使用了估计。蒙特卡洛方法没有使用自举，因为它根据实际的回报进行更新。动态规划方法和时序差分方法使用了自举。

采样是指更新时通过采样得到一个期望。蒙特卡洛方法是纯采样的方法。动态规划方法没有使用采样， 它是直接用贝尔曼期望方程来更新状态价值的。时序差分方法使用了采样。时序差分目标由两部分组成， 一部分是采样（采取一条支路），一部分是自举（使用TD目标值估计回报值）。

动态规划方法直接计算期望，没有进行采样，而是对某个状态下所有下一状态的价值进行自举后求期望来预估当前状态的价值。如果展开来看的话，其贝尔曼期望备份包含两层加和，即计算两次期望，得到一个更新。

$$
V\left(s_{t}\right) \leftarrow \mathbb{E}_{\pi}\left[r_{t+1}+\gamma V\left(s_{t+1}\right)\right]\\V_{i}(s) \leftarrow \sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) V_{i-1}\left(s^{\prime}\right)\right)
$$

![动态规划备份](DP.png)

蒙特卡洛方法在当前状态下，采取一条支路，在这条路径上进行更新，更新这条路径上的所有状态，即:

$$
V\left(s_{t}\right) \leftarrow V\left(s_{t}\right)+\alpha\left(G_{t}-V\left(s_{t}\right)\right)
$$

![蒙特卡洛备份](MC.png)

时序差分从当前状态开始，往前走了一步，关注的是非常局部的步骤，即：

$$
\mathrm{TD}(0): V\left(s_{t}\right) \leftarrow V\left(s_{t}\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_{t}\right)\right)
$$

![TD(0) 备份](TD_0.png)

如果时序差分方法需要**更广度**的更新，就变成了动态规划方法(因为动态规划方法是把所有状态都考虑进去来进行更新)。如果时序差分方法需要**更深度**的更新，就变成了蒙特卡洛方法。例如：穷举搜索的方法不仅需要很深度的信息，还需要很广度的信息。
