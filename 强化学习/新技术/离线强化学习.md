
# 简述

***什么是离线强化学习***

离线强化学习（Offline Reinforcement Learning, Offline RL），又称作批量强化学习（Batch Reinforcement Learning, BRL），是强化学习的一种变体，主要研究的是如何利用预先收集的大规模**静态数据集**来训练强化学习智能体。与常见的强化学习相比最显著的区别就是不进行任何形式的交互和探索。

***Offline RL vs 模仿学习***
- 模仿学习问题通常由一个最优的或一个高性能的专家来提供数据，而且一些问题要求数据被标记为专家经验和非专家经验。而 Offline RL 通常需要从大量的次优数据中进行学习
- 模仿学习更符合普通监督学习的范式，专家数据集就是评价标准，所以通常没有奖励的概念，而Offline RL是需要显示考虑奖励项（在收集）。

***Offline RL vs Off-policy RL***
Off-policy可以使用历史策略收集到数据来优化当前策略，而且在学习过程中仍然通过在线的交互来收集数据，Offline 不进行任何形式的交互探索。

***为什么需要离线强化学习***

- 若没有仿真器，直接与实体机器人/汽车进行交互来收集数据，样本收集困难或危险，如果从固定数据集学习，不需要额外的交互和探索，可以减少潜在风险。
- 传统在线强化学习需要反复探索环境和收集大量数据来训练智能体，代价比较高
- 在线强化学习在实际部署（离线场景）过程中效果不佳。
	- 在线学习通常是在仿真环境下训练的是仿真环境与真实环境存在一定的偏差。

***