基于模型的强化学习(Model-Based Reinforcement Learning, MBRL)是指利用学习到的环境模型来加速或改进强化学习的范式。这种方法的主要思路是先从与环境的交互中学习一个模拟环境的模型,然后基于该模型进行策略搜索或者规划,以减少昂贵的环境交互。


## MPC
模型预测控制


## MB-MF
（hybrid model-based and model-free, MB-MF，混合有模型和无模型强化学习方法 ）
该算法包括一个参数化的环境模型和无模型强化学习算法。首先，利用随机打靶法与环境交互产生的真实数据，利用真实交互数据训练环境模型，经过反复迭代优化, 可以得到预测较为准确的环境模型。然后使用无模型强化学习算法（SAC）替换随机打靶法与环境模型交互获取动作序列，每次选择最优序列的第一个动作与环境交互，通过不断学习，对无模型强化学习算法进行更新。


对于环境模型, 经过反复迭代优化, 可以得到预测较为准确的环境模型。
该算法首先利用环境模型训练出初始策略，然后利用无模型强化学习算法对该策略训练优化，在得到更好的效果的同时提升了样本利用率。

在 MB-MF 算法中, 使用 $\hat{f}_\theta\left(\hat{s}_{t^{\prime}}, a_{t^{\prime}}\right)$ 表示参数化的环境模型, 该模型使用与环境交互产生的真实数据 $\left(s_t, a_t, r_t, s_{t+1}\right)$ 进行训练, 并将这些真实交互数据保存至状态转移缓存区 $D$ 中。环境模型包括状态转移概率模型和奖励预测模型, 环境模型的训练损失如下所示:
$$
L_f=\frac{1}{|D|} \sum_{\left(s, a, s_{t+1}\right)=D} \frac{1}{2}\left\|\left(s_{t+1}-s_t\right)-\hat{f}_\theta\left(s_t, a_t\right)\right\|^*
$$

在训练初期, 使用随机打靶法与虚拟环境模型交互获取动作序列。接下来, 采用模型预测控制的规划方法, 利用环境模型找到最优动作序列 $A_t^H=\left(\mathrm{a}_t, \ldots, a_{t+H-1}\right)$,
$$
\left\{\begin{array}{l}
A_t^H=\arg \max \sum_{t^{\prime}=t}^{t+H-1} \gamma^{t^{\prime}-t} r\left(\hat{s}_{t^{\prime}}, a_{t^{\prime}}\right) \\
\hat{s}_t=s_t, \hat{s}_{t+1}=\hat{s}_{t^{\prime}}+\hat{f}_\theta\left(\hat{s}_{t^{\prime}}, a_{t^{\prime}}\right)
\end{array}\right.
$$

获得最优动作序列后, 每次选择最优序列的第一个动作与环境交互, 执行动作 $a_t$转移到新的状态 $s_{t+1}$, 然后重新评估找到下一个最优动作。

对于环境模型, 经过反复迭代优化, 可以得到预测较为准确的环境模型。随后使用无模型强化学习算法替换随机打靶法与环境模型交互获取动作序列。无模型强化学习采用 SAC 算法, 该算法基于 Actor-Critic 网络结构, 内部的深度神经网络结构主要由双 $\mathrm{Q}$ 网络 $Q_{\omega_j}$ 和策略网络 $\pi_\theta$ 构成。双 $\mathrm{Q}$ 网络的输入为状态和动作信息, 输出为状态动作对的 $Q$ 值, 使网络参数的更新更稳定, 双 $Q$ 网络的训练损失函数如下所示:
$$
\begin{gathered}
L=\frac{1}{N} \sum_{i=1}^N\left(y_i-Q_{\omega_j}\left(s_i, a_i\right)\right)^2 . \\
y_i=r_i+\gamma \min _{j=1,2} Q_{\omega_j^{-}}\left(s_{i+1}, a_{i+1}\right)-\alpha \log \pi_\theta\left(a_{i+1} \mid s_{i+1}\right), a_{i+1} \sim \pi_\theta\left(\mid s_{i+1}\right) .
\end{gathered}
$$

其中, **$\alpha$ 为最大化奖励和最大化熵之间的权重**, 用于权衡策略的探索和利用; $y_i$为目标值, 双 $Q$ 网络每次选择输出中较小的状态动作值作为目标 $Q$ 值用于更新 $Q$ 网络。策略网络的输入为状态信息, 输出为动作信息, 策略网络的训练损失如下所示:
$$
L_\pi(\theta)=\frac{1}{N} \sum_{i=1}^N\left(\alpha \log \pi_\theta\left(\tilde{a}_i \mid s_i\right)-\min _{j=1,2} Q_{\omega_j}\left(s_i, \tilde{a}_i\right)\right) 。
$$

由于策略是一个分布, 动作采样后无法求导, 因此采用重参数化的方式采样获取动作 $\tilde{a}$ 。


