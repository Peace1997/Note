# 常问问题

## 一、概念区分

### 奖励 & 回报 & 价值

* 奖励（reward）：智能体交互过程中“即时”获取的
* 回报（return）：对未来奖励的累加，奖励按照 $\gamma$ 进行折扣：

$$
G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
$$

* 价值（value）：状态收获的期望，衡量某一状态或动作-状态的优劣:

$$
V(s) = \mathbb{E}[G_t \mid S_t =s] \\ Q(s,a) = \mathbb{E}[G_t \mid S_t =s,a_t=a]
$$

> 对 $G_t$ 取了一个期望，期望就是从这个状态开始，我们可能获得多大的价值。**期望也可以看成未来可能获得奖励的当前价值的表现**。
>
> 一个状态价值是该状态的收获的期望值，也就是说从该状态开始依据状态转移概率矩阵采样生成一系列的状态序列， 对每一个状态序列计算该状态的收获，然后对该状态的所有收获计算平均值得到一个平均收获。采样的序列越多，就越接近该状态的价值，因为价值可以更准确的反映某一状态的重要程度。

### 贝尔曼方程 & 贝尔曼期望方程 （基于MDP）

#### 1. 贝尔曼方程

贝尔曼方程就是**当前状态与未来状态的迭代关系**，表示当前状态的价值函数可以通过下个状态的价值函数来计算。

> **另一种计算计算某状态价值的方法就是 MC**, 在某一状态下，多次进行采样，获得每条轨迹对应的回报值，然后求平均值来计算该状态下的价值。

从状态价值函数里，我们可以推导出状态价值函数的贝尔曼方程：

$$
V_\pi(s) =R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V_\pi \left(s^{\prime}\right)
$$

主要包含两部分：**即时奖励 和 未来奖励折扣的总和**。

动作价值函数（Q函数）贝尔曼方程：(其推导过程与贝尔曼方程近似)

$$
Q_\pi(s,a) = R(s,a) + \gamma \sum_{s' \in S}p(s' \mid s,a)V_\pi (s')
$$

利用公式 $V_\pi(s) = \sum_{a \in A} \pi(a \mid s)Q_\pi(s,a)$ 我们可以推导出状态价值函数与动作价值函数的关联：

贝尔曼方程推导：(从价值函数中进行推导)
$$
 \begin{aligned} V(s) &=\mathbb{E}\left[G_{t} \mid s_{t}=s\right] \\ &=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots \mid s_{t}=s\right] \\ &=\mathbb{E}\left[r_{t+1} \mid s_{t}=s\right]+\gamma \mathbb{E}\left[r_{t+2}+\gamma r_{t+3}+\gamma^{2} r_{t+4}+\ldots \mid s_{t}=s\right] \\ &=R(s)+\gamma \mathbb{E}\left[G_{t+1} \mid s_{t}=s\right] \\ &=R(s)+\gamma \mathbb{E}\left[V\left(s_{t+1}\right) \mid s_{t}=s\right] \\ &=R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right) \end{aligned}
 $$
> 倒数第二步由**全期望公式**推导而来。注意区分 $r 、R$



#### 2. 贝尔曼期望方程

定义了基于策略$\pi$的状态价值函数和动作价值函数后，根据贝尔曼方程，我们可以得到相应的贝尔曼期望方程。

$$
 \begin{aligned} 
& V_\pi (s) = \mathbb{E}_\pi[r_{t+1} +\gamma V_\pi (s_{t+1}) \mid s_t =s ] \\
&   Q_\pi (s, a) = \mathbb{E}_\pi[r_{t+1} +\gamma Q_\pi (s_{t+1}, a_{t+1}) \mid s_t =s , a_t =a]
\end{aligned}
$$

将Q函数贝尔曼方程 $Q_\pi(s) = R(s) + \gamma \sum_{s' \in S}p(s' \mid s,a)V_\pi (s')$ ，带入价值函数与Q函数关联公式$V_\pi(s) = \sum_{a \in A} \pi(a \mid s)Q_\pi(s,a)$ ，我们可以得到**当前状态的价值与未来状态价值的关联**（贝尔曼期望方程的另一种形式）

$$
V_{\pi}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{\pi}\left(s^{\prime}\right)\right)
$$

同理，将价值函数与Q函数关联公式带入Q函数贝尔曼方程，我们可以得到**当前时刻的Q函数与未来时刻的Q函数的关联**。

$$
Q_{\pi}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) Q_{\pi}\left(s^{\prime}, a^{\prime}\right)
$$

***

### 值函数 & 动作值函数 & 优势函数

#### 1. 值函数（状态-价值函数 State Value Function）$V ( s )$：

是从状态到价值的映射。可以理解为在该状态下所有可能动作所对应的动作值函数乘以采取该动作的概率的和。更通俗的讲，值函数$V ( s )$是**该状态下所有动作值函数关于动作概率的平均值**。

#### 2. 动作值函数（行为价值函数 State-Action Value Function）$Q ( s,a )$

针对某一状态下某一行为的价值评估，是**单个动作所对应的值函数**。

#### 3. 优势函数（Advantage Function）：$Q π ( s , a ) − V π ( s )$

能**评价当前动作值函数相对于平均值的大小**，因此优势指的是动作值函数相比于当前状态的值函数的优势。如果优势函数大于零，则说明该动作比平均动作好，如果优势函数小于零，则说明当前动作还不如平均动作好。

### 基于值 & 基于策略 & 演员-评论家

#### 1. 基于值（value-based）：

显式的学习价值函数，隐式的学习它的策略，策略其从学到的价值函数里面推算出来的。给定一个状态就能计算出每种可能动作的奖励，即**根据值的大小选择相应的动作**。例如：Q-Learning、 Sarsa

不过基于值的方法存在一定的问题：

* 不适合解决**连续性**空间问题，因为基于值的方法需要保存状态-动作的对应关系，因此很多连续动作空间，都因为大量的状态而无法计算。
* **随机策略**学习较难，采用基于值的方法在确定的状态下将得到确定的反馈，因此在使用这种方法的下一步动作是确定的， 可能会落入错误的循环中，陷入局部最优。

#### 2. 基于策略（policy-based）：

直接学习策略，并没有学习价值函数，给定一个状态，它就会输出对应动作的概率，优化策略的过程也就是优化**相应状态-动作对被选择的概率的过程**。例如：策略梯度算法

#### 3. 演员-评论家（actor-critic）：

智能体把策略和价值函数都进行学习，智能体会根据策略策略做出动作，而价值函数会对做出的动作给出价值。在原有策略梯度的基础上加速学习过程，取得更好的效果。

### 确定性策略 & 随机性策略

#### 1. 确定性策略（Deterministic Policy）：

策略在一个确定的状态下能够产生一个**确定的行为**。智能体直接采取最有可能的动作：

$$
a^{*}=\underset{a}{\arg \max \pi}(a \mid s)
$$

#### 2. 随机性策略（Stochastic Policy）：

在确定的状态下不能够产生一个确定的行为，而是提供各种可能**行为的概率**，然后对这个概率分布进行采样，得到智能体将要采取的动作。

$$
\pi(a \mid s)=p\left(a_{t}=a \mid s_{t}=s\right)
$$

强化学习一般使用随机性策略，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境，随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要。 采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。

### 行为策略 & 目标策略

#### 1. 行为策略（behavior policy）

行为策略是探索环境的策略，一般用$\mu$来表示，通过行为策略尽可能多样化的收集经验信息，然后把这些经验交给目标策略去学习。

#### 2. 目标策略（target policy）

目标策略是我们需要学习的策略，一般用$\pi$ 来表示，它可以根据自己的经验来学习最优的策略,不需要和环境进行交互。

比如Q-Learning 中的目标策略优化，它不会管下一步去哪里探索，它只会选取奖励最大的策略，因此行为策略收集的信息越多，目标策略学习的数据就越多，那么所做的决策相对也越好。

### 同策略 & 异策略

#### 1. 同策略（on-policy）

同策略行为策略和目标策略是**同一个策略**。它只使用一个策略$\pi$，不仅使用策略$\pi$与环境交互产生经验，也使用策略$\pi$学习。目标策略的优化需要**兼顾探索**，为了平衡探索和利用，训练时就会显得“胆小”一点，在悬崖行走问题中，会尽可能远离悬崖边，哪怕不小心探索了一点，也还是在安全区域内。

> 在SARSA算法中，行为策略是$\varepsilon-greedy$策略，目标策略也是$\varepsilon-greedy$策略；

#### 2. 异策略（off-policy）

行为策略和目标策略**不是同一个策略**。因为分离了目标策略和行为策略，目标策略优化的过程不需要兼顾探索，可以大胆地用行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。因此在悬崖行走问题中，会更加激进一些， 希望每一步都能获得最大利益。

> 在Q-Learning算法中，行为策略是$\varepsilon-greedy$策略，目标策略是贪婪策略。


上面同策略和异策略是针对同一智能体而言的，如果进行扩展，不局限于同一智能体的话，与环境交互的智能体和要学习的智能体可以是不同时刻的
- 要学习的智能体和与环境交互的智能体是相同的，我们称之为同策略。
- 如果要学习的智能体和与环境交互的智能体不是相同的，我们称之为异策略。



### 有模型 & 无模型

#### 1. 有模型（model-based）

智能体需要**建立一个描述环境运作过程的模型**，以此来指导价值、策略函数的更新。具体来说，智能体知道状态转移函数和奖励函数后，他就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，智能体可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境中的状态和交互反应，就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略就可以。

#### 2.无模型（model-free）

个体并不需要去了解这个环境是如何运作的，它通过学习价值函数和策略函数进行决策。无模型强化学习**没有对真实环境进行建模**，通常情况下， 状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新动作策略，这样反复迭代直到学习到最优策略。

#### 3. 总结

总之，有模型强化学习相比免模型强化学习仅仅多出一个步骤，即对真实环境进行建模。在实际应用中，如果不清楚该用有模型 强化学习还是免模型强化学习，可以先思考在智能体执行动作前，是否能对下一步的状态和奖励进行预测， 如果能，就能够对环境进行建模，从而采用有模型学习。

*   无模型强化学习通常属**于数据驱动型**方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略，正因如此，无模型学习的**泛化性**要优于有模型强化学习

    > 有模型强化学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型强 学习算法的泛化性。
*   有模型的深度强化学习可以在一定程度上缓解训练数据匮乏的问题，因为智能体可以在虚拟世界中进行训练。

    > 在无模型强化学习中，智能体只能一步一步地采取策略，等待真实环境的反馈;有模型强化学习可 以在虚拟世界中预测出将要发生的事，并采取对自己最有利的策略。例如 雅达利游戏平台

### 学习&规划 、 探索&利用、 预测&控制

#### 1. 学习（Learning）& 规划（Planning）：

* 学习：在强化学习中，环境初始是未知的，智能体不知道环境是如何工作的，个体与环境进行交互，逐渐改进策略。
* 规划：环境是已知的，智能体被告知了整个环境的运作规则的详细信息。智能体 能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。智能体不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。

#### 2. 探索（Exploration） & 利用（Exploitation）

* 探索：是个体初期不知道哪种动作是正确的，只能通过不断**试错**去探索，通过不断尝试不同的行为去判断这个行为能不能得到一个好的奖励，从而帮助个体利用这些探索信息去得到一个最佳策略。
*   利用：当智能体收集一定的环境信息后，根据目前的信息来选择行为进行决策，这时的决策是**局部最优解**，短期内能获得的最大收益，不去尝试新的行为，根据当前已经知道的信息去获取一个自认为最优的解。

> 为此需要协调好探索和利用，若只进行探索则无法固定到最优的一个结果。而只利用的话，个体没有机会体验可能发生的变化，就很容易陷入局部最优解，甚至找不到可靠的解。

#### 3. 预测（Prediction）& 控制（Control）

* 预测（Prediction）：是给定一个马尔可夫决策过程和一个**策略**$\pi$​, 计算它的价值函数，也就是计算每个状态的价值。根据现有的策略，个体对未来进行评价，即**策略评估**。它间接**评价**在一个给定策略下个体表现的优劣程度。
* 控制（Control）：给定一个马尔可夫决策过程，去寻找一个最佳的策略，然后同时输出它的最佳价值函数 $V^{*}$ 以及最佳策略 $\pi^{*}$。对所有的策略进行评估，发现最优的那个策略，即求解**最优策略**。试图寻找一个最优策略来最大化奖励，即最优个体的表现。

> 这两者的区别就是，预测问题是**给定一个策略**，我们要确定它的价值函数是多少。而控制问题是在**没有策略的前提下**，我们要确定最佳的价值函数以及对应的决策方案。
>
> 在**马尔可夫决策过程**里面，预测和控制都可以通过**动态规划**解决
>
> 两者为递进关系，在强化学习中，我们通过预测问题，进而解决控制问题。





## 二、问题提问



### 多智能体强化学习中的非平稳性问题

#### 产生原因：

在马尔科夫博弈中，环境的状态转移函数 $T$ 和单个智能体的奖励函数 $r_i$ 受到 所有智能体动作的影响. 在训练多个智能体的过程中, 每个智能体的策略随着时间在不断变化，因此每个智能体所感知到的转移概率分布和奖励函数也会发生变化. 通常的单智能体强化学习算法假定这些函数具有平稳性, 因而这些算法不能很好的用在多智能体的场景下。
尤其对于采用经验回放池的算法，影响更为严重，从过去收集的轨迹中采样时，在相同状态和动作下过去可能会取得较好的奖励，由于状态转移概率的改变，在当前不一定会获得较好的奖励，策略分布可能差距较大。

> **马尔科夫博弈**：是由马尔可夫决策过程和博弈论相结合的产物，首先马尔可夫决策过程表示了多智能体系统的状态符合马尔可夫性，即下一刻状态只与当前状态有关，与前面的时刻没有直接关系；其次博弈代表了多智能体之间的关系，或是合作，或是竞争，亦或两者都有。
>
> 由于多个智能体同时根据自己的利益改进他们的政策，从每个智能体的角度来看，环境动态变得是不平稳的，智能体本身无法判断状态过渡（奖励的变化），是由于其自己的行动，还是由于其对手的探索。

一个智能体的值函数也取决于其他智能体的行为，因此我们也需要对其他智能体的行为进行采样。

#### 解决方法：

- 将其他智能体视为环境一部分？（Ind-DQN and Ind-DDPG.）
- **中心化的评论家（CTDE）**：评论家的训练是集中的，这使得其可以接触到所有智能体的观察和行动，而演员的训练是分散的。常见方法COMA和MADDPG
- **去中心化的学习方法**：在多智能体系统中处理非平稳性并不一定需要进行集中训练.在
  - **自我博弈**：神经网络使用每个智能体自身的观察作为输入，通过将其与当前或以前的版本进行比较，来学习可以推广到任何对手的策略，该方法可能会导致训练中缺乏探索，因此可以将神经网络参数存储在学习过程中的不同位置，然后，在自我博弈过程中，在当前和以前版本的神经网络参数中随机选择对手.
  - **经验回放池**：直接使用经验回放池可能会存储大量与去中心化学习无关的信息，从而导致性能变差，因此可以采用**重要性采样**动态调整以往经验的权重。
- **对手建模(Opponent Modeling)**：通过模拟其他智能体的意图和政策，可以稳定智能体的训练过程。当前智能体使用他们的策略来预测其他智能体的行为，另一种方式是利用一个单独的网络为其他智能体建模.这两个网络的组合可以通过连接它们的隐藏状态，也可以通过混合使用专家来实现。
- **元学习(Meta-Learning)：**
  - 使用学习方法，一旦环境动态发生变化，就尝试快速更改策略或环境模型，不过使用少量的学习更新可能不能很好的适应动态环境。
  - 另一种方法是预测环境的变化，而不是制定一种训练深度神经网络对环境变化作出反应的学习算法，当给定环境的动态变化时可以制定一个优化过程来寻找初始神经网络参数，这样便可以使用少量的更新过程来更好的学习。
- **通信**：通过通信机制，不同的智能体可以交换关于他们的观察、行动和意图的信息，以稳定其训练过程。每个智能体可以通过显示或隐式方法共享自己观测的特征信息，来进行通信交流。





REF：

https://zhuanlan.zhihu.com/p/72553328
