# 常问问题

# 一、概念区分

## 奖励 & 回报 & 价值

* 奖励（reward）：智能体交互过程中“即时”获取的
* 回报（return）：对未来奖励的累加，奖励按照 $\gamma$ 进行折扣：

$$
G_{t}=R_{t+1}+\gamma R_{t+2}+\ldots=\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
$$

* 价值（value）：状态收获的期望，衡量某一状态或动作-状态的优劣:

$$
\begin{align*}
V(s) = \mathbb{E}[G_t \mid S_t =s] \\ Q(s,a) = \mathbb{E}[G_t \mid S_t =s,a_t=a]
\end{align*}

$$

> 对 $G_t$ 取了一个期望，期望就是从这个状态开始，我们可能获得多大的价值。**期望也可以看成未来可能获得奖励的当前价值的表现**。
>
> 一个状态价值是该状态的收获的期望值，也就是说从该状态开始依据状态转移概率矩阵采样生成一系列的状态序列， 对每一个状态序列计算该状态的收获，然后对该状态的所有收获计算平均值得到一个平均收获。采样的序列越多，就越接近该状态的价值，因为价值可以更准确的反映某一状态的重要程度。



## 状态 & 观测

**状态（state）**：状态指的是智能体当前的环境或情况。在强化学习中，状态通常是由一个或多个变量描述的。例如，在走迷宫的例子中，状态可以包括机器人的位置和迷宫的地图。状态可以是连续的（例如机器人的位置），也可以是离散的（例如迷宫的地图）。

**观测（Observation）**：是智能体对其周围环境的感知或测量。在强化学习中，观测可以是完全观测，即智能体可以获得环境的所有信息，也可以是部分观测，即智能体只能获得一部分信息。例如，在走迷宫的例子中，如果机器人能够感知到周围所有的墙壁和障碍物，那么这是完全观测。如果机器人只能感知到自己的位置和相邻的四个方格的情况，那么这是部分观测。

简而言之，状态描述了智能体所处的环境，而观测是智能体对环境的感知。


## 贝尔曼方程 & 贝尔曼期望方程 
（基于MDP）
***1. 贝尔曼方程***

贝尔曼方程就是**当前状态与未来状态的迭代关系**，表示当前状态的价值函数可以通过下个状态的价值函数来计算。

> **另一种计算某状态价值的方法就是 MC**, 在某一状态下，多次进行采样，获得每条轨迹对应的回报值，然后求平均值来计算该状态下的价值。

从状态价值函数里，我们可以推导出状态价值函数的贝尔曼方程：

$$
V_\pi(s) =R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V_\pi \left(s^{\prime}\right)
$$

主要包含两部分：**即时奖励 和 未来奖励折扣的总和**。

动作价值函数（Q函数）贝尔曼方程：(其推导过程与贝尔曼方程近似)

$$
Q_\pi(s,a) = R(s,a) + \gamma \sum_{s' \in S}p(s' \mid s,a)V_\pi (s')
$$

利用公式 $V_\pi(s) = \sum_{a \in A} \pi(a \mid s)Q_\pi(s,a)$ 我们可以推导出状态价值函数与动作价值函数的关联：

贝尔曼方程推导：(从价值函数中进行推导)
$$
 \begin{aligned} V(s) &=\mathbb{E}\left[G_{t} \mid s_{t}=s\right] \\ &=\mathbb{E}\left[r_{t+1}+\gamma r_{t+2}+\gamma^{2} r_{t+3}+\ldots \mid s_{t}=s\right] \\ &=\mathbb{E}\left[r_{t+1} \mid s_{t}=s\right]+\gamma \mathbb{E}\left[r_{t+2}+\gamma r_{t+3}+\gamma^{2} r_{t+4}+\ldots \mid s_{t}=s\right] \\ &=R(s)+\gamma \mathbb{E}\left[G_{t+1} \mid s_{t}=s\right] \\ &=R(s)+\gamma \mathbb{E}\left[V\left(s_{t+1}\right) \mid s_{t}=s\right] \\ &=R(s)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right) \end{aligned}
 $$
> 倒数第二步由**全期望公式**推导而来。注意区分 $r 、R$



***2.贝尔曼期望方程***

定义了基于策略$\pi$的状态价值函数和动作价值函数后，根据贝尔曼方程，我们可以得到相应的贝尔曼期望方程。

$$
 \begin{aligned} 
& V_\pi (s) = \mathbb{E}_\pi[r_{t+1} +\gamma V_\pi (s_{t+1}) \mid s_t =s ] \\
&   Q_\pi (s, a) = \mathbb{E}_\pi[r_{t+1} +\gamma Q_\pi (s_{t+1}, a_{t+1}) \mid s_t =s , a_t =a]
\end{aligned}
$$

将Q函数贝尔曼方程 $Q_\pi(s) = R(s) + \gamma \sum_{s' \in S}p(s' \mid s,a)V_\pi (s')$ ，带入价值函数与Q函数关联公式$V_\pi(s) = \sum_{a \in A} \pi(a \mid s)Q_\pi(s,a)$ ，我们可以得到**当前状态的价值与未来状态价值的关联**（贝尔曼期望方程的另一种形式）

$$
V_{\pi}(s)=\sum_{a \in A} \pi(a \mid s)\left(R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V_{\pi}\left(s^{\prime}\right)\right)
$$

同理，将价值函数与Q函数关联公式带入Q函数贝尔曼方程，我们可以得到**当前时刻的Q函数与未来时刻的Q函数的关联**。

$$
Q_{\pi}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) Q_{\pi}\left(s^{\prime}, a^{\prime}\right)
$$

***

## 3. 值函数 & 动作值函数 & 优势函数

***1. 值函数（状态-价值函数 State Value Function）$V ( s )$：***

是从状态到价值的映射。可以理解为在该状态下所有可能动作所对应的动作值函数乘以采取该动作的概率的和。更通俗的讲，值函数$V ( s )$是**该状态下所有动作值函数关于动作概率的平均值**。

***2. 动作值函数（行为价值函数 State-Action Value Function）$Q ( s,a )$***

针对某一状态下某一行为的价值评估，是**单个动作所对应的值函数**。

***3. 优势函数（Advantage Function）：$Q π ( s , a ) − V π ( s )$***

能**评价当前动作值函数相对于平均值的大小**，因此优势指的是动作值函数相比于当前状态的值函数的优势。如果优势函数大于零，则说明该动作比平均动作好，如果优势函数小于零，则说明当前动作还不如平均动作好。

## 4.基于值 & 基于策略 & 演员-评论家

***1. 基于值（value-based）：***

显式的学习价值函数，隐式的学习它的策略，策略其从学到的价值函数里面推算出来的。给定一个状态就能计算出每种可能动作的奖励，即**根据值的大小选择相应的动作**。例如：Q-Learning、 Sarsa

不过基于值的方法存在一定的问题：

* 不适合解决**连续性**空间问题，因为基于值的方法需要保存状态-动作的对应关系，因此很多连续动作空间，都因为大量的状态而无法计算。
* **随机策略**学习较难，采用基于值的方法在确定的状态下将得到确定的反馈，因此在使用这种方法的下一步动作是确定的， 可能会落入错误的循环中，陷入局部最优。

***2. 基于策略（policy-based）：***

直接学习策略，并没有学习价值函数，给定一个状态，它就会输出对应动作的概率，优化策略的过程也就是优化**相应状态-动作对被选择的概率的过程**。例如：策略梯度算法

***3. 演员-评论家（actor-critic）：***

智能体把策略和价值函数都进行学习，智能体会根据策略策略做出动作，而价值函数会对做出的动作给出价值。在原有策略梯度的基础上加速学习过程，取得更好的效果。

## 5.确定性策略 & 随机性策略

***1. 确定性策略（Deterministic Policy）：***

策略在一个确定的状态下能够产生一个**确定的行为**。智能体直接采取最有可能的动作：

$$
a^{*}=\underset{a}{\arg \max \pi}(a \mid s)
$$

***2. 随机性策略（Stochastic Policy）：***

在确定的状态下不能够产生一个确定的行为，而是提供各种可能**行为的概率**，然后对这个概率分布进行采样，得到智能体将要采取的动作。

$$
\pi(a \mid s)=p\left(a_{t}=a \mid s_{t}=s\right)
$$

强化学习一般使用随机性策略，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境，随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要。 采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。

## 6. 行为策略 & 目标策略

***1. 行为策略（behavior policy）***

行为策略是探索环境的策略，一般用$\mu$来表示，通过行为策略尽可能多样化的收集经验信息，然后把这些经验交给目标策略去学习。

***2. 目标策略（target policy）***

目标策略是我们需要学习的策略，一般用$\pi$ 来表示，它可以根据自己的经验来学习最优的策略,不需要和环境进行交互。

比如Q-Learning 中的目标策略优化，它不会管下一步去哪里探索，它只会选取奖励最大的策略，因此行为策略收集的信息越多，目标策略学习的数据就越多，那么所做的决策相对也越好。

## 6. 同策略 & 异策略

***1. 同策略（on-policy）***

同策略行为策略和目标策略是**同一个策略**。它只使用一个策略$\pi$，不仅使用策略$\pi$与环境交互产生经验，也使用策略$\pi$学习。目标策略的优化需要**兼顾探索**，为了平衡探索和利用，训练时就会显得“胆小”一点，在悬崖行走问题中，会尽可能远离悬崖边，哪怕不小心探索了一点，也还是在安全区域内。

> 在SARSA算法中，行为策略是$\varepsilon-greedy$策略，目标策略也是$\varepsilon-greedy$策略；

***2. 异策略（off-policy）***

行为策略和目标策略**不是同一个策略**。因为分离了目标策略和行为策略，目标策略优化的过程不需要兼顾探索，可以大胆地用行为策略探索得到的经验轨迹来优化目标策略，从而更有可能探索到最佳策略。因此在悬崖行走问题中，会更加激进一些， 希望每一步都能获得最大利益。

> 在Q-Learning算法中，行为策略是$\varepsilon-greedy$策略，目标策略是贪婪策略。


上面同策略和异策略是针对同一智能体而言的，如果进行扩展，不局限于同一智能体的话，与环境交互的智能体和要学习的智能体可以是不同时刻的
- 要学习的智能体和与环境交互的智能体是相同的，我们称之为同策略。
- 如果要学习的智能体和与环境交互的智能体不是相同的，我们称之为异策略。



## 7. 有模型 & 无模型

有模型和无模型方法的根本区别就是有无对环境进行建模。

***1. 有模型（model-based）***

智能体需要**建立一个描述环境运作过程的模型**，以此来指导价值、策略函数的更新。具体来说，智能体在与环境进行一定数量的交互后，将收集到的样本用来构建环境的模型，然后基于此模型模拟更多的交互，无成本的得到足够多的样本，就不需要再真实环境中交互。等环境运行到某些状态时，就可以直接在构建的模型中获取这些状态，加快训练速度，降低获取样本的成本。如果建模不准确，则会与真实环境有较大误差。

智能体知道状态转移函数和奖励函数后，他就能知道在某一状态下执行某一动作后能带来的奖励和环境的下一状态，智能体可以对真实环境进行建模，构建一个虚拟世界来模拟真实环境中的状态和交互反应，就不需要在真实环境中采取动作，直接在虚拟世界中学习和规划策略就可以。

***2.无模型（model-free）***

个体并不需要去了解这个环境是如何运作的，它通过学习价值函数和策略函数进行决策。无模型强化学习**没有对真实环境进行建模**，通常情况下， 状态转移函数和奖励函数很难估计，甚至连环境中的状态都可能是未知的，智能体只能在真实环境中通过一定的策略来执行动作，等待奖励和状态迁移，然后根据这些反馈信息来更新动作策略，这样反复迭代直到学习到最优策略。

***3. 总结***

有模型方法：
	- 交互更少、样本有效性更高；
	- 依赖于转移模型的建立，难以建立足够准确的模型

无模型方法：
	- 无须环境的先验知识，容易实现
	- 样本利用率低收敛速度慢


总之，有模型强化学习相比免模型强化学习仅仅多出一个步骤，即对真实环境进行建模。在实际应用中，如果不清楚该用有模型 强化学习还是免模型强化学习，可以先思考在智能体执行动作前，是否能对下一步的状态和奖励进行预测， 如果能，就能够对环境进行建模，从而采用有模型学习。

*   无模型强化学习通常属于**数据驱动型**方法，需要大量的采样来估计状态、动作及奖励函数，从而优化动作策略，正因如此，无模型学习的**泛化性**要优于有模型强化学习

    > 有模型强化学习算需要对真实环境进行建模，并且虚拟世界与真实环境之间可能还有差异，这限制了有模型强 学习算法的泛化性。
    
*   有模型的深度强化学习可以在一定程度上缓解训练数据匮乏的问题，因为智能体可以在虚拟世界中进行训练。

    > 在无模型强化学习中，智能体只能一步一步地采取策略，等待真实环境的反馈;有模型强化学习可 以在虚拟世界中预测出将要发生的事，并采取对自己最有利的策略。例如 雅达利游戏平台


## 8. 学习& 规划：

* 学习（Learning）：在强化学习中，环境初始是未知的，智能体不知道环境是如何工作的，个体与环境进行交互，逐渐改进策略。
* 规划（Planning）：环境是已知的，智能体被告知了整个环境的运作规则的详细信息。智能体 能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算。智能体不需要实时地与环境交互就能知道未来环境，只需要知道当前的状态，就能够开始思考，来寻找最优解。

## 9.  探索 & 利用

探索与利用的平衡（The balance between exploration and exploitation）是强化学习中最核心的问题之一。利用指的是根据当前已知知识或数据学习到最优动作，而探索则是指探索未知环境以获取新知识（从而潜在地可能找到回报更大的动作）。探索和利用之间平衡的关键在于：**如何在选择探索和选择利用之间做出权衡，以便在未知世界中高效地探索，以实现回报最大化。**

* 探索（Exploration）：是个体初期不知道哪种动作是正确的，只能通过不断**试错**去探索，通过不断尝试不同的行为去判断这个行为能不能得到一个好的奖励，从而帮助个体利用这些探索信息去得到一个最佳策略。
*  利用（Exploitation）：当智能体收集一定的环境信息后，根据目前的信息来选择行为进行决策，这时的决策是**局部最优解**，短期内能获得的最大收益，不去尝试新的行为，根据当前已经知道的信息去获取一个自认为最优的解。

> 为此需要协调好探索和利用，若只进行探索则无法固定到最优的一个结果。而只利用的话，个体没有机会体验可能发生的变化，就很容易陷入局部最优解，甚至找不到可靠的解。

## 10. 预测& 控制

* 预测（Prediction）：是给定一个马尔可夫决策过程和一个**策略**$\pi$, 计算它的价值函数，也就是计算每个状态的价值。根据现有的策略，个体对未来进行评价，即**策略评估**。它间接**评价**在一个给定策略下个体表现的优劣程度。
* 控制（Control）：给定一个马尔可夫决策过程，去寻找一个最佳的策略，然后同时输出它的最佳价值函数 $V^{*}$ 以及最佳策略 $\pi^{*}$。对所有的策略进行评估，发现最优的那个策略，即求解**最优策略**。试图寻找一个最优策略来最大化奖励，即最优个体的表现。

> 这两者的区别就是，预测问题是**给定一个策略**，我们要确定它的价值函数是多少。而控制问题是在**没有策略的前提下**，我们要确定最佳的价值函数以及对应的决策方案。
>
> 在**马尔可夫决策过程**里面，预测和控制都可以通过**动态规划**解决
>
> 两者为递进关系，在强化学习中，我们通过预测问题，进而解决控制问题。




# 二、问题提问

## 什么是强化学习
**定义**：强化学习（reinforcement learning）是智能系统(智能体)在与环境的连续**互动**中学习最优行为策略的机器学习问题。

**核心**：强化学习的核心就是 策略和奖励，策略就是给定状态下采取的动作，**根据奖励信号去改进策略**。

**目标**：智能系统的目标不是短期奖励的最大化，而是长期**累积奖励的最大化**。**==我们期望智能体在最大化回报的同时，保持足够大的策略多样性和策略的通用性。==**

**本质**：强化学习的本质是智能体在环境中通过不断**试错** (trial and error)并利用环境的延迟回报进行学习。

智能体与环境交互获得观测信息，智能体根据观测信息决定要对环境实施一个行为，环境会根据该行为做出相应改变并给予一定的反馈信息（奖励），智能体根据接收的反馈信息，建立“自身状态“”所施行为”以及“所得反馈”之间的联系，作为**自身记忆**的一部分给后续的决策提供参考。

强化学习问题是从策略集寻找最优策略，因此强化学习是一个**优化问题**。

## 强化学习的探索方式
- **ε-greedy 探索**：智能体以一定的概率ε选择随机动作, 以 1-ε的概率选择当前最优动作。ε值的选择很关键, 太大会导致过度探索, 太小则可能陷入局部最优。通常ε会随着训练逐渐衰减。

- **玻尔兹曼探索 (Boltzmann Exploration)：** 是强化学习中一种基于 softmax 分布的探索策略。通过对动作值函数的输出进行 softmax 运算, 从而得到一个概率分布, 然后根据该分布随机采样动作。
$$
π(a|s) = \frac{e^{Q(s, a) / \tau}}{\sum_{a'}e^{Q(s, a') / \tau}}
$$
	- 当温度系数τ很大时, 概率分布接近于均匀分布, 探索性更强; 当τ很小时, 分布接近确定性策略, 利用性更强。
	- 为更好的设置温度系数，可以通过下面几种方法进行调整
		-  **基于时间/步数的衰减**
		- **基于熵的调整**：目标是保持策略的熵在一个合理的范围内。具体而言,先计算当前策略的熵 H,如果 H 过大说明探索过多,则降低τ;如果 H 过小说明利用过多,则增大τ。公式为τ' = τ * exp(α * (H_target - H)),其中α是步长参数。
	- 不同于ε-greedy, 玻尔兹曼探索直接输出一个随机策略, 不需要维护两种策略。

- **置信区间上界（Upper Confidence Bound，UCB）：** 为每一个动作维护一个值, 这个值由两部分组成, 一部分是该行为的**期望回报**（利用），另一部分是一个**置信区间上界**（探索），对于很少被选择的行为会有较大的值，鼓励去探索。在每一个时间步, UCB 算法选择最大化这个值的行为执行。

- **噪声扰动 (Noise)**：
	- 在确定性策略的输出动作上添加噪声扰动，避免策略陷入确定性或次优的状态。
	- 在策略梯度方法中,我们需要通过采样数据来估计策略梯度。添加噪声可以使采样的轨迹更加分散,避免局限在少数几条轨迹上,从而提高梯度估计的准确性。更准确的梯度估计有助于优化出更加平滑的策略
- **熵正则化 (Entropy Regularization)**：在策略目标函数中加入熵损失项，鼓励策略具有一定的随机性和探索性。可自动调节探索程度, 无需设置探索率等超参数。


## 强化学习优势与应用


强化学习的优势应用前景广阔, 主要体现在以下几个方面:
1. **自主决策和自适应能力**: 强化学习可以让系统在复杂的环境中自主学习、做出决策, 并随环境变化而不断调整, 这在需要自适应性的场景中非常有优势, 如自动驾驶、机器人控制等。
2. **处理复杂决策问题**: 强化学习能够处理大量变量和不确定因素的复杂决策问题, 在游戏、资产管理、供应链优化等领域有广泛应用前景。
3. **优化系统性能:** 通过强化学习可以自动调整系统参数, 实现性能最优化, 在工业自动化、能源管理等领域有望发挥重要作用。
4. **模拟和探索新方案:** 强化学习可以在模拟环境中进行大量探索和试验, 发现新的解决方案, 在医疗、设计、创新等领域具有潜力。
5. **与人类智能的协作:** 结合人类专家的知识和经验, 强化学习可以实现人机协作, 发挥各自的优势, 在医疗诊断、金融决策等领域前景广阔。

强化学习的技术优势
- **具备序列决策能力**：强化学习擅长解决连续决策问题, 能够通过对环境的反馈 (奖赏或惩罚)不断优化决策序列, 从而获得最优策略。这使其能很好地应用于决策控制、机器人运动规划、游戏 AI 等序列决策领域。
- **无需完备监督数据**： 与监督学习不同, 强化学习不需要大量人工标注的训练数据, 只需一个合理的奖赏函数设计和环境交互, 便可自主学习获取最优策略。这极大降低了数据标注成本。
- **实时决策能力：** 例如强化学习在最优控制领域的一个很大的优势就是求解速度快，强化学习训练完成后输出的是一个策略网络，它的结算速度要远高于 MPC
- **在线增量学习** ：强化学习可以在与环境持续交互过程中不断学习和提升, 是一种很好的在线增量学习范式。
- **权衡探索与利用**：强化学习需要在探索 (尝试新策略以获取更多经验)和利用 (使用当前已知最佳策略)之间达到平衡, 有助于发现全局最优解而不陷入局部极小值。


## 问题与挑战

- **奖赏函数设计的困难**：奖赏函数是强化学习的核心, 它定义了智能体应该优化的目标。但是在复杂的实际问题中, 确定一个合理且高效的奖赏函数往往是个大挑战。奖赏函数设计不当可能导致智能体学习出错误甚至潜在危险的行为策略。

- **样本效率低下**：与监督学习相比, 强化学习需要通过大量的 trial-and-error 来积累经验, 从而学习得到好的策略, 这个过程通常是低效的。如何提高强化学习的样本利用效率是一个亟待解决的问题。

- **现实环境部署难度高**：将强化学习模型和算法从模拟环境迁移到真实物理环境中仍然面临诸多挑战, 例如真实感知的噪声、不确定性等。

## Sim2real
**难点：**

- **环境差异**：模拟环境与真实环境之间通常存在差异，模拟中常常无法精确模拟真实世界中的所有物理属性, 如摩擦力、空气阻力、变形等, 导致模拟环境与真实环境在动力学方程上存在偏差。.
- **安全性和鲁棒性**：模拟环境通常是理想化和受控的, 而真实环境中存在更多的不确定因素和极端条件, 对策略的安全性和鲁棒性提出更高要求。
-  **数据效率低下**：由于真实环境中数据采集成本高且存在风险, 无法像模拟环境那样高效获取大量数据, 这制约了强化学习的数据驱动性能。

**解决方法：**
- 域随机化: 在模拟环境中引入**随机噪声**, 增加模拟与真实环境的差异性, 提高泛化能力。
-  **使用更精确的模拟器**：使用更精确的模拟器可以减少现实差距。
- **在模拟环境中加入更多的细节**：在模拟环境中加入更多的细节可以使模拟环境更加接近现实环境。
- 混合精化: 首先在模拟环境中**预训练**, 再利用真实环境中的少量数据精化策略。
- 域适应技术: 使用少量**监督数据**, 通过域适应技术缩小模拟和真实数据的分布差异。
- 逐步部署: 先在接近真实环境的高保真模拟环境中部署, 再逐步过渡到真实环境。

现实世界中大部分的决策都不会有仿真环境，这个时候人们自然而然的就想到利用采集到的离线数据得到决策模型。常用的解决方法包括：**模仿学习**和**离线强化学习算法**。



## 多智能体强化学习中的非平稳性问题？

***产生原因：***

多智能强化学习容易受到非平稳性问题的影 响，在多智能体训练过程中，每个智能体的策略随  
时间不断变化，状态转移函数和奖励函数受所有智能体动作的影响也不断变化，智能体有时无法  
判断自己在某一时刻获得的奖励是由于自身动作 还是其他智能体的动作导致的。

在马尔科夫博弈中，环境的状态转移函数 $T$ 和单个智能体的奖励函数 $r_i$ 受到 所有智能体动作的影响. 在训练多个智能体的过程中, 每个智能体的策略随着时间在不断变化，因此每个智能体所感知到的转移概率分布和奖励函数也会发生变化. 通常的单智能体强化学习算法假定这些函数具有平稳性, 因而这些算法不能很好的用在多智能体的场景下。
尤其对于采用经验回放池的算法，影响更为严重，从过去收集的轨迹中采样时，在相同状态和动作下过去可能会取得较好的奖励，由于状态转移概率的改变，在当前不一定会获得较好的奖励，策略分布可能差距较大。

> **马尔科夫博弈**：是由马尔可夫决策过程和博弈论相结合的产物，首先马尔可夫决策过程表示了多智能体系统的状态符合马尔可夫性，即下一刻状态只与当前状态有关，与前面的时刻没有直接关系；其次博弈代表了多智能体之间的关系，或是合作，或是竞争，亦或两者都有。
>
> 由于多个智能体同时根据自己的利益改进他们的政策，从每个智能体的角度来看，环境动态变得是不平稳的，智能体本身无法判断状态过渡（奖励的变化），是由于其自己的行动，还是由于其对手的探索。

一个智能体的值函数也取决于其他智能体的行为，因此我们也需要对其他智能体的行为进行采样。

***解决方法：***

- 将其他智能体视为环境一部分？（Ind-DQN and Ind-DDPG.）
- **中心化的评论家（CTDE）**：评论家的训练是集中的，这使得其可以接触到所有智能体的观察和行动，而演员的训练是分散的。常见方法COMA和MADDPG
- **去中心化的学习方法**：在多智能体系统中处理非平稳性并不一定需要进行集中训练.在
  - **自我博弈**：神经网络使用每个智能体自身的观察作为输入，通过将其与当前或以前的版本进行比较，来学习可以推广到任何对手的策略，该方法可能会导致训练中缺乏探索，因此可以将神经网络参数存储在学习过程中的不同位置，然后，在自我博弈过程中，在当前和以前版本的神经网络参数中随机选择对手.
  - **经验回放池**：直接使用经验回放池可能会存储大量与去中心化学习无关的信息，从而导致性能变差，因此可以采用**重要性采样**动态调整以往经验的权重。
- **对手建模(Opponent Modeling)**：通过模拟其他智能体的意图和政策，可以稳定智能体的训练过程。当前智能体使用他们的策略来预测其他智能体的行为，另一种方式是利用一个单独的网络为其他智能体建模.这两个网络的组合可以通过连接它们的隐藏状态，也可以通过混合使用专家来实现。
- **元学习(Meta-Learning)：**
  - 使用学习方法，一旦环境动态发生变化，就尝试快速更改策略或环境模型，不过使用少量的学习更新可能不能很好的适应动态环境。
  - 另一种方法是预测环境的变化，而不是制定一种训练深度神经网络对环境变化作出反应的学习算法，当给定环境的动态变化时可以制定一个优化过程来寻找初始神经网络参数，这样便可以使用少量的更新过程来更好的学习。
- **通信**：通过通信机制，不同的智能体可以交换关于他们的观察、行动和意图的信息，以稳定其训练过程。每个智能体可以通过显示或隐式方法共享自己观测的特征信息，来进行通信交流。

REF：

https://zhuanlan.zhihu.com/p/72553328

## 强化学习目前的瓶颈

- 探索效率
- 样本利用率
- 可迁移性

https://www.zhihu.com/question/449478247

## 强化学习落地方案


***设计方案层面**

- 在前期设计时，需要正确识别哪些任务、场景是适合 RL 的任务，哪些任务、场景是适合领域传统方法去做的，如果前期不能很好识别，后期可能很难训练出稳定有效的策略。
- 对于一个完整可交付的项目，单纯的RL技术可能很难独立支撑，可能RL只在其中一个环节起作用，这个作用还可能会影响其他环节，RL对整个项目的最终效益很难评估。

***算法层面***

- 如何把问题建成合理高效的MDP是关键的步骤，这需要我们我们对该领域的传统方法比较熟悉，也就是说将强化学习算法与领域知识相结合才能更好的激发RL的价值。
- 首先我们选择更好调参的 RL 算法，积累更多的调参经验，从而改善产出比，进行更好落地。有时简单的RL算法在实际应用中效果会更好。

***部署层面***

sim2real

- 先在仿真环境下训练然后移植到真实环境中
- 直接在真实环境中进行训练（模仿学习）



#### 问题与挑战

- **奖赏函数设计的困难**：奖赏函数是强化学习的核心,它定义了智能体应该优化的目标。但是在复杂的实际问题中,确定一个合理且高效的奖赏函数往往是个大挑战。奖赏函数设计不当可能导致智能体学习出错误甚至潜在危险的行为策略。

- **样本效率低下**：与监督学习相比, 强化学习需要通过大量的 trial-and-error 来积累经验, 从而学习得到好的策略, 这个过程通常是低效的。如何提高强化学习的样本利用效率是一个亟待解决的问题。

- **现实环境部署难度高**：将强化学习模型和算法从模拟环境迁移到真实物理环境中仍然面临诸多挑战,例如真实感知的噪声、不确定性等。

#### sim2real
难点：
- **环境差异**：模拟环境与真实环境之间通常存在差异，模拟中常常无法精确模拟真实世界中的所有物理属性, 如摩擦力、空气阻力、变形等, 导致模拟环境与真实环境在动力学方程上存在偏差。.
- **安全性和鲁棒性**：模拟环境通常是理想化和受控的, 而真实环境中存在更多的不确定因素和极端条件, 对策略的安全性和鲁棒性提出更高要求。
-  **数据效率低下**：由于真实环境中数据采集成本高且存在风险,无法像模拟环境那样高效获取大量数据,这制约了强化学习的数据驱动性能。
