
## 一、定义

多智能体系统是一种分布式计算技术，可用于解决各种领域的问题，包括机器人系统、分布式决策、交通控制 和商业管理等。多智能体强化学习是多智能体系统研究领域中的一个重要分支，**它将强化学习技术、博弈论等应用到 多智能体系统，使得多个智能体能在更高维且动态的真实场景中通过交互和决策完成更错综复杂的任务。**
> 多 智 能 体 深 度 强 化 学 习 是 机 器 学 习 领 域 的 一 个 新 兴 的 研 究 热 点 和 应 用 方 向 ，涵 盖 众 多 算 法 、规 则 、框 架 ，并 广 泛 应 用 于 自 动 驾 驶 、能 源 分 配 、编 队 控 制 、航 迹 规 划 、路 由 规 划 、社 会 难 题 等 现 实 领 域。

多智能体强化学习遵循随机博弈 (Stochastic Game，SG) 过程


### 三种学习框架

多智能体强化学习有三种主要的学习框架：
- **集中式训练集中式执行**：将所有智能体视为一个整体，有一个中心节点收集所有数据，学习所有智能体的策略并控制所有的智能体
- **分布式训练分布式执行**：每一个智能体将其他智能体 视为环境的一部分，每个智能体学习自己的策略， 自主控制，并且通常智能体之间没有显式的通信， 可能会有隐式通信
- **集中式训练分布式执行**：有一个中心节点收集 所有智能体的数据，帮助所有智能体学习策略，但 是智能体的策略网络是独立的，具有独立的输入数 据，独立运行，在学习结束后可以分别部署到单个 智能体模块

### 三种任务类型

MARL 的算法根据其回报函数的不同可以分为完全合作型 (Fully Cooperative)、完全竞争型 (Fully Competitive) 和混合型 ( Mixed) 三种任务类型。

- **完全合作型**：智能体的回报函数是相同的，表示所有智能体都在为实现共同的目标 而努力，其代表算法有团队 Q 学习 (Team Q -learning)、 分布式 Q 学习 (Distributed Q-learning)
- **完全竞争型**：智能体的回报函数是相反的，环境通常存在两 个完全敌对的智能体，它们遵循 SG 原则。智能体的目标是最大化自身的回报，同时尽可能最小化对方回报，其代表算法为 Minimax- Q。
- **混合型**：智能体的回报函数并无确定性正负关系，该模型适合自利型(Self-interested)智能体，一般来说此类任务的求解大都与博弈论中均衡解的概念相关，即当环境中的一个 状态存在多个均衡时，智能体需要一致选择同一个均衡。比较典型的有纳什 Q 学习(Nash Q -learning)、相关 Q 学习(Correlated Q - learning)、朋友或敌人Q 学习(Friend or Foe Q - learning等。

### MARL优缺点

***MARL优点***
- 合作型智能体间可以互相配合完成高复杂度的任务;
- 多个智能 体可以通过并行计算提升算法的效率;
- 竞争型智能体间 也可以通过博弈互相学习对手的策略，

***MARL 缺点***
- RL 固有的探索利用矛盾 (Explore and Exploit) 和维度灾难 (Curse of Dimensionality);
- 多智能体环境非平稳性 (None- stationary) 问题;
- 多智能体信度分配 (Multiagent Credit Assignment) 问 题 ;
- 最优均衡解问题 ;
- 学习目标选择问题等。

## 二、MARL 分类

### 1. 无关联型
直接将单智能体强化学习方法推广到多智能体系统，无需在智能体之间构 、建通信规则，每个智能体独立与环境交互并完成训练过程，该类方法能够有效地**规避维度灾难**带来的影响，且在**可扩展性**方面有先天性的优势。由于智能体之间互不通联，每个智能体将其他 智能体看作环境的一部分，从个体的角度上看，环境是处在不断变化中的，这种环境**非平稳性**严重影响了学习策略的稳定和收敛，另外该类方法的学习效率和速率都十分低下。

***两种方法：***
- 独立式学习：对每个智能体分别使用强化学习算 法, 而将其它智能体看作环境的一部分。
- 集中式学习：将所有智能体的状态和动作集中在一起, 构成一个扩张的状态和动作空间, 并直接使用单智能体的强化学习算法

![[MARL2.png]]

### 通信规则型

在智能体间建立**显式的通信机制**(如通信方式、通信时间、通信对象等)，并在学习过程中逐渐确 定和完善该通信机制，训练结束后，每个智能体需要根 据其他智能体所传递的信息进行行为决策，此类方法多应用于完全合作型任务和非完全观测环境。

强 化 互 学 习 (Reinforced Inter- Agent Learning， RIAL) 和 差分互学习 (Differentiable Inter- Agent L earning，DIAL ) 是比较有代表性的通信规则型算法，它们遵循集中训练分散执行框架都使用中心化的 Q 网络在智能体之间进行信息传递，该网络的输出不仅包含 Q 值，还包括在智能体之间交互的信息。
![[MARL1.png]]

### 互相协作型
此类方法并不直接在多智能体间建立显式的通信规则，而是使用传统 MARL 中的一些理论使智能体学习 到合作型策略。

值函数分解网络 (Value Decomposition Networks， VDN) 及其改进型 QMIX 和 QTRAN 等将环境的全局回报按照每个智能体对环境做出的贡献进行拆分，具体是根据每个智能体对环境的联合回报的贡献大小将全局 Q 函数分解为与智能体一一对应的本地 Q 函数， 经过分解后每个 Q 函数只和智能体自身的历史状态和动作有关，上述三种算法的区别在于 Q 函数分解的方式不同。

![[MARL3.png]]

### 建模学习型

## 三、算法

### IQL

独立式 Q 学习算法 (independent Q-learning, IQL) 是一个 典型的无关联型算法， 将 `DQN` 扩展到分布式的多智能体强化学习环境学习过程中。 每个智能体获得其局部观测, 并且向着**最大化整体奖励值**的方向调整每个智能体的策略, 即每个智能体独立的执行 Q 学习算法. 

由于每个智能体在学习的过程中, 其它智能体的策略同时发生变化, 打破了环境静态性的假 设, 该方法在离散状态–动作空间下的小规模问题 上具有一定的效果, 对复杂问题无法获得理想的效果


### VDN
（Value-Decomposition Networks）



### QMIX

