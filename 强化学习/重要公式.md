# 三、无模型控制：

广义策略迭代

### Sarsa：同策略时序差分控制

$$
Q\left(s_{t}, a_{t}\right) \leftarrow Q\left(s_{t}, a_{t}\right)+\alpha\left[r_{t+1}+\gamma Q\left(s_{t+1}, a_{t+1}\right)-Q\left(s_{t}, a_{t}\right)\right]
$$

### Q： 异策略时序差分控制

$$
Q\left(s_{t}, a_{t}\right) \leftarrow Q\left(s_{t},a_{t}\right)+\alpha\left[r_{t+1}+\gamma \max _{a} Q\left(s_{t+1}, a\right)-Q\left(s_{t}, a_{t}\right)\right]
$$



# 四、策略梯度

### 策略梯度算法
$$
\nabla \bar{R}_{\theta}=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
$$
### 策略梯度实现技巧：
- 增加基线
  $$
  \nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(R\left(\tau^{n}\right)-b\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
  $$
- 分配合适的分数
  $$
  \nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(\sum_{t^{\prime}=t}^{T_{n}} \gamma^{t^{\prime}-t} r_{t^{\prime}}^{n}-b\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
  $$

### REINFORCE：蒙特卡洛策略梯度

$$
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} G_{t}^{n} \nabla \log \pi_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
$$



# 五、近端策略优化
### 重要性采样

$$
\mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)\right]
$$

此前策略梯度一条采样一条轨迹更新后，就不再使用了，即更新参数只进行一次，在这里通过重要性采样后，网络$\theta$可以多次更新参数,一直到$\theta$训练到一定的程度，更新多次后，$\theta^{\prime}$再重新进行采样，同策略变为异策略。

通过重要性采样，实际要优化的目标函数：
$$
J^{\theta^{\prime}}(\theta)=\mathbb{E}_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]
$$

### 近端策略优化

因为PPO的$\theta^{\prime}$  是 $\theta_{old}$ ，所以即使重要性采样，可以将同策略变为异策略，但这里$\theta^{\prime}$仍为$\theta$，所以PPO仍为同策略。

PPO是为了避免因重要性采样导致偏差较大而采取的算法。有两种优化方法
针对策略（最大化目标函数）
- 近端策略优化惩罚
  $$
  \begin{aligned}
  &J_{\mathrm{PPO}}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta \mathrm{KL}\left(\theta, \theta^{k}\right) \\
  &J^{\theta^{k}}(\theta) \approx \sum_{\left(s_{t}, a_{t}\right)} \frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)
  \end{aligned}
  $$
- 近端策略优化裁剪
  $$
  \begin{aligned}
  J_{\mathrm{PPO} 2}^{\theta^{k}}(\theta) \approx \sum_{\left(s_{t}, a_{t}\right)} \min &\left(\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right)\right.\\
  &\left.\operatorname{clip}\left(\frac{p_{\theta}\left(a_{t} \mid s_{t}\right)}{p_{\theta^{k}}\left(a_{t} \mid s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^{k}}\left(s_{t}, a_{t}\right)\right)
  \end{aligned}
  $$
  



# 六、深度Q网络

DQN：基于深度学习的Q学习算法， 主要结合了价值函数近似与神经网络技术，并采用目标网络和经验回放的方法进行网络的训练。

### 状态价值函数



### 动作价值函数



### 深度Q网络技巧

- 目标网络
- 探索
  - $\epsilon$ -贪心
  - 玻尔兹曼探索：
- 经验回放



# 七、 深度Q网络进阶

### 双深度Q网络

为解决Q值高估问题，使用使用目标网络替代原Q网络

### 竞争深度Q网络

竞争深度Q网络不直接输出Q值，而是分成两条路径运算，一条输出状态价值标量，一条输出状态-动作向量（每个动作都有值），使其相加得到Q（s，a），这样不用对所有的状态-动作都采样，可以高效的估计Q值。

### 优先经验回放池

多去训练那些表现不好的数据（损失大）


### 噪声网络

增加探索的一种方法，给参数增加噪声，噪声在一个回合内是否固定的是区别他和$\varepsilon$-贪心的区别。


### 分布式Q函数

只用Q值的期望进行建模来代表整个奖励，可能会失去一些信息，无法对奖励的分布进行建模。

### rainbow

多种技巧综合起来
