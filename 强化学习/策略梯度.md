
## 一、 简述
**什么是策略？**
策略就是智能体在与环境交互的过程中，在不同状态下如何选择动作。

**什么是策略函数？**
策略函数就是在给定的状态和一定的参数设置下，采取任何可能行为的概率。
#参数化策略函数
>如果策略 $\pi$ 就是一个神经网络，网络里面有一些参数， 我们用 $\theta$ 来代表 π 的参数，$\theta$就是策略函数。

**策略函数的目标是什么？**
通过不断优化策略函数的参数, 去提高任一状态下采取“最优”行为的概率，遵循这个策略产生的行为获得更高的回报。
> 这个最优可能并不是全局最优，只是局部最优

**什么是策略目标函数？**
我们怎么去衡量某一状态下不同的动作的好坏，就需要有一个目标函数（累计回报的期望），目标函数值越高，说明动作越好。
> 这个目标函数可以是值函数，通过价值函数来引导策略函数的更新

**什么是策略梯度？**
根据策略函数的目标，在策略目标函数的引导下，对策略函数使用梯度上升算法优化参数以最大化奖励。

**策略梯度原理是什么？**
策略梯度不通过误差反向传播进行更新，它通过观测值选出一个行为直接利用目标函数进行反向传播， 通过目标函数对选择行为的可能性进行增加和减弱，好的行为下一次被选中的概率会增加，不好的行为下次被选中的概率会减少，

**基于策略的方法与基于价值函数的方法有什么不同？**
首先两种方法的目标都是希望能获得最优的策略（回报最高）
- 基于价值函数的方法就是，计算每一个状态-动作的价值， 然后选择价值最高的动作进行执行，即优化（贪婪）值函数就可以得到最优策略。这是一种间接的做法，我们迭代计算的是值函数，根据值函数对策略改进。估计 #后验概率 来得到预测，类比 #NaiveBayes 。
- 基于策略的方法，是直接对策略函数进行迭代计算，也就是迭代更新参数化的策略函数，直到累计回报的期望最大，此时的参数所对应的策略为最优策略。不估计后验概率而直接优化学习目标，可类比 #SVM

**为什么要引入策略梯度？**
随着状态-价值空间不断增大（状态、动作连续），把每一个状态都独立出来，指出在某个状态下应该执行某个行为是不太可能的。因此，为了解决大规模问题，我们需要参数化策略，用少量的参数来合理的近似策略函数。

**策略梯度的优缺点？**


## 二、轨迹
### 1. 环境与演员
一个回合里，环境输出的 $s$  与演员输出的动作 $a$ 全部组合起来，就是一个**轨迹**：
$$
\tau=\left\{s_{1}, a_{1}, s_{2}, a_{2}, \cdots, s_{t}, a_{t}\right\}
$$
给定策略函数的参数 $\theta$, 我们可以计算某个轨迹 $\tau$ 发生的概率为
$$
\begin{aligned}
p_{\theta}(\tau) &=p\left(s_{1}\right) p_{\theta}\left(a_{1} \mid s_{1}\right) p\left(s_{2} \mid s_{1}, a_{1}\right) p_{\theta}\left(a_{2} \mid s_{2}\right) p\left(s_{3} \mid s_{2}, a_{2}\right) \cdots \\
&=p\left(s_{1}\right) \prod_{t=1}^{T} p_{\theta}\left(a_{t} \mid s_{t}\right) p\left(s_{t+1} \mid s_{t}, a_{t}\right)
\end{aligned}
$$
> 这里需要注意的是：
> -  $p_{\theta}(a_t \mid s_t)$ 表示的是由策略里的网络参数 $\theta$ 所决定的，策略网络的输出是一个分布，演员根据这个分布**采样**，决实际要采取的动作，
> -  $p _{\theta}(s_{t+1} \mid s_t, a_t)$ 是由环境决定的，表示从状态 $s_t$ 下执行动作 $a_t$ 到达状态 $s_{t+1}$ 的概率（由环境内部决定，可能有概率也可能没有概率），这也是一个概率分布。 

某个轨迹出现的概率取决于**环境的动作和智能体的动作**，环境的动作是环境根据其内部的参数或内部设定的规则采取的动作，通常我们无法控制环境，因为环境是设定好的。我们能控制的是 $p_{\theta}(a_t \mid s_t)$，智能体的动作是演员可以控制的，因此我们可以控制演员，演员动作的不同，**每个同样的轨迹就会有不同的出现的概率**。

### 2. 总奖励
奖励函数根据某一状态下采取的某一动作决定这个动作可以得到的分数。每个状态动作对 $<s_t,a_t>$ 都会对对应一个奖励 $r_t$，把轨迹的所有奖励 $r$ 都加起来，就能得到轨迹 $\tau$ 的总奖励： $R(\tau)$

*为什么这里衡量一个状态的好坏要用轨迹总奖励而不是即时奖励的*
对于一些序列决策的监督任务，他们通常是利用**贪心**的思想去解决的，对序列中每个元素寻优（局部），从而得到整个序列的最优。而强化学习直接优化的目标是整个轨迹（全局），因此只用即时奖励去衡量一个状态是不合理的，所以要用轨迹总奖励去衡量。
> 这里使用轨迹奖励也是不准确的，也是需要优化的，但是它处理强化学习问题的出发点是对的，

## 三、策略梯度

### 1. 期望奖励
我们要做的目标就是调整演员内部的参数 $\theta$，使得期望奖励 $R(\tau)$ 的值越大越好，这与强化学习的目标使得期望回报的值越大是一致的。
根据上一小节，智能体的动作和环境的动作都是具有随机性的，所以 $R(\tau)$ 也是一个随机变量，因此我们能计算的就是 $R(\tau)$ 的期望值：
$$
\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[R(\tau)]$$
从分布 $p_{\theta}(\tau)$ 采样一个轨迹 $\tau$ ，计算 $R(\tau)$ 的期望值，就是**期望奖励**（expected reward），也就是我们的**策略目标函数**。


### 2. 期望奖励梯度
我们要最大化这个期望奖励，其中仅有 $p_{\theta}(\tau)$ 与 $\theta$ 是有关系的，通产情况下没有办法采样完整，所以我们采样 $N$ 个 $\tau$ 并计算每一个值，把每一个的值加起来求平均，就可以得到期望奖励的梯度 $\nabla \bar{R}_{\theta}$ ： [[#期望奖励的梯度推导过程]]
$$
\begin{aligned}
\nabla \bar{R}_{\theta}&=\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)\\
&=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
\end{aligned}
$$
> N 表示轨迹个数，$T_n$ 表示一条轨迹中所有（s，a）数量。
> $\log p_{\theta} (a_{t}^{n} \mid s_{t}^{n})$: 对数概率(log probability)

在这里我们也可以看出与监督学习的区别，对于我们执行的每一个动作，在监督学习中都有一个标签 $y$（例如\[0, 0, 1]）去判断是否正确，此时损失函数（交叉熵）的梯度公式为：$y*\nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)$，因为此时有一个正确的标签来指导神经网络朝着正确的方向（梯度下降）更新。而在强化学习中，输出的动作没有标签来判断它是否是绝对正确的，此时的奖励R就充当对动作的评价。奖励越大，那就说 明当前输出的真实的动作就越好，损失就越需要重视，因为梯度闪



我们的目标是最大化期望奖励（回报），因此对于我们**策略**（$\pi$），我们采用期望奖励作为我们的**策略目标函数**（$\bar{R}$），从宏观上来说，我们需要建立策略（可以理解为自变量）和策略目标函数（因变量）的一个映射关系，这个关系呢通常是非线形的，在这里我们建立**第一个非线性对应关系**，即策略与策略目标函数。在理论上，存在一个策略，使得策略目标函数的值最大，根据这一方向，我们即通过不断调整策略，去得到一个更高的策略目标函数值，从而逐渐逼近最优的策略目标函数。

*那既然策略作为自变量的话，如何修改策略的值呢？如何修改更为合理呢？*
第一个问题：如果将策略看作是一个神经网络的话，我们修改神经网络的参数 $\theta$ (权重和偏置) 就会改变神经网络的输出，即影响每个动作输出的概率，进而相应的策略也会发生改变。在这里我们**第二个非线性对应关系**，策略目标函数与策略参数。
第二个问题：神经网络的权重和偏置可以有无数总组成，那如何修改神经网络的参数更为合理呢，一种直观的方法就是常提到的 #梯度上升 、 #梯度下降  。为了直观介绍如何通过这两种方法影响神经网络的更新，我们需要在回到上面宏观的策略和策略目标函数上，因此策略和策略目标函数之间存在一种非线性关系，那么最简单更新策略的方式，就求策略目标函数关于策略的梯度。如果想要使策略目标函数最大，就沿着梯度正方向移动（梯度上升），就有可能会逼近最大值（局部最大），相反想要策略目标函数最小，就沿着梯度负方向移动（[[梯度下降]]），就有可能会逼近最小值（局部最小）。策略通常由权重和偏置组成，就可以转换为求策略的参数（$\theta$）关于策略目标函数（$\bar{R}$）的梯度— $\nabla \bar{R}_{\theta}$ 。然后我们通过 #反向传播 （梯度上升）来更新策略的参数。
$$
\theta \leftarrow \theta+\eta \nabla \bar{R}_{\theta}
$$
> 因为学习率也是要调整的，因此可用 Adam、RMSProp 等方法来调整学习率


*通过上面两个问题，我们了解宏观策略更新的方法，那落实到每一条轨迹应该做呢？*
要获得使策略目标函数（期望奖励）最大的策略，根据期望奖励公式 $\bar{R}_{\theta}=\sum_{\tau} R(\tau) p_{\theta}(\tau)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}[R(\tau)]$ 可知，期望奖励与轨迹以及轨迹对应的奖励有关，我们希望获得较大奖励的轨迹被采样到的概率也越高，获得较低奖励的轨迹被采样到的概率也越低。在这里我们建立**第三个非线性对应关系**，轨迹概率分布与策略目标函数的非线性关系。

接着我们在对一条轨迹展开：如果在轨迹 $\tau$ 中，我们在状态 $s_t$ 执行动作 $a_t$，最后发现 $\tau$ 的奖励是正的，那我们就要增加在 $s_t$ 执行 $a_t$ 的概率，反之奖励是负的，我们就要减少在 $s_t$ 执行 $a_t$ 的概率。

*如何实现上述的操作呢？*
由于策略根据当前状态 $s_t$，产生动作概率分布，在概率分布中采样获得动作 $a_t$，我们希望在状态 $s_t$ 下好的动作执行的概率大，不好的动作执行概率小，那怎么评判动作好不好呢？就是由该轨迹的奖励来决定，在这里就建立起**第四个非线性对应关系**，动作概率分布与策略目标函数梯度的非线性关系，或者说是动作与策略目标函数之间的关系，那为什么我们希望取得到一个更好的动作呢？因为更好的动作，他对应的奖励就越高，奖励越高，策略目标函数值越大，那么在基于当前动作点求梯度时，根据**梯度上升**的方法，若移动相同的步长（学习率决定），好的动作会比差的动作更容易接近最优的动作点，所以我们才更去希望在好的动作周围进行采样，所以要增大好动作发生的概率，根据动作和其对应的奖励，我们就能知道我们要想的概率分布大概是什么了。根据 #极大似然估计  的思想，根据我们期望的概率分布，去反推最有可能产生该概率分布的策略参数。根据上述要求，我们可以使用梯度上升来更新策略函数的参数。从而最终得到：
$$
\theta \leftarrow \theta+\eta \nabla \bar{R}_{\theta}
$$
$$
\nabla \bar{R}_{\theta}=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
$$
刚才第四个非线性对应关系，为了更好理解，**固定了状态**，即针对某个状态来说的，只说了动作与策略目标函数的关系，但一条轨迹是由多个状态来表示的，所以加入状态后由二维的非线性对应关系，就变成了三维的非线性对应关系，因此我们希望每个状态下都能取的总奖励高的动作，这与我们强化学习的目标完成了闭环。


在这里回收四个非线性对应关系
- **策略**与策略目标函数（期望奖励）：$\pi$  ~ $\bar{R}$
- **策略参数**与策略目标函数： $\pi_{\theta}$ ~ $\nabla \bar{R}_{\theta}$
- **轨迹** 与策略目标函数：$\nabla \bar{R}_{\theta} =\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau)$
- **动作概率分布**与策略目标函数：$\nabla \bar{R}_{\theta}=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)$

对于一个状态来说，我们期望在该状态下获得更好的动作取获得更高的总奖励，那就改变动作概率分布，让好的动作出现的概率也越大。而对一条轨迹来说，每个状态下采取的动作，都是希望该轨迹尽量更优，那怎样产生更优的轨迹呢，就需要调整策略的参数。

在折回去，我们通过调整策略的参数，在每个状态才下产生对整个轨迹来说更好的动作概率分布，产生更优的动作，从而形成一条更优的状态-动作 轨迹。



通常我们只需设计好我们的期望奖励就可以，剩下的就通过 Pytorch/Tensorflow 直接通过反向传播 `loss.backward()` 直接计算梯度，再通过优化器`optimizer` 更新参数。
$\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right)  \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)$



### 3. 总结
策略梯度的目的就是，通过改变策略（神经网络）的参数，使得策略产生的  能得到大的奖励的 动作的 概率变大，即更优的动作概率分布。也就是说不断进行策略迭代，使策略参数向期望奖励增大的方向更新（梯度上升），从而优化动作概率分布，使之能采样出更好的动作。

- 无法处理奖励总为正值的情况。按照正奖励增加动作概率，负奖励减少动作概率的原则，如果所有动作奖励都为正，则按理所有的动作概率都该增加，这显然是无法完成，这时根据它们前面的权重 R($\tau$) 是不一样的。权重是有大有小的，权重小的，该动作的概率提高的就少;权重大的，该动作的概率提高的就多。所以提高少的，在做完归一化(normalize)以后，动作的概率就是下降的;提高多的，该动作的概率才会上升。
- 奖励使用的是"全局奖励"，宏观上还是以一条轨迹作为更新单位。即这条轨迹下所有的状态-动作对都用这一个奖励作为他的权重，显然这是不合理的，这样某条轨迹下的状态-动作对就没有区分性。
- 对于策略梯度，更新完模型以后，我们要重新采样数据在更新模型，采样的数据只会用一次。
- 对于策略梯度，采用非确定性策略，即在某个状态下采取某个动作是不确定的，注意与确定性策略梯度（DPG）区分。

## 四、策略梯度改进
### 1. 添加基线
**作用**：解决奖励总为正值的情况
**解决**：轨迹奖励减去一个基线值，从而区分小的正奖励和大的正奖励。
$$
\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n}\left(R\left(\tau^n\right)-b\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)
$$
b 可以是对 $\tau$ 的值取期望，即 $b=E[R(\tau)]$，在训练时不断记录 $R(\tau)$ 的值，但通常情况下是通过一个神经网络（Critic）的输出来模拟 b。R − b 这一项称为**优势函数**(advantage function)，即$A^\theta(s_t,a_t)$。优势函数的意义是当前动作相对于其他动作有多好，不是绝对的好，而是相对的好。


### 2. 分配合适的分数
**作用**：为每一个动作分配合适的奖励，而不是使用之前的全局奖励
**解决**：计算某个状态-动作对的奖励的时候，不把整场游戏得到的奖励全部加起来，只计算从这个动作执行以后得到的奖励，之前的奖励对当前的状态没有影响。同时需要对未来奖励进行折扣，因为一般情况下，时间拖的也就，该动作对未来的影响就越小。

### 3. REINFORCE:蒙特卡洛策略梯度
**作用**：计算每个步骤的未来总奖励。
**解决**：算法完成一个**回合**之后，再利用这个回合的数据去学习，做 一次更新（MC）。
$$
\nabla \bar{R}_\theta \approx \frac{1}{N} \sum_{n=1}^N \sum_{t=1}^{T_n} G_t^n \nabla \log \pi_\theta\left(a_t^n \mid s_t^n\right)
$$
其中$G^n_t$表示的某一轨迹中，每一步骤的未来总奖励，其计算公式如下所示：
$$
\begin{aligned}
G_t &=\sum_{k=t+1}^T \gamma^{k-t-1} r_k \\
&=r_{t+1}+\gamma G_{t+1}
\end{aligned}
$$
根据未来总奖励的关系式，从后往前一步一步地推，先算 $G_T$ ，然后往前推一直算出$G_1$

## 补：公式推理
### 期望奖励的梯度推导过程
$$
\begin{aligned}
\nabla \bar{R}_{\theta} &=\sum_{\tau} R(\tau) \nabla p_{\theta}(\tau) \\
&=\sum_{\tau} R(\tau) p_{\theta}(\tau) \frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)} \\
&=\sum_{\tau} R(\tau) p_{\theta}(\tau) \nabla \log p_{\theta}(\tau) \\
&=\mathbb{E}_{\tau \sim p_{\theta}(\tau)}\left[R(\tau) \nabla \log p_{\theta}(\tau)\right] \\
\\
& \approx \frac{1}{N} \sum_{n=1}^{N} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(\tau^{n}\right) \\
&=\frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)
\end{aligned}
$$

倒数第三步计算的理想情况下（所有轨迹都能采到）的期望奖励的梯度，最后一步计算的是实际中采用的期望奖励。

在这里 $N$ 表示轨迹个数，$T_n$ 表示在某条轨迹下轨迹的长度。因此第一个求和是把所有轨迹加起来，第二个求和是把其中一条轨迹的所有状态-动作加起来。因此在计算（n 确定）$\sum_{t=1}^{T_{n}} R\left(\tau^{n}\right) \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)$ 时，$R\left(\tau^{n}\right)$ 是一个定值（共有n个），即$R\left(\tau^{n}\right) \sum_{t=1}^{T_{n}}  \nabla \log p_{\theta}\left(a_{t}^{n} \mid s_{t}^{n}\right)$

 $\nabla \log p_{\theta}(\tau)$ 的具体计算过程可写为：$$
\begin{aligned}
\nabla \log p_{\theta}(\tau) &=\nabla\left(\log p\left(s_{1}\right)+\sum_{t=1}^{T} \log p_{\theta}\left(a_{t} \mid s_{t}\right)+\sum_{t=1}^{T} \log p\left(s_{t+1} \mid s_{t}, a_{t}\right)\right) \\
&=\nabla \log p\left(s_{1}\right)+\nabla \sum_{t=1}^{T} \log p_{\theta}\left(a_{t} \mid s_{t}\right)+\nabla \sum_{t=1}^{T} \log p\left(s_{t+1} \mid s_{t}, a_{t}\right) \\
&=\nabla \sum_{t=1}^{T} \log p_{\theta}\left(a_{t} \mid s_{t}\right) \\
&=\sum_{t=1}^{T} \nabla \log p_{\theta}\left(a_{t} \mid s_{t}\right)
\end{aligned}
$$因为 $p(s_1)$ 和 $p _{\theta}(s_{t+1} \mid s_t, a_t)$ 由环境决定，与 $\theta$ 无关，所以他们梯度计算的结果为0


*梯度计算时为什么要使用对数概率？*
即为什么要将$\nabla p_{\theta}(\tau)$转换为$\sum_{t=1}^{T_{n}} \nabla \log p_{\theta}(\tau^n)$ ？
首先，如果不转换为log怎么求，：
$$
\begin{aligned}
p_{\theta}(\tau) &=p\left(s_{1}\right) p_{\theta}\left(a_{1} \mid s_{1}\right) p\left(s_{2} \mid s_{1}, a_{1}\right) p_{\theta}\left(a_{2} \mid s_{2}\right) p\left(s_{3} \mid s_{2}, a_{2}\right) \cdots \\
&=p\left(s_{1}\right) \prod_{t=1}^{T} p_{\theta}\left(a_{t} \mid s_{t}\right) p\left(s_{t+1} \mid s_{t}, a_{t}\right)
\end{aligned}
$$
-首先这些概率要不断相乘，如果这条轨迹很长，无论最后奖励有多高，这么多0~1的数乘起来，很容易发生 #数据下溢 ，出色的动作反而得不到概率上的提升。

而利用log特性，将乘法转换为加法。既防止了数据下溢，每个样本的关联性也打消了（因为转成加法后， $p _{\theta}(s_{t+1} \mid s_t, a_t)$ 就被去掉了）

从策略的收敛速度上来说，策略通过log运算，把原来0~1区间上的数映射到了 $(-\infty,0)$，并且没有改变数据的性质和相关关系（单调性保持不变），区间范围扩大了，一方面增大了梯度的大小，更新的更快了。另一方面，可以进一步拉大好的动作和差的动作之间的差距，使得策略尽可能朝着好的动作的方向去更新

连乘计算消耗资源，且容易造成计算精度的不准确

计算策略梯度时要使用对数的原因，总结如下吧：
1）打消同一个轨迹中样本计算上的关联性，方便计算
2）增大数据范围，加快策略更新速度
3）解决连乘计算的精度不准确问题以及消耗资源问题，
[参考](https://blog.csdn.net/qq_39004117/article/details/91465973?spm=1001.2101.3001.6650.12&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-12-91465973-blog-108828545.pc_relevant_multi_platform_featuressortv2dupreplace&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-12-91465973-blog-108828545.pc_relevant_multi_platform_featuressortv2dupreplace&utm_relevant_index=13)