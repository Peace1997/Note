---
aka description: Proximal Policy Optimization 近端策略优化
tags:
---

# 一、PPO简述

PPO (Proximal Policy Optimization) 是一种改进的策略梯度的算法，结合了重要性采样和 GAE（Generalized Advantage Estimator）的思想，对新旧策略的分布进行限制，保证每一次新计算出的策略能够和原策略相差不大，稳步提升算法的表现。

*PPO解决TRPO存在的问题*
PPO解决TRPO求解约束优化问题复杂度过高，重要性采样计算方差很大的问题。解决方式为：
- **截断式优化目标**（降低优化问题计算复杂度）
- **悲观的约束**：当优势值大于0时，保持足够谨慎的更新；当优势值小于0时，保持足够的惩罚力度。（降低重要性采样方差较大的问题）
- 结合其他改进方法（**GAE**）

近端策略优化算法借鉴了信任区域策略优化算法，通过采用一阶优化，在采样效率、算法表现以及实现和调试的复杂度之间取得了新的平衡。

*PPO基本流程*
（1）数据收集：收集序列轨迹，预计算所需值（Return、优势估计）
（2）数据训练：两重训练循环，提高数据利用效率
- 第一重循环：将收集到的数据训练多个Epoch
- 第二重循环：在某一个Epoch内，将数据划分为多个mini-batch进行更新。
（3）网络更新
- 策略更新：截断式优化目标+悲观约束
- 价值函数更新：结合多步时序差分方法
## PPO & TRPO 

*TRPO实现方式*
TRPO通过可靠的方向和合适的步长，解决如何让策略持久稳定提升问题。利用两个技巧来实现上述目标
- 通过**KL散度**控制新旧策略的更新幅度
- 通过**重要性采样**解决新旧策略分布差异的问题
采用当前策略（更新前的策略，旧策略）去收集数据，用收集到的数据去计算一个更新方向，这个更新方向因为有**重要性采样的校正**，因为**有信赖域的约束**，因此可以保证策略的提升。

*PPO和TRPO算法的区别*
KL 散度的区别：在 PPO1 中 KL 散度应用在目标函数的公式，TRPO 把 KL 散度当作约束

PPO 的核心理念就是避免 TRPO 算法中计算 KL 散度导致的复杂性。而 TRPO 算法就是为了解决参数迭代过程中步长过小和过大造成的问题，从参数空间的迭代直接进入到策略空间的迭代，并保证小步单调上升的迭代。

## PPO算法的实现技巧
（1）新的优化目标
**策略网络**：
基于重要性采样、优势函数、KL 散度//裁剪（防止不同分布差距过大；正则化）的方法设计了新的优化目标函数，最大化策略梯度来更新策略网络。
**价值网络**：
通过 reward to go 计算每个状态的回报值，通过GAE的方法计算优势值，用于指导价值网络的更新。

（2）重要性采样
PPO采用了**重要性采样**的方法， 可以利用旧策略采样的数据完成新策略的更新。所以 $\theta$ 可以利用 $\theta^k$ 采样的数据完成多次更新。

（3）分布纠正
PPO为了避免不同分布之间差异过大，可以通过**KL 散度**、**裁剪**的方法来对分布差异进行纠正。

（4）解决策略早熟问题
为解决PPO的中出现策略早熟的问题，PPO中控制策略探索能力的关键点：
- **选择动作时增加温度（temperature）系数**：温度系数对应输出的这个策略动作概率分布进行一个变换，温度系数增加在分母上，温度系数越高，更接近一个均匀分布，熵越高；温度系数越低，整个分布会被拉的更加尖锐，熵越低，从而干更加主动去选择概率最高的动作。通过调整温度系数，使智能体在不同的训练阶段/不同环境中，都拥有足够的策略探索能力。
- **计算损失函数时增加一项熵激励（Entropy Bonus）**：对于策略的熵额外增加一个激励，随着训练的进行，也能保持策略的熵不要下降的太快。在训练的周期内设置影响的bonus衰减（指数衰减 or 类比SAC）

# 二、近端策略优化

**问题解决**：
当我们使用重要性采样时，如果两个分布相差太多（方差较大），会影响梯度计算的结果，因为为了避免分布之间相差太多，就提出了 PPO 方法来解决。

## 1. 同策略问题

同样是使用重要性采样，PPO 在这里的话是同策略的，这是因为 PPO 中与环境交互采样数据的策略 $\theta^\prime$ 是 $\theta_{old}$ ，即行为策略同目标策略（[[强化学习/常问问题#行为策略 目标策略]]）都是 $\pi_\theta$ , 因此 PPO 是同策略的算法。

## 2. 目标函数约束

为了有效避免行为策略与目标策略的分布相差较大，应多加一个约束，这个约束就是 $\theta$ 与 $\theta^{\prime}$ 输出的动作的**KL 散度**（ [[常用解释#KL 散度]]），这一项用于描述 $\theta$ 与 $\theta^{\prime}$ 的相似程度
> 在这里应用的 KL 散度描述的并不是这两个分布之间参数上的距离，二是它们行为上的距离。行为距离就是给定同样的状态，输出动作之间的差异，由于这动作的分布都是概率分布，所以我们计算的是这两个概率分布的 KL 散度。

*为什么不直接计算 $\theta$ 和 $\theta'$ 分支之间参数上的距离，而是行为的距离 ?*
这是因为对于策略网络，参数的变化与动作的变化不一定是完全一致的，有时候参数稍微改变，输出的动作会差异很大，有时参数变化很多，输出的动作可能没什么改变。因为我们最终关注的是动作上的差异，所以通过 KL 散度来控制动作分布的差距。

## 3. 近端策略优化流程

- 首先我们初始化一个策略的参数 $\theta^0$、价值函数参数 $\phi^0$
- 在每一个迭代里，我们用前一个训练的迭代得到的策略的参数 $\theta^k$ 去与环境交互，采用得到大量状态-动作对。
- 根据 $\theta^k$ 交互的结果，结合价值函数 $\phi^k$ 我们估测 $A^{\theta_{k}}(s_t,a_t)$  ，可以使$\theta$完成多次更新，同时也会更新我们的价值函数参数$\phi$。

*核心方法*
因为我们采用了**重要性采样**的方法， 可以利用旧策略采样的数据完成新策略的更新。
为了避免不同分布之间差异过大，可以通过**KL 散度**、**裁剪**的方法来对分布差异进行纠正。

所以 $\theta$ 可以利用 $\theta^k$ 采样的数据完成多次更新。

## 4. 近端策略优化惩罚

对于近端策略优化惩罚 (PPO-penalty)，有一个**自适应 KL 散度**（adaptive K divergence） 的纠正方法，其要最大化的目标函数为：
$$
\begin{aligned}
&J_{\mathrm{PPO}}^{\theta^k}(\theta)=J^{\theta^k}(\theta)-\beta \mathrm{KL}\left(\theta, \theta^k\right) \\
&J^{\theta^k}(\theta) \approx \sum_{\left(s_t, a_t\right)} \frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^k}\left(a_t \mid s_t\right)} A^{\theta^k}\left(s_t, a_t\right)
\end{aligned}
$$
在这里将 KL 散度作为惩罚项，其中参数 $\beta$ 是可以动态调整的，我们需要事先设定一个可接受的 KL 散度的最大值 $KL_{max}$ 和最小值 $KL_{min}$，如果我们优化一次上述公式后：
- ${KL}\left(\theta, \theta^k\right)$ > $KL_{max}$，说明新旧分布差异是比较大，说明后面的惩罚项没有起到作用，因此应该增加惩罚力度，即**增大** $\beta$ 的值。
- ${KL}\left(\theta, \theta^k\right)$ < $KL_{min}$，说明新旧分布差异是比较小的，就说明后面的惩罚项太强了，因为我们更想要的优化的的目标是前一项，如果新旧分布差异过小的话，就有可能会倾向于优化后一项，因此我们减少 $\beta$ 的值。

## 5. 近端策略优化裁剪

对于近端策略优化裁剪（PPO-clip），策略目标函数里面是没有KL散度的，而是增加了一个**裁剪**操作，其要最大化的目标函数为：
$$
\begin{aligned}
J_{\mathrm{PPO} 2}^{\theta^k}(\theta) \approx \sum_{\left(s_t, a_t\right)} \min \left(\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^k}\left(a_t \mid s_t\right)} A^{\theta^k}\left(s_t, a_t\right),\right.
\left.\operatorname{clip}\left(\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^k}\left(a_t \mid s_t\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^k}\left(s_t, a_t\right)\right)
\end{aligned}
$$
- **min** 表示将在原始目标函数与裁剪后的目标函数中选择一个较小的项；之所以选择较小项的原因是 PPO 想要每次更新的步幅小一点。
- **clip** 表示直接对分布差异的进行裁剪限制，当分布差异高于上限（$1 + \epsilon$ ） 和低于下限（ $1 - \epsilon$ ）时直接裁剪截断；而 PPO1 中设定的最大值 $KL_{max}$、最小值 $KL_{min}$ 仅仅是判断条件。其中 $\epsilon$ 是一个超参数，通常设定为 0.1 或 0.2。裁剪效果如下图所示：
![400](img/PPO_clip.png)

对于整个目标函数来看，优势函数 $A^{\theta^k}\left(s_t, a_t\right)$ 的正负值会对目标函数的选取造成影响：
- A > 0 ; 说明当前的状态-动作对是好的， 我们希望增大这个状态-动作对的概率 $p_\theta(a_{t}\mid s_{t})$，但是它与 $p_{\theta^k}(a_{t}\mid s_{t})$ 的比值不能超过 $1+\epsilon$ ；
- A < 0 ；说明当前状态-动作对是不好的，我们希望把 $p_{\theta}(a_{t} \mid s_{t})$ 减小。如果这时 $p_\theta(a_{t}\mid s_{t})$ 比 $p_{\theta^k}(a_{t}\mid s_{t})$ 还大的话，明显是错误的，我们需要尽可能的减小 $p_{\theta}(a_{t} \mid s_{t})$，但是它与 $p_{\theta^k}(a_{t}\mid s_{t})$ 的比值不能小于 $1-\epsilon$；
![](PPO_clip2.png)
之所以设定这样的裁剪方式，是PPO2希望每次概率分布更新时，无论是增加某状态-动作对的概率还是减小状态-动作对的概率，都要有一定的限度，**不能使的两个分布差距相差过大**，这也就是PPO解决重要性采样缺点的方式。PPO每次更新的步幅虽然小，但是每次都是有效的。


![[PPO.png]]

# 三、相关知识


## 1、重要性采样
importance sampling

###  先导知识
对于一个随机变量，我们通常用**概率密度函数** f (x) , 来刻画该变量的概率分布 p 特性。
具体来说，给定随机变量的一个取值 x，可以根据概率密度函数来计算该值对应的概率 (密度)。  x $\rightarrow$ f (x)
反过来，也可以根据概率密度函数提供的概率分布信息来生成随机变量的一个取值，这就是**采样**。 x ~ p
因此，从某种意义上来说，采样是概率密度函数的逆向应用。与根据概率密度函数计算样本点对应的概率值不同，采样过程往往没有那么直接，通常需要根据待采样分布的具体特点来选择合适的**采样策略**。

### 重要性采样方法

我们通过计算一个目标函数的期望值，来体现重要性采样的使用方法和使用场景。


#### 问题描述

已知函数 $f(x)$、及其对应分布 $p$，我们从分布 $p$ 中对 $x$ 进行采样，然后带入 $f$，即可得到 $f(x)$，如果我们可以对分布 p 进行积分如何求解期望，如果我们不能对分布 p 进行积分，如何求解 $f(x)$ 的期望？如果我们不能从分布 p 采样数据，如何求解 $f(x)$ 的期望？

#### 问题解决

对于问题 1（**积分**）：如果我们可以对分布 $p$ 求积分的话，可以直接利用积分公式进行求解：
$$
\mathbb{E}_{x \sim p}[f(x)] = \int f(x) p(x) \mathrm{d} x
$$
> 通过积分的方法，我们可以获取分布p中所有的x的值，这样计算出来的期望值也是最准确的。 p(x)为概率函数

对于问题 2 （**采样**）：如果不能对分布 $p$ 求积分，此时我们可以通过在 p 中进行采样数据 $x^i$，把 $x^i$ 带入 f，计算求平均值，即可近似 $f(x)$ 的期望值。
$$
\mathbb{E}_{x \sim p}[f(x)] \approx \frac{1}{N} \sum_{i=1}^N f\left(x^i\right)
$$
> 由于我们可以对p分布进行采样，通过从p分布中采样尽可能多的值来近似最后的期望值

对于问题 3（**重要性采样**），如果我们无法从 p 中进行采样的话，我们可以从其他分布 q 来进行采样计算，由于 p、q 分布存在差异，所以我们得需对 q 分布进行纠正，这就需要用到我们所说的重要性采样的方法。

==重要性采样核心公式==
首先我们问题1的积分公式进行转换：
$$
\int f(x) p(x) \mathrm{d} x=\int f(x) \frac{p(x)}{q(x)} q(x) \mathrm{d} x=\mathbb{E}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]
$$
通过转换，我们就可以将原问题：从 p 分布中采样计算 f (x) 的期望，转换为从 q 中转换，计算 $f(x) \frac{p(x)}{q(x)}$ 的期望，其中，$\frac{p(x)}{q(x)}$ 就是**重要性权重**，用来修正这两个分布的差异。
$$
\mathbb{E}_{x \sim p}[f (x)]=\mathbb{E}_{x \sim q}\left[f (x) \frac{p (x)}{q (x)}\right]
$$
#### 重要性采样的作用

- 计算期望时，由于无法从原始分布 p 采样的情况，通过在相似分布 q 中采样，来近似待求的**期望结果**，或者说逼近所求分布p。
- 在训练过程中，对于有较大影响的训练数据进行更多的采样，对参数更新影响较小的数据则进行较少的采样。具体实现方式是，根据每个训练样本的收益来计算它的重要性权重。然后，将这些重要性权重作为概率值，进行随机采样，选择要用于训练的样本。这样做能够保证在训练过程中，更多的精力集中在有较大影响的样本上，从而使得算法的收敛速度更快。


#### 重要性采样的问题

对于重要性采样，虽然 q 的分布可以是任取的，但是实际上 **p、q 的分布不能差距过大**，否则可能会出现问题：

*期望相同，方差较大*
通过下图可知，p、q 分布差距是比较大的，p 分布采样到的 x 主要集中在左侧，而 q 分布采样主要集中在右侧，但是如果采样方差计算的话，两者之间的差距就会表现出来。其中方差的计算公式为：$\operatorname{Var}[X]=E\left[X^2\right]-(E[X])^2$
$$
\begin{align*}
&\operatorname{Var}_{x \sim p}[f(x)]=\mathbb{E}_{x \sim p}\left[f(x)^2\right]-\left(\mathbb{E}_{x \sim p}[f(x)]\right)^{2}\\\\	
\operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right] &=\mathbb{E}_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^2\right]-\left(\mathbb{E}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^2\\
\\&=\mathbb{E}_{x \sim p}\left[f(x)^2 \frac{p(x)}{q(x)}\right]-\left(\mathbb{E}_{x \sim p}[f(x)]\right)^2
\end{align*}

$$
![[importance_sampling.png]]
通过对比两者的方差公式，主要的差距是体现在方差公式的第一项，从q分布的采样方差要比p分布的采样方差多一个重要性权重，因此如果重要性权重很大的话，他们的方差差距也是比较大的。

因此从 p、q 中只要采集够多的样本，就可以让他们他们的期望值是比较相似的，但是如果采样的样本不够多的话，由于两者方差比较大， 就很容易使最后期望值的计算结果差距比较大。

**总结：**
因此要想解决重要性采样的缺点：
- 从 p、q 分布中采集足够多的样本
- p、q 分布的差异不能过大


### 3. 重要性采样在异策略中的应用

将同策略的策略梯度，通过重要性采样，转换为异策略的策略梯度。

#### 方法介绍

在策略梯度中，我们是通过策略 $\pi_\theta$ 来与环境进行交互，采样获得轨迹 $\tau$，通过计算 $\mathbb{E}_{\tau \sim p_\theta(\tau)}\left[R(\tau) \nabla \log p_\theta(\tau)\right]$ 来更新策略，而现在我们使用策略 $\pi_{\theta^\prime}$ 代替策略 $\pi_\theta$ 来与环境交互，策略 $\pi_{\theta^\prime}$ 只用于给策略 $\pi_\theta$ 做示范，提供与环境的交互数据，策略 $\pi_\theta$ 借由这些交互数据来更新参数 $\theta$。

基于重要性采样的思想，我们原本是希望从 $\theta$ 中进行进行采样的，但是现在改为通过 $\theta^\prime$ 进行采样，这样采集的轨迹数据 $\tau$ 就都来自于分布 $\theta^\prime$，而我们要学习的策略是 $\theta$，由于两个分布存在差异，现在通过重要性权重 $\frac{p_\theta(\tau)}{p_\theta^\prime(\tau)}$ 来修正

*为什么要用 $\theta^\prime$ 替换 $\theta$ 来进行采样？*
使用 $\theta^\prime$ 进行采样时，采样的数据是与 $\theta$ 无关的，$\theta$ 可以利用 $\theta^\prime$ 采样的大量数据完成**多次更新**，一直到 $\theta$ 训练更新到一定程度时，在使用 $\theta^\prime$ 重新进行采样。

#### 梯度计算方法

在进行策略梯度计算时，我们并不是针对 $\tau$ 来进行的，而是面向每个状态-动作对分开计算：
$$
\mathbb{E}_{\left(s_t, a_t\right) \sim \pi_\theta}\left[A^\theta\left(s_t, a_t\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)\right]
$$
这里采用的技巧是：[[5. 策略梯度#四、策略梯度改进]]，优势函数表示在当前状态 $s_t$ 下未来奖励的累积减去基线（Critic 的预测值），用于判断当前状态下采取动作的**相对好坏**，如果优势函数值大于 0，则增大当前状态下输出该动作的概率。

由于重要性采样，将同策略的策略梯度改变为了异策略，之前与环境进行交互的策略 $\pi_\theta$ 变为了 $\theta^\prime$，所以现在 $s_{t}、a_{t}$ 是策略 $\pi_{\theta^\prime}$ 与环境交互采样到的数据，同时优势函数也需要进行修改；但是要进行更新的策略是 $\pi_\theta$，因为 $\theta^\prime$ 与 $\theta$ 是不同的模型，所以要有一个修正的重要性权重：$\frac{p_\theta(s_t, a_t)}{p_{\theta^{\prime}}(s_t,a_t)}$，其中分子分母拆开为：
$$
\begin{aligned}
p_\theta\left(s_t, a_t\right) &=p_\theta\left(a_t \mid s_t\right) p_\theta\left(s_t\right) \\
p_{\theta^{\prime}}\left(s_t, a_t\right) &=p_{\theta^{\prime}}\left(a_t \mid s_t\right) p_{\theta^{\prime}}\left(s_t\right)
\end{aligned}
$$
基于此，带入原本的策略梯度的公式可以得出：
$$
\mathbb{E}_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} \frac{p_\theta\left(s_t\right)}{p_{\theta^{\prime}}\left(s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)\right]
$$
但是由于通常我们很难计算环境中的$p(s_t)$发生的概率，而$p(a_{t} | s_{t})$是可以根据策略网络模型得出的，因此计算时我们常常会忽略掉$p(s_t)$的计算。所以基于重要性采样的**异策略的梯度**：
$$
\mathbb{E}_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right) \nabla \log p_\theta\left(a_t^n \mid s_t^n\right)\right]
$$
#### 目标策略

当未使用重要性采样时，我们要去优化的目标函数为：
$$
J(\theta)=\mathbb{E}_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}A^{\theta}\left(s_t, a_t\right)]
$$
当我们使用重要性采样的时候, 要去优化的目标函数为
$$
J^{\theta^{\prime}}(\theta)=\mathbb{E}_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right)\right]
$$


我们也根据梯度与目标函数的关系，可以反推原来的目标函数：
$$
\nabla f(x)=f(x) \nabla \log f(x)
$$
即$\nabla p_\theta(a_{t}\mid s_t)$ 

$\theta$ 求梯度时, $p_{\theta^{\prime}}\left(a_t \mid s_t\right)$ 和 $A^{\theta^{\prime}}\left(s_t, a_t\right)$ 都是常数。


## 2、GAE
Generalized Advantage Estimator 广义优势估计，这是一种**平衡优势函数估计中偏差和方差的方法**。
>- 策略梯度法与AC法的区别：  
    使用全部奖励来估计策略梯度，尽管无偏但是方差大；Actor-Critic 方法使用值函数来估计奖励，能够降低方差但是偏差较大。
    **高方差需要更多的样本来训练，偏差会导致不收敛或收敛结果较差。**

首先采用时序差分的方式来计算回报（ $r_t+\gamma v(s_{t+1})$ ）和其增量$\delta_t$
$$
 {TD_{Error}}=\delta_t=r_t+\gamma v\left(s_{t+1}\right)-v\left(s_t\right)
$$
其中，$\gamma$表示折扣因子，表示未来奖励对当前状态的影响程度。

定义k步估计的优势函数，当k越大，方差越大，偏差越小。
$$
\begin{aligned}
&\begin{array}{ll}
\hat{A}_t^{(1)}:=\delta_t^V & =-V\left(s_t\right)+r_t+\gamma V\left(s_{t+1}\right) \\
\hat{A}_t^{(2)}:=\delta_t^V+\gamma \delta_{t+1}^V & =-V\left(s_t\right)+r_t+\gamma r_{t+1}+\gamma^2 V\left(s_{t+2}\right) \\
\hat{A}_t^{(3)}:=\delta_t^V+\gamma \delta_{t+1}^V+\gamma^2 \delta_{t+2}^V & =-V\left(s_t\right)+r_t+\gamma r_{t+1}+\gamma^2 r_{t+2}+\gamma^3 V\left(s_{t+3}\right)
\end{array}\\
&\hat{A}_t^{(k)}:=\sum_{l-n}^{k-1} \gamma^l \delta_{t+l}^V=-V\left(s_t\right)+r_t+\gamma r_{t+1}+\cdots+\gamma^{k-1} r_{t+k-1}+\gamma^k V\left(s_{t+k}\right)
\end{aligned}
$$
然后运用GAE公式进行**优势函数**的估计：（**指数加权平均法**）
#指数加权平均
$$
\begin{aligned}
\hat{A}_t^{\mathrm{GAE}(\gamma, \lambda)} & :=(1-\lambda)\left(\hat{A}_t^{(1)}+\lambda \hat{A}_t^{(2)}+\lambda^2 \hat{A}_t^{(3)}+\ldots\right) \\
& =(1-\lambda)\left(\delta_t^V+\lambda\left(\delta_t^V+\gamma \delta_{t+1}^V\right)+\lambda^2\left(\delta_t^V+\gamma \delta_{t+1}^V+\gamma^2 \delta_{t+2}^V\right)+\ldots\right) \\
& =(1-\lambda)\left(\delta_t^V\left(1+\lambda+\lambda^2+\ldots\right)+\gamma \delta_{t+1}^V\left(\lambda+\lambda^2+\lambda^3+\ldots\right)\right. \\
& \left.\quad \quad \quad \quad+\gamma^2 \delta_{t+2}^V\left(\lambda^2+\lambda^3+\lambda^4+\ldots\right)+\ldots\right) \\
& =(1-\lambda)\left(\delta_t^V\left(\frac{1}{1-\lambda}\right)+\gamma \delta_{t+1}^V\left(\frac{\lambda}{1-\lambda}\right)+\gamma^2 \delta_{t+2}^V\left(\frac{\lambda^2}{1-\lambda}\right)+\ldots\right) \\
& =\sum_{l=0}^{\infty}(\gamma \lambda)^l \delta_{t+l}^V
\end{aligned}
$$
当$\lambda$=0时，相当于TD-error,会引入偏差，但是方差更小
当$\lambda$=1时，相当于A∞,求和项较多具有高方差
当 0 < $\lambda$ < 1 就是平衡方差与偏差的过程
$$
\begin{array}{ll}
\operatorname{GAE}(\gamma, 0): & \hat{A}_t:=\delta_t \quad=r_t+\gamma V\left(s_{t+1}\right)-V\left(s_t\right) \\
\operatorname{GAE}(\gamma, 1): & \hat{A}_t:=\sum_{l=0}^{\infty} \gamma^l \delta_{t+l}=\sum_{l=0}^{\infty} \gamma^l r_{t+l}-V\left(s_t\right)
\end{array}
$$

为了快速估计序列中所有时刻的估计值，采用**倒序计算**，从 t+1 时刻估计 t 时刻：
$$
\hat{A}_t^{G A E(\gamma, \lambda)}=\sum_{l=0}^{\infty}(\gamma \lambda)^l \delta_{t+1}^V=\delta_t^V+\gamma \lambda \hat{A}_{t+1}^{G A E(\gamma, \lambda)}
$$
通过GAE可以进行策略梯度的计算：
$$
g^\gamma \approx \mathbb{E}\left[\sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) \hat{A}_t^{\mathrm{GAE}(\gamma, \lambda)}\right]=\mathbb{E}\left[\sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right) \sum_{l=0}^{\infty}(\gamma \lambda)^l \delta_{t+l}^V\right]
$$
## 3、rewards to go
其中 rewards-to-go（$\hat{R}_t$）是计算每个状态的回报值。
$t$ 时刻的剩余可得回报 $\hat{R}_t$ 是指在一个Trajectory $\tau=\left(s_1, a_1, r_1, s_2, \cdots, s_T\right)$ 中, $t$ 时刻及其以 后所有奖励的累计, 其计算如下所示。
$$
\hat{R}_t=\sum_{k=t}^T r_k
$$
它可以通过**最小化损失函数的方法，指导 Critic 网络（value function）的更新**，使得 Critic 网络更好的预测每个状态的回报值。

## 4、熵正则化
**熵 (Entropy)**：来衡量概率分布的不确定性。

（1）应用背景：
不同的环境类型，不同的环境设置，所需的策略探索能力不同。为解决策略早熟的问题，
>**策略早熟**：策略以某种“投机取巧”的方式找到局部最优解。当环境发生变化时，策略很难完成适应。

策略网络的输出的概率不要集中在一个动作上，至少要给其他的动作一些非零的概率，让这些动作能被探索到。


# 补充

## PPO与TRPO区别

在 PPO 中 KL 散度应用在优化目标函数的公式中：
$$
\begin{aligned}
&J_{\mathrm{PPO}}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta \mathrm{KL}\left(\theta, \theta^{\prime}\right) \\
&J^{\theta^{\prime}}(\theta)=\mathbb{E}_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right)\right]
\end{aligned}
$$
在TRPO（信任区域策略优化(trust region policy optimization) PPO前身）中把KL散度当作约束。
$$
J_{\mathrm{TRPO}}^{\theta^{\prime}}(\theta)=\mathbb{E}_{\left(s_t, a_t\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_\theta\left(a_t \mid s_t\right)}{p_{\theta^{\prime}}\left(a_t \mid s_t\right)} A^{\theta^{\prime}}\left(s_t, a_t\right)\right], \mathrm{KL}\left(\theta, \theta^{\prime}\right)<\delta
$$







补充：

一个策略的熵越大，意味着这个策略选择各个动作的概率更加“平均”。在PPO中，为了提高算法的探索能力，我们一般在actor的loss中增加一项策略熵，并乘以一个系数`entropy_coef`，使得在优化actor_loss的同时，让策略的熵尽可能大。一般我们设置`entropy_coef=0.01`。

是否使用策略熵的对比如图7所示，通过对比可以看出，使用策略熵可以一定程度提高训练效果


https://zhuanlan.zhihu.com/p/512327050





Critic 网络：
拟合状态值函数，


**为什么需要 Critic 网络而不是采用奖励？**
Critic 网络用于判断当前状态在未来中可能获得的价值（累积奖励），奖励通常是衡量当前状态下的及时奖励，无法判断当前状态在未来中是否是好的，因此通过 Critic 网络评判状态的好坏会更加合理。

优势函数是正的就增大概率，如果是负的就减小概率。


Policy：
$$
L\left(s, a, \theta_{k}, \theta\right)=\min \left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)} A^{\pi_{\theta_{k}}}(s, a), \quad \operatorname{clip}\left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{k}}(a \mid s)}, 1-\epsilon, 1+\epsilon\right) A^{\pi_{\theta_{k}}}(s, a)\right)
$$
Value ：
